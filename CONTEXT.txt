# TLDRScraper Comprehensive Context
# Generated: $(date -u +"%Y-%m-%d %H:%M UTC")

================================================================================
                              DOCUMENTATION CONTEXT
================================================================================

Total: 8 markdown files
<files>
<file path="AGENTS.md">
---
last_updated: 2025-11-21 07:35, afaa731
---
# Agents Guide

## Project overview

Newsletter aggregator that scrapes tech newsletters from multiple sources, displays them in a unified interface, and provides AI-powered TLDRs.

- Stack:
   * Python: Flask backend, serverless on Vercel
   * React 19 + Vite (frontend) (in `client/`)
   * Supabase PostgreSQL for all data persistence
   * OpenAI GPT-5 for TLDRs
- Storage: Project uses Supabase Database (PostgreSQL) for all data persistence (newsletters, article states, settings, scrape results). Data is stored server-side with client hooks managing async operations.
- Cache mechanism: Server-side storage with cache-first scraping behavior. Daily payloads stored as JSONB in PostgreSQL. 

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed flows & user interactions documentation and [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) for a map of the project structure.

## Environment

The single source of truth for what is available locally is the output of:

```bash
env | grep -E -o '^[A-Z_]+' | grep -e TLDR -e TOKEN -e API -e KEY | sort -u  # Should print the names of all environment variables without values on a need-to-know basis.
```

**Run `source ./setup.sh` first thing to install all server and client dependencies and tooling, build the client, verify your environment and provide you with convenience functions and crucial context for the project.**

### Expected Environment Variables for AI Agents **besides Cursor Background Agents** (for Claude, Codex, etc.)

- FIRECRAWL_API_KEY
- GITHUB_API_TOKEN
- OPENAI_API_KEY
- SUPABASE_API_KEY
- SUPABASE_DATABASE_PASSWORD
- SUPABASE_SERVICE_KEY
- SUPABASE_URL

This is true both for local and production environments.

## Development & Setup

### Running the server and logs watchdog
```bash
# Verify the environment and dependencies are set up correctly.
source ./setup.sh

# Start the server and watchdog in the background. Logs output to file.
start_server_and_watchdog

# Verify the server is running.
print_server_and_watchdog_pids

# Exercise the API with curl requests.
curl http://localhost:5001/api/scrape
curl http://localhost:5001/api/tldr-url
curl ...additional endpoints that may be relevant...

# Stop the server and watchdog.
kill_server_and_watchdog
```


## Client setup

Builds client:
```bash
source setup.sh
```

### Frontend development

For frontend development with hot reload:

```bash
cd client
npm run dev
```

This runs Vite dev server on port 3000 with API proxy to localhost:5001.

#### Testing Client With Playwright

1. Use this browser configuration:
```python
launch_options = {
    'headless': True,
    'args': [
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage',
        '--disable-accelerated-2d-canvas',
        '--no-first-run',
        '--no-zygote',
        '--disable-gpu',
        '--disable-software-rasterizer',
        '--disable-web-security',
        '--disable-features=IsolateOrigins,site-per-process',
        '--disable-blink-features=AutomationControlled',
    ]
}
browser = p.chromium.launch(**launch_options)
context = browser.new_context(
    viewport={"width": 1920, "height": 1080},
    ignore_https_errors=True,
    bypass_csp=True,
)
page = context.new_page()
```

2. Take, download and view screenshots yourself to assess visuals
3. Utilize event monitoring (on "console", "request", "pageerror", ...)
4. Lean on testing real user flows
5. Wait intelligently
    - `wait_until="domcontentloaded"`, not "networkidle"
    - `page.wait_for_selector('body', state="visible")`
    - time.sleep(2~3) for React hydration
6. Leverage CSS classes as distinguishers


### `uv` installation and usage

- Install `uv`:
```bash
source setup.sh
```

Never run Python directly. Always use `uv` to run Python.
Do: `uv run python3 ...`. Do not: `python3 ...`.
Do: `uv run --with=dep1 python3 ...`. Do not: `pip install ...`.

- Use Python via `uv` for quick testing:
```bash
uv run python3 - <<'PY'
import json, sys
print("hello from uv python")
PY
```
- `uv` can transiently install dependencies if you need or consider integrating any:
```bash
uv run --with=dep1,dep2,dep3 python3 - <<'PY'
import dep1, dep2, dep3, os
dep1.do(os.environ["MY_API_KEY"])
PY
```

## Practical guidance

- Trust and Verify: Lean heavily on curling and running transient Python programs in a check-verify-trial-and-error process to make sure you know what you're doing, that you are expecting the right behavior, and to verify assumptions that any particular way of doing something is indeed the right way. This is doubly true when it comes to third-party integrations, third-party libraries, network requests, APIs, the existence and values of environment variables.
- Run `source ./setup.sh` to verify the environment and dependencies are set up correctly. Use `source setup.sh && start_server_and_watchdog` and `source setup.sh && print_server_and_watchdog_pids` to confirm the local server is running. Generously exercise the API with `curl` requests (e.g., `/api/scrape`, `/api/tldr-url`) throughout the development process to catch regressions early. Use `source setup.sh && kill_server_and_watchdog` for cleanup.
- Verify every new behavior, fix or modification you make by utilizing your shell and Playwright. If possible, execute the modified flow to ensure nothing is broken.
- Make note of the various sub agents available to you (.claude/agents/) and use them in the circumstances they describe.


## Development Conventions

1. Do not abbreviate variable, function or class names. Use complete words. Write clean code.
2. Write code that fails early and clearly rather than writing fallbacks to "maybe broken" inputs. Zero "Just in case my inputs are corrupted" code. Fallback-rich code is to be avoided because it explodes complexity and often just silently propagates bugs downstream. Good code assumes that its inputs are valid and complete. It trusts upstream code to have completed its job. This ties closely to separation of concerns. And if something important fails, or an assumption is broken, fail early and clearly. Broken code should be discovered early and loudly and fixed quickly; It should not be tolerated, nor worked around.
3. Write highly cohesive, decoupled logic.
4. Early return from functions when possible.
5. Utilize existing logic when possible. Do not re-implement anything.
6. Write flat, optimized logical branches. Avoid nested, duplicate-y code. Write DRY and elegant logic.
7. Prefer `import modulename` and call `modulename.function()` rather than `from modulename import function`. Namespacing is an easy clarity win. `import os.path; os.path.join(...)` is better than `from os.path import join(...)`.
8. Always use `util.resolve_env_var` to get environment variables.
9. Add a doctest example to pure-ish functions (data in, data out).
10. `util.log` when something is going wrong, even if it is recoverable. Be consistent with existing logging style.

<Bad: fallback-rich, squirmy code>
```py
@app.route("/api/tldr-url", methods=["POST"])
def tldr_url():
    """Requires 'url' in request body"""
    # Unnecessarily defends against broken upstream guarantees.
    data = request.get_json() or {}
    url = data.get("url", "")
    result = tldr_service.tldr_url_content(url) or ""
```
</Bad: fallback-rich, squirmy code>

<Good: straightforward, upstream-trusting code>
```py
@app.route("/api/tldr-url", methods=["POST"])
def tldr_url():
    """Requires 'url' in request body"""
    # Assumes upstream guarantees are upheld (inputs are valid and complete) — thus keeps the state machine simpler.
    # If upstream guarantees are broken (e.g., missing 'url'), we WANT to fail as early as possible (in this case, `data['url']` will throw a KeyError)
    data = request.get_json()
    url = data['url']
    result = tldr_service.tldr_url_content(url)
```
</Good: straightforward, upstream-trusting code>

<Bad: unnecessarily defensive, therefore nested code>
```py
# `MyResponse.words` is an optional list, defaulting to None (`words: list | None = None`).
response: MyResponse = requests.post(...)

# Both checks are redundant:
#  1. `response.words` is guaranteed to exist by the MyResponse model
#  2. `response.words` can be coerced to an empty iterable if it is None instead of checked.
if hasattr(response, 'words') and response.words:
    for word in response.words:
        ...  # Nested indentation level
```
</Bad: unnecessarily defensive, therefore nested code>

<Good: straightforward, confident, flatter code with fewer logical branches>
```py
response: MyResponse = requests.post(...)

# The `or []` is a safe coercion to an empty iterable if `response.words` is None. In the empty case, the loop will not run, which is the desired behavior.
for word in response.words or []:
    ...  # Single indentation level; as safe if not safer than the bad, defensive example above.
```
</Good: straightforward, confident, flatter code with fewer logical branches>

## The Right Engineering Mindset

1. Avoid increasing complexity without a truly justified reason. Each new line of code or logical branch increases complexity. Complexity is the enemy of the project. In your decision-making, ask yourself how might you **REDUCE complexity** in your solution, rather than just solve the immediate problem ad-hoc. Oftentimes, reducing complexity means **removing code**, which is OK. If done right, removing code is beneficial similarly to how clearing Tetris blocks is beneficial — it simplifies and creates more space.
2. Prefer declarative code design over imperative approaches. From a variable to an entire system, if it can be declaratively expressed upfront, do so. People understand things better when they can see the full picture instead of having to dive in. Difficulty arises when flow and logic are embedded implicitly in a sprawling implementation.
3. Avoid over-engineering and excessive abstraction. Abstractions have to be clearly justified. Simplicity and clarity are key.
4. If you're unsure whether your response is correct, that's completely fine—just let me know of your uncertainty and continue responding. We're a team.
5. Do not write comments in code, unless they are critical for understanding. Especially, do not write "journaling" comments saying "modified: foo", "added: bar" or "new implementation", as if to leave a modification trail behind.
6. For simple tasks that could be performed straight away, do not think much. Just do it. For more complex tasks that would benefit from thinking, think deeper, proportionally to the task's complexity. Regardless, always present your final response in a direct and concise manner. No fluff.
7. Do NOT fix linter errors unless instructed by the user to do so.
8. Docstrings should be few and far between. When you do write one, keep it to 1-2 sentences max.

## Crucial Important Rules: How To Approach a Task.

The following points are close to my heart:
1. Before starting your task, you must understand how big the affected scope is. Will the change affect the entire stack & flow, from the db architecture to the client logic? Map out the moving parts and coupling instances before thinking and planning.
2. If you are fixing a bug, hypothesize of the root cause before planning your changes.
3. Plan step-by-step. Account for the moving parts and coupling you found in step (1).
4. When making changes, be absolutely SURGICAL. Every line of code you add incurs a small debt; this debt compounds over time through maintenance costs, potential bugs, and cognitive load for everyone who must understand it later. Therefore, make only laser-focused changes.
4. No band-aid fixes. When encountering a problem, first brainstorm what possible root causes may explain it. band-aid fixes are bad because they increase complexity significantly. Root-cause solutions are good because they reduce complexity.


## Being an Effective AI Agent

1. Know your weaknesses: your eagerness to solve a problem can cause tunnel vision. You may fix the issue but unintentionally create code duplication, deviate from the existing design, or introduce a regression in other coupled parts of the project you didn't consider. The solution is to literally look around beyond the immediate fix, be aware of (and account for) coupling around the codebase, integrate with the existing design, and periodically refactor.
2. You do your best work when you can verify yourself. With self-verification, you can and should practice continuous trial and error instead of a single shot in the dark.

## Documentation

1. YAML frontmatter is automatically updated in CI. Do not manually update it.
2. CLAUDE.md is a read-only exact copy of AGENTS.md. It is generated automatically in CI. It is read-only for you. Any updates should be made in AGENTS.md and not CLAUDE.md.

</file>
<file path="ARCHITECTURE.md">
---
last-updated: 2025-11-14 14:33, e0594d7
last_updated: 2025-11-18 10:35, af7d9f0
---
# TLDRScraper Architecture Documentation

## Overview

TLDRScraper is a newsletter aggregator that scrapes tech newsletters from multiple sources, displays them in a unified interface, and provides AI-powered TLDRs. The architecture follows a React 19 + Vite frontend communicating with a Flask backend via REST API, with all state and cache data persisted server-side in Supabase PostgreSQL.

## Technology Stack

**Frontend:**
- React 19
- Vite (build tool)
- Marked.js (markdown parsing)
- DOMPurify (XSS sanitization)

**Backend:**
- Flask (Python web framework)
- Supabase PostgreSQL (database for all state/cache persistence)
- curl_cffi (web scraping)
- Jina Reader API (web scraping fallback)
- Firecrawl API (web scraping fallback, optional)
- MarkItDown (HTML → Markdown conversion)
- OpenAI GPT-5 (AI TLDRs)

## Architecture Diagram

```plaintext
┌─────────────────────────────────────────────────────────────────────────┐
│                             User Browser                                 │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                       React 19 Application                        │  │
│  │  ┌────────────┐  ┌──────────────┐  ┌──────────────────────────┐  │  │
│  │  │  App.jsx   │  │ Components   │  │    Hooks                 │  │  │
│  │  │            │  │              │  │                          │  │  │
│  │  │  - Root    │  │ - ScrapeForm │  │ - useArticleState        │  │  │
│  │  │  - Hydrate │  │ - CacheToggle│  │ - useSummary             │  │  │
│  │  │  - Results │  │ - Results    │  │ - useSupabaseStorage     │  │  │
│  │  │    Display │  │   Display    │  │                          │  │  │
│  │  │            │  │ - ArticleList│  │ Lib                      │  │  │
│  │  │            │  │ - ArticleCard│  │ - scraper.js             │  │  │
│  │  │            │  │              │  │ - storageApi.js          │  │  │
│  │  └────────────┘  └──────────────┘  └──────────────────────────┘  │  │
│  └───────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────┘
                                    │
                                    │ HTTP REST API
                                    ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                          Flask Backend (Python)                          │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                         serve.py (Routes)                         │  │
│  │  POST /api/scrape             POST /api/tldr-url                 │  │
│  │  GET/POST /api/storage/setting/<key>                             │  │
│  │  GET/POST /api/storage/daily/<date>                              │  │
│  │  POST /api/storage/daily-range                                   │  │
│  │  GET /api/storage/is-cached/<date>                               │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                                    │                                     │
│                                    ▼                                     │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                       tldr_app.py (App Logic)                     │  │
│  │  - scrape_newsletters()    - tldr_url()                           │  │
│  │  - get_tldr_prompt_template()                                    │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                                    │                                     │
│                                    ▼                                     │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                    tldr_service.py (Service Layer)                │  │
│  │  - scrape_newsletters_in_date_range()                             │  │
│  │  - tldr_url_content()                                             │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                 storage_service.py (Storage Layer)                │  │
│  │  - get_setting() / set_setting()                                  │  │
│  │  - get_daily_payload() / set_daily_payload()                      │  │
│  │  - get_daily_payloads_range() / is_date_cached()                  │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│              │                            │                              │
│              ▼                            ▼                              │
│  ┌────────────────────────┐   ┌──────────────────────────────────────┐ │
│  │  newsletter_scraper.py │   │       summarizer.py                  │ │
│  │                        │   │                                      │ │
│  │  - scrape_date_range() │   │  - tldr_url()                       │ │
│  │  - Adapter Factory     │   │  - url_to_markdown()                │ │
│  │                        │   │  - scrape_url()                     │ │
│  │  Uses:                 │   │  - _call_llm()                      │ │
│  │  - TLDRAdapter         │   │                                      │ │
│  │  - HackerNewsAdapter   │   │                                      │ │
│  └────────────────────────┘   └──────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                   Database & External Services                           │
│  ┌──────────────────────────────────────────────────────────────────┐  │
│  │  Supabase PostgreSQL Database                                    │  │
│  │  - settings table (key-value for cache:enabled, etc.)            │  │
│  │  - daily_cache table (JSONB payloads by date)                    │  │
│  └──────────────────────────────────────────────────────────────────┘  │
│  ┌──────────────┐  ┌──────────────┐  ┌────────────────────────────┐   │
│  │  TLDR News   │  │ HackerNews   │  │  OpenAI GPT-5 API          │   │
│  │  Newsletter  │  │  API         │  │  (Summaries & TLDRs)       │   │
│  │  Archives    │  │              │  │                            │   │
│  └──────────────┘  └──────────────┘  └────────────────────────────┘   │
│  ┌──────────────┐  ┌──────────────┐  ┌────────────────────────────┐   │
│  │  Jina Reader │  │  curl_cffi   │  │  Firecrawl API             │   │
│  │  r.jina.ai   │  │  (Chrome)    │  │  api.firecrawl.dev         │   │
│  └──────────────┘  └──────────────┘  └────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## Features & User Interactions

### 1. Newsletter Scraping
**User Action:** Enter start/end dates → Click "Scrape Newsletters"

**Available Interactions:**
- Select date range (max 31 days)
- Submit scrape request
- View progress bar
- View results grouped by date/issue

### 2. Cache Management
**User Action:** Toggle cache checkbox

**Available Interactions:**
- Enable/disable cache
- State persists in Supabase settings table

### 3. Article State Management
**User Action:** Click article link / Remove button / Restore button

**Available Interactions:**
- Click article title → Mark as read
- Click "Remove" → Mark as removed (visual strikethrough)
- Click "Restore" → Restore removed article
- Article states persist in Supabase daily_cache table

### 4. TLDR Generation
**User Action:** Click "TLDR" button on article

**Available Interactions:**
- Click "TLDR" → Fetch TLDR from API
- TLDR displayed inline below article
- Click again → Collapse TLDR (marks as tldrHidden; deprioritized)
- Cached TLDRs show "Available" (green)

### 5. Results Display
**User Action:** View scraped results

**Available Interactions:**
- Articles grouped by: Date → Issue/Category → Section
- Articles sorted by state: Unread → Read → TLDR-hidden → Removed
- Visual state indicators (bold = unread, muted = read, strikethrough = removed)
- Stats display (article count, unique URLs, dates processed)
- Collapsible debug logs

---

## State Machines

### Feature 1: Newsletter Scraping

#### States
1. **idle** - No scraping in progress
2. **validating** - Validating date range input
3. **checking_cache** - Checking if range is fully cached
4. **fetching_api** - Calling backend API
5. **merging_cache** - Merging API results with Supabase cache
6. **complete** - Results displayed
7. **error** - Error occurred

#### State Transitions

```
idle
  │
  ├─ User enters dates
  │    ↓
  │  validating
  │    │
  │    ├─ Valid dates
  │    │    ↓
  │    │  checking_cache
  │    │    │
  │    │    ├─ Fully cached & cache enabled
  │    │    │    ↓
  │    │    │  loading_cache (GET /api/storage/daily-range)
  │    │    │    ↓
  │    │    │  complete (load from Supabase)
  │    │    │
  │    │    └─ Not fully cached OR cache disabled
  │    │         ↓
  │    │       fetching_api
  │    │         │
  │    │         ├─ Success
  │    │         │    ↓
  │    │         │  merging_cache (if cache enabled)
  │    │         │    ↓ POST /api/storage/daily/{date}
  │    │         │  complete
  │    │         │
  │    │         └─ Failure
  │    │              ↓
  │    │            error
  │    │
  │    └─ Invalid dates
  │         ↓
  │       error (validation error)
  │
  └─ (loop back to idle on next interaction)
```

#### Key State Data
- **startDate**: string (ISO date)
- **endDate**: string (ISO date)
- **loading**: boolean
- **progress**: number (0-100)
- **error**: string | null
- **results**: ResultsPayload | null

---

### Feature 2: Cache Management

#### States
1. **enabled** - Cache is active
2. **disabled** - Cache is inactive

#### State Transitions

```
enabled
  │
  ├─ User toggles OFF
  │    ↓
  │  disabled
  │    │
  │    └─ POST /api/storage/setting/cache:enabled {value: false}
  │         ↓ Supabase upsert to settings table
  │
  └─ User toggles ON
       ↓
     enabled
       │
       └─ POST /api/storage/setting/cache:enabled {value: true}
            ↓ Supabase upsert to settings table
```

#### Key State Data
- **enabled**: boolean (reactive, synced to Supabase settings table)
- **loading**: boolean (during database read/write)
- **statusText**: computed string ("(enabled)" | "(disabled)")

---

### Feature 3: Article State Management

#### States (per article)
1. **unread** - Default state, bold text
2. **read** - User clicked/viewed, muted text
3. **removed** - User removed, strikethrough + dashed border

#### State Transitions

```
unread
  │
  ├─ User clicks article link
  │    ↓
  │  read
  │    │
  │    ├─ article.read = { isRead: true, markedAt: timestamp }
  │    │
  │    └─ POST /api/storage/daily/{date} → Supabase upsert
  │
  ├─ User clicks "Remove"
  │    ↓
  │  removed
  │    │
  │    ├─ article.removed = true
  │    │
  │    └─ POST /api/storage/daily/{date} → Supabase upsert
  │
read
  │
  └─ User clicks "Remove"
       ↓
     removed
       │
       ├─ article.removed = true
       │
       └─ POST /api/storage/daily/{date} → Supabase upsert

removed
  │
  └─ User clicks "Restore"
       ↓
     unread (or previous state)
       │
       ├─ article.removed = false
       │
       └─ POST /api/storage/daily/{date} → Supabase upsert
```

#### Key State Data (per article)
- **url**: string (unique identifier)
- **issueDate**: string (storage key component)
- **read**: { isRead: boolean, markedAt: string | null }
- **removed**: boolean

---

### Feature 4: TLDR Generation

#### States (per article TLDR)
1. **unknown** - TLDR not yet requested
2. **creating** - API request in progress
3. **available** - TLDR cached and ready
4. **error** - API request failed

#### State Transitions

```
unknown
  │
  └─ User clicks "TLDR"
       ↓
     creating
       │
       ├─ POST /api/tldr-url { url, summary_effort }
       │
       ├─ Success
       │    ↓
       │  available
       │    │
       │    ├─ tldr.status = 'available'
       │    ├─ tldr.markdown = response.tldr_markdown
       │    ├─ tldr.expanded = true
       │    ├─ Mark article as read
       │    │
       │    └─ POST /api/storage/daily/{date} → Supabase upsert
       │
       └─ Failure
            ↓
          error
            │
            ├─ tldr.status = 'error'
            ├─ tldr.errorMessage = error text
            │
            └─ POST /api/storage/daily/{date} → Supabase upsert

available
  │
  └─ User clicks "Available"
       ↓
     (toggle expanded state, no API call)
```

#### Key State Data (per article)
- **tldr.status**: 'unknown' | 'creating' | 'available' | 'error'
- **tldr.markdown**: string
- **tldr.html**: computed (marked + DOMPurify)
- **tldr.effort**: 'minimal' | 'low' | 'medium' | 'high'
- **tldr.expanded**: boolean (UI state)
- **tldr.errorMessage**: string | null

---

## Call Graphs

### Feature 1: Newsletter Scraping - Complete Flow

#### Client → Backend → External Services

```
User clicks "Scrape Newsletters"
  │
  ├─ ScrapeForm.jsx handleSubmit()
  │    │
  │    ├─ Check validation
  │    │    │
  │    │    └─ If invalid: return early
  │    │
  │    └─ Call scraper.scrape(startDate, endDate)
  │
  └─ scraper.js scrape(startDate, endDate)
       │
       ├─ Reset state:
       │    - loading.value = true
       │    - progress.value = 0
       │    - error.value = null
       │
       ├─ Step 1: Check cache
       │    │
  │    └─ scraper.js isRangeCached(startDate, endDate)
  │         │
  │         ├─ Compute date range: computeDateRange()
  │         │    │
  │         │    └─ Returns: ['2024-01-03', '2024-01-02', '2024-01-01']
  │         │
  │         └─ Check each date in Supabase:
  │              │
  │              └─ GET /api/storage/is-cached/2024-01-01
  │                   │
  │                   ├─ If ALL dates cached AND cacheEnabled = true
  │                   │    │
  │                   │    └─ scraper.js loadFromCache()
  │                   │         │
  │                   │         ├─ POST /api/storage/daily-range
  │                   │         ├─ Build stats: buildStatsFromPayloads()
  │                   │         ├─ Update progress state
  │                   │         │
  │                   │         └─ Return cached results
       │                   │
       │                   └─ If NOT fully cached OR cache disabled
       │                        │
       │                        └─ Continue to API call...
       │
       ├─ Step 2: API Call
       │    │
       │    ├─ progress.value = 50
       │    │
       │    └─ window.fetch('/api/scrape', {
       │         method: 'POST',
       │         body: JSON.stringify({ start_date, end_date })
       │       })
       │         │
       │         └─ Server receives request...
       │              │
       │              ├─ serve.py:32 scrape_newsletters_in_date_range()
       │              │    │
       │              │    ├─ Extract request.get_json()
       │              │    │    - start_date: "2024-01-01"
       │              │    │    - end_date: "2024-01-03"
       │              │    │    - sources: null (optional)
       │              │    │
       │              │    └─ tldr_app.py:9 scrape_newsletters(start_date, end_date, source_ids, excluded_urls=[])
       │              │         │
       │              │         └─ tldr_service.py:43 scrape_newsletters_in_date_range()
       │              │              │
       │              │              ├─ tldr_service.py:17 _parse_date_range()
       │              │              │    │
       │              │              │    ├─ Parse ISO dates
       │              │              │    ├─ Validate: start <= end
       │              │              │    ├─ Validate: range < 31 days
       │              │              │    │
       │              │              │    └─ Return (datetime, datetime)
       │              │              │
       │              │              └─ newsletter_scraper.py:251 scrape_date_range(start_date, end_date, source_ids, excluded_urls)
       │              │                   │
       │              │                   ├─ util.get_date_range(start, end)
       │              │                   │    │
       │              │                   │    └─ Returns list of dates: [date1, date2, date3]
       │              │                   │
       │              │                   ├─ Default sources: NEWSLETTER_CONFIGS.keys()
       │              │                   │    - ['tldr_tech', 'tldr_ai', 'hackernews', ...]
       │              │                   │
       │              │                   ├─ Initialize tracking:
       │              │                   │    - all_articles = []
       │              │                   │    - url_set = set()
       │              │                   │    - issue_metadata_by_key = {}
       │              │                   │
       │              │                   └─ For each date in dates:
       │              │                        │
       │              │                        └─ For each source_id in source_ids:
       │              │                             │
       │              │                             ├─ newsletter_scraper.py:172 _collect_newsletters_for_date_from_source()
       │              │                             │    │
       │              │                             │    ├─ newsletter_scraper.py:16 _get_adapter_for_source(config)
       │              │                             │    │    │
       │              │                             │    │    ├─ If source_id.startswith('tldr_'):
       │              │                             │    │    │    │
       │              │                             │    │    │    └─ Return TLDRAdapter(config)
       │              │                             │    │    │
       │              │                             │    │    └─ If source_id == 'hackernews':
       │              │                             │    │         │
       │              │                             │    │         └─ Return HackerNewsAdapter(config)
       │              │                             │    │
       │              │                             │    └─ adapter.scrape_date(date, excluded_urls)
       │              │                             │         │
       │              │                             │         ├─ TLDRAdapter: Scrapes tldr.tech archives
       │              │                             │         │    │
       │              │                             │         │    ├─ Build URL: f"https://tldr.tech/{newsletter_type}/archives/{date}"
       │              │                             │         │    ├─ HTTP GET request
       │              │                             │         │    ├─ Parse HTML for articles
       │              │                             │         │    ├─ Extract metadata from titles: "(N minute read)" or "(GitHub Repo)" → article_meta field
       │              │                             │         │    ├─ Filter out excluded URLs
       │              │                             │         │    │
       │              │                             │         │    └─ Return { articles: [...], issues: [...] }
       │              │                             │         │
       │              │                             │         └─ HackerNewsAdapter: Scrapes HN API (Algolia)
       │              │                             │              │
       │              │                             │              ├─ Fetch 50 stories from Algolia (pre-filtered by date/score)
       │              │                             │              ├─ Filter out excluded URLs (canonical matching)
       │              │                             │              ├─ Calculate leading scores: (2 × upvotes) + comments
       │              │                             │              ├─ Sort by leading score descending
       │              │                             │              ├─ Convert top stories to articles
       │              │                             │              ├─ Extract metadata: "N upvotes, K comments" → article_meta field
       │              │                             │              │
       │              │                             │              └─ Return { articles: [...], issues: [] }
       │              │                             │
       │              │                             ├─ For each article in result:
       │              │                             │    │
       │              │                             │    ├─ Canonicalize URL
       │              │                             │    ├─ Deduplicate via url_set
       │              │                             │    │
       │              │                             │    └─ Append to all_articles
       │              │                             │
       │              │                             └─ Sleep 0.2s (rate limiting)
       │              │
       │              ├─ newsletter_scraper.py:139 _build_scrape_response()
       │              │    │
       │              │    ├─ Group articles by date
       │              │    ├─ Build markdown output (newsletter_merger.py)
       │              │    ├─ Build issues list
       │              │    ├─ Compute stats
       │              │    │
       │              │    └─ Return {
       │              │         success: true,
       │              │         articles: [...],
       │              │         issues: [...],
       │              │         stats: { total_articles, unique_urls, ... }
       │              │       }
       │              │
       │              └─ Flask jsonify() → HTTP Response
       │
       ├─ Step 3: Process Response
       │    │
  │    └─ scraper.js buildDailyPayloadsFromScrape(data)
  │         │
  │         ├─ Group articles by date
  │         ├─ Group issues by date
  │         │
  │         └─ Build daily payloads: [{
  │              date: "2024-01-01",
  │              articles: [...],
  │              issues: [...],
  │              cachedAt: timestamp
  │            }]
  │
  ├─ Step 4: Merge with Cache (if enabled)
  │    │
  │    └─ scraper.js mergeWithCache(payloads)
  │         │
  │         └─ For each payload:
  │              │
  │              ├─ GET /api/storage/daily/{date}
  │              │    │
  │              │    ├─ If cached data exists:
  │              │    │    │
  │              │    │    └─ Merge articles (preserve summary, tldr, read, removed)
  │              │    │
  │              │    └─ POST /api/storage/daily/{date} (save to Supabase)
  │              │
  │              └─ Return merged payload
  │
  ├─ Step 5: Update State
  │    │
  │    ├─ Update progress state
  │    ├─ Set results state: { success, payloads, source, stats }
  │    │
  │    └─ Return results
  │
  └─ Step 6: Display Results
       │
       └─ ScrapeForm.jsx passes results via callback
            │
            └─ App.jsx handleResults(data)
                 │
                 ├─ Update results state
                 │
                 └─ ResultsDisplay.jsx renders:
                      │
                      ├─ Stats
                      ├─ Debug logs
                      │
                      └─ ArticleList (grouped by date/issue)
                           │
                           └─ ArticleCard (for each article)
```

---

### Feature 4: TLDR Generation - Complete Flow

```
User clicks "TLDR" button
  │
  ├─ ArticleCard.jsx onClick={handleTldrClick}
  │    │
  │    └─ useSummary hook toggle()
  │         │
  │         ├─ Check if TLDR already available
  │         │
  │         └─ useSummary.js fetch(summaryEffort)
  │                   │
  │                   └─ window.fetch('/api/tldr-url', {
  │                        method: 'POST',
  │                        body: JSON.stringify({ url, summary_effort })
  │                      })
  │                        │
  │                        └─ Server receives request...
  │                             │
  │                             ├─ serve.py:68 tldr_url()
  │                             │    │
  │                             │    └─ tldr_app.py:32 tldr_url(url, summary_effort)
  │                             │         │
  │                             │         └─ tldr_service.py:79 tldr_url_content(url, summary_effort)
  │                             │              │
  │                             │              ├─ util.canonicalize_url(url)
  │                             │              │
  │                             │              └─ summarizer.py:291 tldr_url(url, summary_effort)
  │                             │                   │
  │                             │                   ├─ url_to_markdown(url)
  │                             │                   │    [Same flow as summarize]
  │                             │                   │
  │                             │                   ├─ Fetch TLDR prompt template:
  │                             │                   │    │
  │                             │                   │    └─ _fetch_tldr_prompt()
  │                             │                   │         │
  │                             │                   │         └─ Fetch from GitHub:
  │                             │                   │              "https://api.github.com/repos/giladbarnea/llm-templates/contents/text/tldr.md"
  │                             │                   │
  │                             │                   ├─ Build prompt:
  │                             │                   │    template + "\n\n<tldr this>\n" + markdown + "\n</tldr this>"
  │                             │                   │
  │                             │                   └─ Call LLM:
  │                             │                        │
  │                             │                        └─ _call_llm(prompt, summary_effort)
  │                             │                             [Same flow as summarize]
  │                             │
  │                             └─ Return { success, tldr_markdown, canonical_url, summary_effort }
  │
  └─ Client receives response:
       │
       ├─ Update article state:
       │    {
       │      status: 'available',
       │      markdown: result.tldr_markdown,
       │      effort: summaryEffort,
       │      checkedAt: timestamp,
       │      errorMessage: null
       │    }
       │
       ├─ Set expanded state to true
       ├─ Mark article as read (if not already)
       │
       └─ Display inline TLDR
```

---

## Data Structures

### DailyPayload (Supabase: `daily_cache` table, keyed by date)

```typescript
{
  date: string,              // "2024-01-01"
  cachedAt: string,          // ISO timestamp
  articles: Article[],       // Array of articles for this date
  issues: Issue[]            // Array of newsletter issues for this date
}
```

### Article

```typescript
{
  url: string,               // Canonical URL (unique identifier)
  title: string,
  articleMeta: string,       // Metadata extracted from source (e.g., "158 upvotes, 57 comments" or "5 minute read")
  issueDate: string,         // "2024-01-01"
  category: string,          // "TLDR Tech", "HackerNews", etc.
  sourceId: string,          // "tldr_tech", "hackernews"
  section: string | null,    // Section title within newsletter
  sectionEmoji: string | null,
  sectionOrder: number | null,
  newsletterType: string | null,

  // User state
  removed: boolean,
  tldrHidden: boolean,
  read: {
    isRead: boolean,
    markedAt: string | null  // ISO timestamp
  },

  // AI-generated content
  summary: {
    status: 'unknown' | 'creating' | 'available' | 'error',
    markdown: string,
    effort: 'minimal' | 'low' | 'medium' | 'high',
    checkedAt: string | null,
    errorMessage: string | null
  },

  tldr: {
    status: 'unknown' | 'creating' | 'available' | 'error',
    markdown: string,
    effort: 'minimal' | 'low' | 'medium' | 'high',
    checkedAt: string | null,
    errorMessage: string | null
  }
}
```

### Issue

```typescript
{
  date: string,              // "2024-01-01"
  source_id: string,         // "tldr_tech"
  category: string,          // "TLDR Tech"
  title: string | null,      // Issue title
  subtitle: string | null    // Issue subtitle
}
```

### ScrapeRequest (POST /api/scrape)

```typescript
{
  start_date: string,        // "2024-01-01"
  end_date: string,          // "2024-01-03"
  sources?: string[]         // ["tldr_tech", "hackernews"] (optional)
}
```

### ScrapeResponse (API response)

```typescript
{
  success: boolean,
  articles: Article[],       // All articles (flattened)
  issues: Issue[],           // All issues
  stats: {
    total_articles: number,
    unique_urls: number,
    dates_processed: number,
    dates_with_content: number,
    network_fetches: number,
    cache_mode: string,
    debug_logs: string[]
  },
  output: string             // Markdown formatted output
}
```

---

## Component Dependency Graph

```
App.jsx
  │
  ├── CacheToggle.jsx
  │     └── useSupabaseStorage('cache:enabled')
  │           └── GET/POST /api/storage/setting/cache:enabled
  │
  ├── ScrapeForm.jsx
  │     └── scraper.js functions
  │           └── storageApi.js (GET/POST /api/storage/daily/*)
  │
  └── ResultsDisplay.jsx
        │
        └── DailyResults (per date)
              │
              ├── useSupabaseStorage('newsletters:scrapes:{date}')
              │     └── GET/POST /api/storage/daily/{date}
              │
              └── ArticleList.jsx
                    │
                    └── ArticleCard.jsx
                          ├── useArticleState(date, url)
                          │     └── useSupabaseStorage('newsletters:scrapes:{date}')
                          │           └── GET/POST /api/storage/daily/{date}
                          │
                          └── useSummary(date, url)
                                └── useArticleState(date, url)
```

---

## Sequence Diagram: Full Scraping Flow

```mermaid
sequenceDiagram
    participant User
    participant ScrapeForm
    participant useScraper
    participant Supabase
    participant Flask
    participant NewsletterScraper
    participant TLDRAdapter
    participant ExternalAPI

    User->>ScrapeForm: Enter dates & click "Scrape"
    ScrapeForm->>useScraper: scrape(startDate, endDate)

    alt Cache enabled & fully cached
        useScraper->>Supabase: GET /api/storage/is-cached/{date} (for each date)
        Supabase-->>useScraper: All dates cached
        useScraper->>Supabase: POST /api/storage/daily-range
        Supabase-->>useScraper: Return cached payloads
        useScraper-->>ScrapeForm: Return cached results
    else Not fully cached
        useScraper->>Flask: POST /api/scrape {start_date, end_date}
        Flask->>NewsletterScraper: scrape_date_range()

        loop For each date
            loop For each source
                NewsletterScraper->>TLDRAdapter: scrape_date(date)
                TLDRAdapter->>ExternalAPI: GET tldr.tech/archives/{date}
                ExternalAPI-->>TLDRAdapter: HTML content
                TLDRAdapter-->>NewsletterScraper: {articles, issues}
            end
        end

        NewsletterScraper->>NewsletterScraper: Build response & dedupe
        NewsletterScraper-->>Flask: {articles, issues, stats}
        Flask-->>useScraper: JSON response

        useScraper->>useScraper: buildDailyPayloadsFromScrape()

        alt Cache enabled
            useScraper->>Supabase: GET /api/storage/daily/{date} (merge)
            useScraper->>Supabase: POST /api/storage/daily/{date} (save)
        end

        useScraper-->>ScrapeForm: Return results
    end

    ScrapeForm->>User: Display articles
```

---

## Key Algorithms

### 1. Article Sorting Algorithm (ArticleList.jsx)

```javascript
// Sort articles by state (unread → read → tldrHidden → removed), then by original order
function sortArticles(articles) {
  return articles.sort((a, b) => {
    const stateA = getArticleState(a)  // 0=unread, 1=read, 2=tldrHidden, 3=removed
    const stateB = getArticleState(b)

    // Primary sort: by state
    if (stateA !== stateB) return stateA - stateB

    // Secondary sort: preserve original order within same state
    return (a.originalOrder ?? 0) - (b.originalOrder ?? 0)
  })
}
```

### 2. Date Range Computation (scraper.js)

```javascript
// Compute all dates between start and end (inclusive, descending)
function computeDateRange(startDate, endDate) {
  const dates = []
  const start = new Date(startDate)
  const end = new Date(endDate)

  const current = new Date(end)
  while (current >= start) {
    dates.push(current.toISOString().split('T')[0])
    current.setDate(current.getDate() - 1)
  }

  return dates  // ['2024-01-03', '2024-01-02', '2024-01-01']
}
```

### 3. Cache Merge Algorithm (scraper.js)

```javascript
// Merge new scrape results with existing cached data from Supabase
async function mergeWithCache(payloads) {
  const merged = []

  for (const payload of payloads) {
    const existing = await storageApi.getDailyPayload(payload.date)

    if (existing) {
      // Merge: preserve user state (read, removed, tldrHidden) and AI content (tldr)
      const mergedPayload = {
        ...payload,
        articles: payload.articles.map(article => {
          const existingArticle = existing.articles?.find(a => a.url === article.url)
          return existingArticle
            ? { ...article, tldr: existingArticle.tldr,
                read: existingArticle.read, removed: existingArticle.removed, tldrHidden: existingArticle.tldrHidden }
            : article
        })
      }
      await storageApi.setDailyPayload(payload.date, mergedPayload)
      merged.push(mergedPayload)
    } else {
      await storageApi.setDailyPayload(payload.date, payload)
      merged.push(payload)
    }
  }

  return merged
}
```

### 4. URL Deduplication (newsletter_scraper.py:172)

```python
# Deduplicate articles across sources using canonical URLs
url_set = set()
all_articles = []

for article in scraped_articles:
    canonical_url = util.canonicalize_url(article['url'])
    article['url'] = canonical_url

    if canonical_url not in url_set:
        url_set.add(canonical_url)
        all_articles.append(article)
```

---

## Database Schema (Supabase PostgreSQL)

### Table: settings

```sql
CREATE TABLE settings (
  key TEXT PRIMARY KEY,
  value JSONB NOT NULL,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Example row:
{ key: 'cache:enabled', value: true, updated_at: '2024-01-01T12:00:00Z' }
```

### Table: daily_cache

```sql
CREATE TABLE daily_cache (
  date DATE PRIMARY KEY,
  payload JSONB NOT NULL,
  cached_at TIMESTAMPTZ DEFAULT NOW()
);

-- Example row:
{
  date: '2024-01-01',
  payload: {
    date: '2024-01-01',
    cachedAt: '2024-01-01T12:00:00Z',
    articles: [{url, title, read, removed, tldr, ...}, ...],
    issues: [{date, source_id, category, ...}, ...]
  },
  cached_at: '2024-01-01T12:00:00Z'
}
```

### Storage Flow

1. **Initial Scrape**: API response → Build payloads → POST /api/storage/daily/{date} → Supabase upsert
2. **Cache Hit**: GET /api/storage/daily-range → Read from Supabase → Skip scrape API call
3. **User Interaction**: Modify article state → POST /api/storage/daily/{date} → Supabase upsert → Dispatches 'supabase-storage-change' event
4. **Summary/TLDR**: Fetch from API → Update article → POST /api/storage/daily/{date} → Supabase upsert

---

## Error Handling

### Frontend Errors

1. **Validation Errors**
   - Date range > 31 days → Show inline error
   - Start date > end date → Show inline error

2. **Network Errors**
   - API unreachable → Show error message
   - Timeout → Show error message

3. **Summary/TLDR Errors**
   - Scraping failed → summary.status = 'error'
   - LLM API failed → summary.errorMessage = "..."
   - Button shows "Retry" instead of "Available"

### Backend Errors

1. **Scraping Errors**
   - Individual source failures → Log warning, continue with other sources
   - All sources fail → Return partial results

2. **Summary/TLDR Errors**
   - Try multiple scraping methods (curl_cffi → Jina Reader → Firecrawl)
   - Return 502 on network errors
   - Return 500 on LLM errors

---

## Performance Considerations

1. **Caching Strategy**
   - Cache at daily granularity (not per-source)
   - Merge strategy preserves user state and AI content
   - Cache check before every API call

2. **Rate Limiting**
   - 0.2s delay between source scrapes
   - Prevents overwhelming external APIs

3. **Lazy Loading**
   - Summaries/TLDRs fetched on-demand
   - Not included in initial scrape
   - Cached after first fetch

4. **Component Optimization**
   - Scoped CSS modules prevent style leakage
   - useMemo caches derived state
   - Conditional rendering optimizes DOM updates

---

## Security Measures

1. **XSS Prevention**
   - DOMPurify sanitizes all markdown → HTML conversions
   - dangerouslySetInnerHTML only used with sanitized content

2. **CSRF Protection**
   - Same-origin policy (frontend served from same domain)

3. **Input Validation**
   - Date range validation (client + server)
   - URL canonicalization (prevents cache poisoning)

4. **API Key Management**
   - OpenAI API key server-side only
   - GitHub token for private repos (optional)
   - Firecrawl API key for hard-to-scrape sites (optional)

---

## Testing Considerations

### Unit Tests (Frontend)

- `scraper.js`: Date range computation, cache hit/miss logic
- `useArticleState.js`: State mutations (read/unread/removed)
- `useSummary.js`: Toggle expansion, fetch logic

### Integration Tests

- Full scraping flow (API → cache → display)
- Summary generation end-to-end
- Cache merge behavior

### E2E Tests

- User scrapes date range → Views results
- User marks article as read → State persists
- User generates summary → Summary displays and caches

---

## Future Enhancements

1. **Reasoning Effort Selector**
   - Dropdown on summary button (minimal/low/medium/high)
   - Currently hardcoded to "low"

2. **Source Filtering**
   - UI to select which newsletters to scrape
   - Backend already supports `sources` parameter

3. **Export Functionality**
   - Export articles as markdown/JSON
   - Bulk copy summaries

4. **Search & Filter**
   - Full-text search across articles
   - Filter by category/source/state

---

## File Structure

```
TLDRScraper/
├── client/                    # React 19 frontend
│   ├── src/
│   │   ├── App.jsx           # Root component
│   │   ├── main.jsx          # Entry point
│   │   ├── components/       # UI components
│   │   │   ├── ArticleCard.jsx
│   │   │   ├── ArticleList.jsx
│   │   │   ├── CacheToggle.jsx
│   │   │   ├── ResultsDisplay.jsx
│   │   │   └── ScrapeForm.jsx
│   │   ├── hooks/            # Custom React hooks
│   │   │   ├── useArticleState.js
│   │   │   ├── useSupabaseStorage.js
│   │   │   └── useSummary.js
│   │   └── lib/              # Utilities & logic
│   │       ├── scraper.js
│   │       ├── storageApi.js
│   │       └── storageKeys.js
│   ├── index.html
│   ├── vite.config.js
│   └── package.json
│
├── api/                       # Backend entry point
│   └── index.py
│
├── serve.py                   # Flask routes
├── tldr_app.py               # Application logic layer
├── tldr_service.py           # Service layer
├── storage_service.py        # Supabase storage operations
├── supabase_client.py        # Supabase client initialization
├── newsletter_scraper.py     # Scraping orchestration
├── summarizer.py             # URL → Summary/TLDR
├── newsletter_adapter.py     # Base adapter
├── tldr_adapter.py           # TLDR newsletter adapter
├── hackernews_adapter.py     # HackerNews adapter
├── newsletter_merger.py      # Markdown formatting
├── newsletter_config.py      # Source configurations
└── util.py                   # Shared utilities
```

---

## Conclusion

TLDRScraper is a full-stack newsletter aggregator with sophisticated client-side state management, intelligent caching, and AI-powered content summarization. The architecture separates concerns clearly:

- **React hooks** handle reactive state and async storage operations
- **Flask routes** provide clean REST endpoints (scraping + storage)
- **Service/adapter layers** abstract data sources and database operations
- **Supabase PostgreSQL** provides server-side persistence with JSONB storage
- **OpenAI integration** enhances content with AI-powered summaries

The system is designed for extensibility (new newsletter sources via adapters), performance (database caching with cache-first scraping), and user experience (reactive UI with async loading states).

</file>
<file path="BUGS.md">
---
last_updated: 2025-11-19 20:28, bbcedc2
---
# Bugs Encountered

## Third Party

- [ ] Supabase error: `SSL: CERTIFICATE_VERIFY_FAILED`. See [this GitHub comment](https://github.com/supabase/supabase/discussions/29935#discussioncomment-12050763) for potential solution.
## Scraping

### Failed fetching content
- [ ] https://olmocr.allen.ai/blog. Content was returned with no error but empty-ish. Needs JS enabled (25-10-26 7:45AM IST)
- [ ] https://www.gatesnotes.com/home/home-page-topic/reader/three-tough-truths-about-climate Empty content
- [ ] https://x.com/satyanadella/status/1989755076353921404: Enable JavaScript or use a supported browser; disable privacy extensions; then try again. (25-11-17)
- [ ] https://msn.com/en-us/money/savingandinvesting/oracle-has-lost-315-billion-in-market-value-since-announcing-its-300-billion-deal-with-openai/ar-AA1QH6et
</file>
<file path="GOTCHAS.md">
---
last_updated: 2025-11-18 10:35, af7d9f0
---
# Gotchas

This document catalogs recurring pitfalls in various topics, including managing client-side state persistence and reactivity, surprising design decisions, and so on.

---

#### 2025-11-17: Child component bypassing state management layer causes infinite API hammering

session-id: 892fa714-0087-4c5a-9930-cffdfc5f5359

**Desired behavior that didn't work**: Just browsing the app should load data once per date, then remain quiet until user interaction.

**What actually happened and falsified original thesis**: Continuous machine-gun API requests to GET /api/storage/daily/{date} for the same dates, hammering the server non-stop. <turned-out-to-be-wrong>We initially assumed it was a React re-render loop from unstable useEffect dependencies (e.g., `defaultValue` object creating new references). Then we thought it was multiple `useSupabaseStorage` hook instances all mounting simultaneously (20+ ArticleCards per date). We added global read deduplication to prevent concurrent requests, which helped but didn't stop the hammering. We had wrongly assumed the hooks were the problem.</turned-out-to-be-wrong>

**Cause & Fix**: `ArticleList.jsx` had a useEffect that directly called `storageApi.getDailyPayload(article.issueDate)` for every article (20+ API calls per date), completely bypassing the `useSupabaseStorage` hook abstraction. This useEffect ran whenever the `articles` prop changed. The parent component recreated the `articles` array on every render (via `.map()`), creating a new reference and triggering the useEffect again. Result: infinite loop of 20+ API calls per render. The entire useEffect was redundant - article states were already being synced via `useSupabaseStorage` in parent components. The fix was to delete the broken useEffect entirely and let ArticleList simply sort the articles it receives from props. **Key lesson**: When you build a proper data management layer (custom hooks), don't bypass it by directly calling storage APIs in child components. Child components should consume data from props, not fetch it themselves. Breaking this rule creates duplicate data fetching, races, and infinite loops.

---

#### 2025-11-15 `750f83e`: Concurrent TLDR updates race to overwrite each other

**Desired behavior that didn't work**: When two articles' TLDR buttons are clicked simultaneously, both TLDRs should be fetched and stored independently.

**What actually happened and falsified original thesis**: One article showed "Available" state but with no content, then clicking "Available" triggered a new TLDR request instead of displaying the cached result. We had wrongly assumed React's state would handle concurrent setValueAsync calls correctly.

**Cause & Fix**: Classic read-modify-write race condition. Both updates captured the same stale `value` from the closure, so the second write overwrote the first, losing one article's TLDR data. The fix was to use a ref (valueRef) to track the latest state, ensuring each concurrent update operates on current data instead of stale closure captures.

---

#### 2025-11-06: useLocalStorage hook instances race to overwrite each other

**Desired behavior that didn't work**: Removed articles should persist their removed state after page refresh.

**What actually happened and falsified original thesis**: Article showed "Restore" button immediately after removal, but after refresh showed "Remove" button. We had wrongly assumed one useLocalStorage instance per key would prevent conflicts.

**Cause & Fix**: Multiple useLocalStorage hook instances (one per ArticleCard) each owned their own copy of the payload. When one instance stored an update, other instances later wrote their stale copy back, erasing the change. Rewrote useLocalStorage to use useSyncExternalStore so every subscriber reads and writes through a single source of truth, dramatically simplifying the flow and eliminating the race.

---

#### 2025-11-04 `102a8dcd`: HackerNews articles not displayed in UI because of surprising server response shape

**Desired behavior that didn't work**: HackerNews articles fetched by backend should appear in the UI.

**What actually happened and falsified original thesis**: HackerNews articles were fetched (183 articles in API response) but invisible in the UI. We had wrongly assumed `articles` field alone was sufficient for display.

**Cause & Fix**: The frontend requires both `articles` and `issues` arrays. It only displays articles that match an issue's category. HackerNews adapter returned empty `issues` array, so all HN articles were filtered out during rendering. The fix was to generate fake issue objects for each HackerNews category.

---

#### 2025-10-31 `3bfceee`: State property lost during cache merge

**Desired behavior that didn't work**: When hiding a TLDR, the article should move to bottom so users can deprioritize completed items.

**What actually happened and falsified original thesis**: The article stayed in place. We had wrongly assumed that saving the state property to storage was sufficient.

**Cause & Fix**: The merge function wasn't transferring the new property from cached data. The fix was to add the missing property to the merge operation.

---

#### 2025-10-31 `16bd653`: Component not reactive to storage changes

**Desired behavior that didn't work**: When state changes in storage, the list should re-sort so visual order reflects current state.

**What actually happened and falsified original thesis**: The list used stale prop values. We had wrongly assumed that components automatically react to storage mutations.

**Cause & Fix**: Computed properties only track their declared dependencies. The fix was to dispatch custom events on storage writes and listen for them in consuming components.

---

</file>
<file path="PROJECT_STRUCTURE.md">
.
├── .claude
│  ├── agents
│  │  ├── codebase-analyzer-narrow.md
│  │  ├── codebase-analyzer.md
│  │  ├── codebase-locator.md
│  │  ├── codebase-pattern-finder.md
│  │  └── web-deep-researcher.md
│  ├── commands
│  │  ├── architecture
│  │  │  ├── create.md
│  │  │  ├── sync_current_changes.md
│  │  │  └── sync_since_last_updated.md
│  │  ├── create_plan_lite.md
│  │  ├── implement_plan.md
│  │  ├── research_codebase.md
│  │  └── research_codebase_nt.md
│  └── settings.json
├── .gitattributes
├── .githooks
│  ├── post-checkout
│  ├── post-merge
│  ├── post-rewrite
│  ├── pre-merge-commit
│  ├── README.md
│  └── sync-upstream-suggestions.md
├── .github
│  └── workflows
│     ├── copy-agents-to-claude.yml.deprecated
│     ├── maintain-documentation.yml
│     ├── update-doc-frontmatter.yml.deprecated
│     ├── update-project-structure.yml.deprecated
│     └── WORKFLOW_DIAGRAM.md
├── .gitignore
├── .vercelignore
├── AGENTS.md
├── anthropic_adapter.py
├── api
│  └── index.py
├── ARCHITECTURE.md
├── BUGS.md
├── bytebytego_adapter.py
├── CLAUDE.md
├── client
│  ├── .gitignore
│  ├── index.html
│  ├── package-lock.json
│  ├── package.json
│  ├── README.md
│  ├── src
│  │  ├── App.css
│  │  ├── App.jsx
│  │  ├── components
│  │  │  ├── ArticleCard.css
│  │  │  ├── ArticleCard.jsx
│  │  │  ├── ArticleList.css
│  │  │  ├── ArticleList.jsx
│  │  │  ├── CacheToggle.css
│  │  │  ├── CacheToggle.jsx
│  │  │  ├── ResultsDisplay.css
│  │  │  ├── ResultsDisplay.jsx
│  │  │  ├── ScrapeForm.css
│  │  │  └── ScrapeForm.jsx
│  │  ├── hooks
│  │  │  ├── useArticleState.js
│  │  │  ├── useSummary.js
│  │  │  └── useSupabaseStorage.js
│  │  ├── lib
│  │  │  ├── scraper.js
│  │  │  ├── storageApi.js
│  │  │  └── storageKeys.js
│  │  └── main.jsx
│  └── vite.config.js
├── cloudflare_adapter.py
├── danluu_adapter.py
├── deepmind_adapter.py
├── GOTCHAS.md
├── hackernews_adapter.py
├── hillel_wayne_adapter.py
├── infoq_adapter.py
├── jessitron_adapter.py
├── lenny_newsletter_adapter.py
├── martin_fowler_adapter.py
├── netflix_adapter.py
├── newsletter_adapter.py
├── newsletter_config.py
├── newsletter_merger.py
├── newsletter_scraper.py
├── node_weekly_adapter.py
├── package-lock.json
├── package.json
├── pointer_adapter.py
├── pragmatic_engineer_adapter.py
├── pyproject.toml
├── react_status_adapter.py
├── README.md
├── requirements.txt
├── SCREENSHOTTING_APP.md
├── scripts
│  ├── generate_context.py
│  ├── markdown_frontmatter.py
│  ├── print_root_markdown_files.sh
│  ├── resolve_quiet_setting.sh
│  └── update_doc_frontmatter.py
├── serve.py
├── setup-hooks.sh
├── setup.sh
├── simon_willison_adapter.py
├── softwareleadweekly_adapter.py
├── storage_service.py
├── stripe_engineering_adapter.py
├── summarizer.py
├── supabase_client.py
├── tests
│  ├── browser-automation
│  │  ├── test_phase6_supabase.py
│  │  └── test_tldr_loading_state_bug.py
│  ├── test_phase6_e2e.py
│  └── unit
│     └── test_canonicalize_url.py
├── thoughts
│  ├── 2025-11-08-migrate-client-localstorage-to-server-supabase
│  │  ├── implementation
│  │  │  ├── phase-1.md
│  │  │  ├── phase-2.md
│  │  │  ├── phase-3.md
│  │  │  ├── phase-4.md
│  │  │  ├── phase-5.md
│  │  │  ├── phase-6.md
│  │  │  └── phase-7.md
│  │  ├── manual-browser-testing.md
│  │  ├── plans
│  │  │  └── localstorage-to-supabase-migration.md
│  │  └── research
│  │     └── supabase-database.md
│  └── done
│     ├── 25-10-27-hackernews-integration
│     │  ├── plan.md
│     │  └── research.md
│     ├── 25-10-28-fix-cache-ui-state-sync
│     │  └── plan.md
│     ├── 25-10-30-multi-newsletter
│     │  └── plan.md
│     ├── 25-10-31-vue-to-react-19-migration
│     │  └── plan.md
│     ├── 25-11-04-code-duplication
│     │  ├── plan-issue-a-section-parsing.md
│     │  ├── plan-issue-b-localstorage-keys.md
│     │  ├── plan-issue-c-article-normalization.md
│     │  └── plan.md
│     ├── 25-11-04-mixed-concerns-refactor
│     │  ├── plan-issue-b-extract-build-scrape-response.md
│     │  ├── plan-issue-c-eliminate-duplicate-parsing.md
│     │  └── plan.md
│     └── 25-11-04-remove-summarize
│        └── plan.md
├── tldr_adapter.py
├── tldr_app.py
├── tldr_service.py
├── TLDRScraper.code-workspace
├── util.py
├── uv.lock
├── vercel.json
├── will_larson_adapter.py
└── xeiaso_adapter.py

</file>
<file path="README.md">
---
last_updated: 2025-11-14 16:24, 722a1a0
---
# TLDRScraper

Newsletter aggregator that scrapes tech newsletters from multiple sources, displays them in a unified interface, and provides AI-powered TLDRs.

## Architecture

- **Frontend**: React 19 + Vite (in `client/`)
- **Backend**: Flask + Python (serverless on Vercel)
- **AI**: OpenAI GPT-5 for TLDRs

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed flows & user interactions documentation and [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) for a map of the project structure.

## Development

## Development & Setup

### Running the server and logs watchdog
```bash
# Verify the environment and dependencies are set up correctly.
source ./setup.sh

# Start the server and watchdog in the background. Logs output to file.
start_server_and_watchdog

# Verify the server is running.
print_server_and_watchdog_pids

# Exercise the API with curl requests.
curl http://localhost:5001/api/scrape
curl http://localhost:5001/api/tldr-url
curl ...additional endpoints that may be relevant...

# Stop the server and watchdog.
kill_server_and_watchdog
```


## Client setup

```bash
cd client
npm install
npm run build
npm run dev
```

### Frontend development

For frontend development with hot reload:

```bash
cd client
npm run dev
```

This runs Vite dev server on port 3000 with API proxy to localhost:5000.


### `uv` installation and usage

- Install `uv` and use Python via `uv`:
```bash
source setup.sh
ensure_uv
uv --version
```

## Vercel Deployment

### How It Works

The application is deployed to Vercel as a Python serverless function with a built React frontend:

1. **Build Phase** (`buildCommand` in `vercel.json`):
   - `cd client && npm install && npm run build`
   - Builds React app

2. **Install Phase** (automatic):
   - Vercel auto-detects `requirements.txt`
   - Installs Python dependencies for the serverless function

3. **Runtime**:
   - `/api/index.py` imports the Flask app from `serve.py`
   - All routes (`/`, `/api/*`) are handled by the Python serverless function
   - Flask serves the built React app from `client/static/dist/`
   - API endpoints process requests

### Key Configuration Files

#### `vercel.json`
```json
{
  "buildCommand": "cd client && npm install && npm run build",
  "outputDirectory": "static/dist",
  "rewrites": [
    { "source": "/(.*)", "destination": "/api/index" }
  ]
}
```

- **buildCommand**: Builds the React frontend
- **outputDirectory**: Points to where React builds output (matches `client/vite.config.js` outDir)
- **rewrites**: Routes all requests to the Python serverless function

#### `api/index.py`
```python
import sys
import os

# Add parent directory to path so we can import serve.py and other modules
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from serve import app as app
```

This is the Vercel serverless function entry point. The path manipulation is required because Vercel's Python runtime doesn't automatically add the parent directory to `sys.path`.

#### `serve.py`
```python
# Configure Flask to serve React build output
app = Flask(
    __name__,
    static_folder='static/dist/assets',
    static_url_path='/assets'
)

@app.route("/")
def index():
    """Serve the React app"""
    static_dist = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'static', 'dist')
    return send_from_directory(static_dist, 'index.html')
```

Flask is configured to:
- Serve static assets from `static/dist/assets` at `/assets/*`
- Serve the React app's `index.html` at the root `/`
- Handle API routes at `/api/*`

### Deployment Requirements

1. **React build output** must be in `client/static/dist/` (configured in `client/vite.config.js`)
2. **Python dependencies** are managed by `uv` and must manually be added to `requirements.txt` (Vercel auto-installs)
3. **Module imports** in `api/index.py` must handle parent directory path
4. **Flask static configuration** must point to built React assets

### Common Vercel Deployment Issues

**Issue**: `pip: command not found`
- **Cause**: Explicit `installCommand` in vercel.json trying to run pip in Node.js context
- **Solution**: Remove `installCommand` - Vercel auto-installs from requirements.txt

**Issue**: `No Output Directory named "public" found`
- **Cause**: Vercel looking for default output directory
- **Solution**: Add `"outputDirectory": "static/dist"` to vercel.json

**Issue**: `404 for /api/index`
- **Cause**: Python module import failing in serverless function
- **Solution**: Add parent directory to sys.path in api/index.py

## Documentation

- [ARCHITECTURE.md](ARCHITECTURE.md) - Detailed flows & user interactions documentation
- [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) - Map of the project structure
- [GOTCHAS.md](GOTCHAS.md) - Documented solved tricky past bugs
- [BUGS.md](BUGS.md) - Known issues
</file>
<file path="SCREENSHOTTING_APP.md">
---
last_updated: 2025-11-18 10:35, af7d9f0
---
# How to Get Screenshots of the App from Remote

**Setup:** User has ngrok endpoint at `https://josue-ungreedy-unphysically.ngrok-free.dev/`

## Steps

1. **Verify Playwright is installed locally:**
```bash
curl -X POST https://josue-ungreedy-unphysically.ngrok-free.dev/ -d "ls ~/Library/Caches/ms-playwright"
```

2. **Create and run Playwright script via heredoc:**
```bash
curl -X POST https://josue-ungreedy-unphysically.ngrok-free.dev/ -d "cat > /tmp/screenshot.js << 'EOF'
const {chromium} = require('playwright');
(async () => {
  const browser = await chromium.launch({headless: true});
  const page = await browser.newPage();
  await page.setViewport({width: 1920, height: 1080});
  await page.goto('http://localhost:3000', {waitUntil: 'domcontentloaded', timeout: 30000});
  await page.waitForSelector('body');
  await new Promise(r => setTimeout(r, 4000));
  await page.screenshot({path: '/tmp/tldr_local.png', fullPage: true});
  await browser.close();
  console.log('Screenshot saved');
})();
EOF
cd ~/dev/TLDRScraper && node /tmp/screenshot.js && ls -lh /tmp/tldr_local.png" -s
```

3. **Transfer screenshot via Git:**
```bash
# Create temporary branch
curl -X POST https://josue-ungreedy-unphysically.ngrok-free.dev/ -d "cd ~/dev/TLDRScraper && git checkout -b screenshots-$(date +%s)" -s

# Copy, commit, and push
curl -X POST https://josue-ungreedy-unphysically.ngrok-free.dev/ -d "cd ~/dev/TLDRScraper && cp /tmp/tldr_local.png ./screenshot.png && git commit -am 'Add screenshot' && git push --set-upstream origin HEAD 2>&1 | grep -E 'screenshots-|branch'" -s
```

Note the branch name from output (e.g., `screenshots-1763449916`)

4. **Download screenshot via GitHub raw URL:**
```bash
curl -s "https://raw.githubusercontent.com/giladbarnea/TLDRScraper/screenshots-1763449916/screenshot.png" -o /tmp/tldr_screenshot.png
file /tmp/tldr_screenshot.png  # Verify it's a valid PNG
```

5. **Display in conversation:** Use Read tool on `/tmp/tldr_screenshot.png`

## Cleanup

**Remote machine:**
```bash
# Remove temp files
curl -X POST https://josue-ungreedy-unphysically.ngrok-free.dev/ -d "rm /tmp/tldr_local.png /tmp/screenshot.js" -s

# Delete local git branch
curl -X POST https://josue-ungreedy-unphysically.ngrok-free.dev/ -d "cd ~/dev/TLDRScraper && git checkout - && git branch -D screenshots-1763449916" -s
```

**Remote GitHub:**
```bash
# Delete remote branch (requires valid GITHUB_API_TOKEN)
curl -X DELETE "https://api.github.com/repos/giladbarnea/TLDRScraper/git/refs/heads/screenshots-1763449916" \
  -H "Authorization: token ${GITHUB_API_TOKEN}" -s
```

**Local machine:**
```bash
rm /tmp/tldr_screenshot.png /tmp/screenshot_script.js
```

## Notes

* **Heredoc vs inline:** Heredoc approach (`cat > file << 'EOF'`) works better than inline escaped JavaScript - avoids quote escaping hell
* **File upload services:** `file.io` and `transfer.sh` timeout with ngrok (ERR_NGROK_3004) - likely a bug in the remoteshell server handling long-running commands
* **Git method:** Most reliable for this setup despite extra steps

</file>
<file path="client/README.md">
---
last_updated: 2025-11-14 16:24, 722a1a0
---
# Vue 3 + Vite

This template should help get you started developing with Vue 3 in Vite. The template uses Vue 3 `<script setup>` SFCs, check out the [script setup docs](https://v3.vuejs.org/api/sfc-script-setup.html#sfc-script-setup) to learn more.

Learn more about IDE Support for Vue in the [Vue Docs Scaling up Guide](https://vuejs.org/guide/scaling-up/tooling.html#ide-support).

</file>
</files>

================================================================================
                              SERVER CONTEXT
================================================================================

Processed Python files (full content)
Total: 33 files
<files>
<file path="anthropic_adapter.py">
"""
Anthropic Research adapter implementation.

This adapter scrapes research articles from Anthropic's website (anthropic.com/research).
Since the site uses client-side rendering, we maintain a curated list of recent articles
and their publication dates, updated periodically.
"""

import logging
from datetime import datetime
import requests

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("anthropic_adapter")


class AnthropicAdapter(NewsletterAdapter):
    """Adapter for Anthropic Research articles."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)
        self.base_url = "https://www.anthropic.com"

        self.known_articles = [
            {
                "title": "Alignment faking in large language models",
                "url": "/research/alignment-faking",
                "date": "2025-01-15",
                "description": "Research on alignment faking in AI large language models"
            },
            {
                "title": "Anthropic Economic Index: September 2025 report",
                "url": "/research/anthropic-economic-index-september-2025-report",
                "date": "2025-09-15",
                "description": "Economic impact analysis of AI adoption"
            },
            {
                "title": "Building AI cyber defenders",
                "url": "/research/building-ai-cyber-defenders",
                "date": "2025-08-20",
                "description": "Research on AI-powered cybersecurity"
            },
            {
                "title": "Economic Index: Geographic adoption patterns",
                "url": "/research/economic-index-geography",
                "date": "2025-07-10",
                "description": "Geographic patterns in AI adoption"
            },
            {
                "title": "Impact on software development",
                "url": "/research/impact-software-development",
                "date": "2025-06-15",
                "description": "AI's impact on software development workflows"
            },
            {
                "title": "Tracing thoughts in language models",
                "url": "/research/tracing-thoughts-language-model",
                "date": "2025-05-12",
                "description": "Research on language model interpretability"
            },
            {
                "title": "Visible extended thinking",
                "url": "/research/visible-extended-thinking",
                "date": "2025-04-18",
                "description": "Making AI reasoning processes more transparent"
            },
            {
                "title": "SWE-bench with Sonnet",
                "url": "/research/swe-bench-sonnet",
                "date": "2025-03-22",
                "description": "Software engineering benchmark results"
            },
            {
                "title": "Mapping the mind of a language model",
                "url": "/research/mapping-mind-language-model",
                "date": "2025-02-14",
                "description": "Research on language model internal representations"
            },
            {
                "title": "Many-shot jailbreaking",
                "url": "/research/many-shot-jailbreaking",
                "date": "2025-01-25",
                "description": "Security research on prompt injection attacks"
            },
        ]

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch Anthropic research articles for a specific date.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date = datetime.fromisoformat(util.format_date_for_url(date))
        target_date_str = target_date.strftime("%Y-%m-%d")

        logger.info(f"[anthropic_adapter.scrape_date] Checking articles for {target_date_str} (excluding {len(excluded_urls)} URLs)")

        try:
            for known_article in self.known_articles:
                article_date = known_article.get('date', '')

                if article_date != target_date_str:
                    continue

                url = known_article.get('url', '')
                if not url:
                    continue

                full_url = f"{self.base_url}{url}"
                canonical_url = util.canonicalize_url(full_url)

                if canonical_url in excluded_set:
                    continue

                article = self._create_article(known_article, target_date_str)
                if article:
                    articles.append(article)

            logger.info(f"[anthropic_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[anthropic_adapter.scrape_date] Error processing articles: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': self.config.category_display_names.get('research', 'Anthropic Research'),
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _create_article(self, known_article: dict, date: str) -> dict | None:
        """Convert known article data to article dict.

        Args:
            known_article: Known article dictionary with title, url, description
            date: Date string

        Returns:
            Article dictionary or None if article should be skipped
        """
        title = known_article.get('title', '').strip()
        url = known_article.get('url', '').strip()
        description = known_article.get('description', '').strip()

        if not title or not url:
            return None

        full_url = f"{self.base_url}{url}"

        return {
            "title": title,
            "article_meta": description[:150] if description else "",
            "url": full_url,
            "category": self.config.category_display_names.get('research', 'Anthropic Research'),
            "date": date,
            "newsletter_type": "research",
            "removed": False,
        }

</file>
<file path="bytebytego_adapter.py">
"""
ByteByteGo newsletter adapter using RSS feed.

This adapter fetches articles from ByteByteGo's newsletter via the RSS feed,
filtering by date and extracting article metadata.
"""

import logging
import re
from datetime import datetime
import requests
import feedparser

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("bytebytego_adapter")


class ByteByteGoAdapter(NewsletterAdapter):
    """Adapter for ByteByteGo newsletter using RSS feed."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)
        self.feed_url = "https://blog.bytebytego.com/feed"

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch newsletter articles for a specific date from RSS feed.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date = datetime.fromisoformat(util.format_date_for_url(date))
        target_date_str = target_date.strftime("%Y-%m-%d")

        logger.info(f"[bytebytego_adapter.scrape_date] Fetching articles for {target_date_str} (excluding {len(excluded_urls)} URLs)")

        try:
            response = requests.get(self.feed_url, timeout=10)
            response.raise_for_status()

            feed = feedparser.parse(response.content)

            logger.info(f"[bytebytego_adapter.scrape_date] Fetched {len(feed.entries)} total entries from feed")

            for entry in feed.entries:
                if not entry.get('published_parsed'):
                    continue

                entry_date = datetime(*entry.published_parsed[:6])
                entry_date_str = entry_date.strftime("%Y-%m-%d")

                if entry_date_str != target_date_str:
                    continue

                link = entry.get('link', '')
                if not link:
                    continue

                canonical_url = util.canonicalize_url(link)

                if canonical_url in excluded_set:
                    continue

                article = self._entry_to_article(entry, target_date_str)
                if article:
                    articles.append(article)

            logger.info(f"[bytebytego_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[bytebytego_adapter.scrape_date] Error fetching feed: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': self.config.category_display_names.get('newsletter', 'ByteByteGo'),
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _strip_html(self, html: str) -> str:
        """Strip HTML tags from text.

        >>> adapter = ByteByteGoAdapter(None)
        >>> adapter._strip_html("<p>Hello <b>world</b></p>")
        'Hello world'
        """
        text = re.sub(r'<[^>]+>', '', html)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _entry_to_article(self, entry: dict, date: str) -> dict | None:
        """Convert RSS feed entry to article dict.

        Args:
            entry: feedparser entry dictionary
            date: Date string

        Returns:
            Article dictionary or None if entry should be skipped
        """
        title = entry.get('title', '')
        if not title:
            return None

        link = entry.get('link', '')
        if not link:
            return None

        summary = entry.get('summary', '')
        summary_text = self._strip_html(summary) if summary else ''

        if summary_text:
            if len(summary_text) > 200:
                summary_text = summary_text[:200] + '...'

        article_meta = summary_text if summary_text else ""

        return {
            "title": title,
            "article_meta": article_meta,
            "url": link,
            "category": self.config.category_display_names.get('newsletter', 'ByteByteGo'),
            "date": date,
            "newsletter_type": "newsletter",
            "removed": False,
        }

</file>
<file path="cloudflare_adapter.py">
"""
Cloudflare Blog adapter using RSS feed.

Cloudflare Blog (blog.cloudflare.com) covers deep technical topics on networking,
security, edge computing, and performance optimization. High-quality technical content
from a leading CDN and security provider.
"""

import logging
import re
from datetime import datetime
import requests
import feedparser

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("cloudflare_adapter")

RSS_FEED_URL = "https://blog.cloudflare.com/rss/"


class CloudflareAdapter(NewsletterAdapter):
    """Adapter for Cloudflare Blog using RSS feed."""

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch Cloudflare blog posts for a specific date using RSS feed.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary with articles and issues
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date_str = util.format_date_for_url(date)
        target_date = datetime.fromisoformat(target_date_str).date()

        logger.info(f"[cloudflare_adapter.scrape_date] Fetching RSS feed for {target_date_str}")

        try:
            response = requests.get(RSS_FEED_URL, timeout=10)
            response.raise_for_status()

            feed = feedparser.parse(response.content)

            if not feed.entries:
                logger.warning(f"[cloudflare_adapter.scrape_date] No entries found in RSS feed")
                return self._normalize_response([], [])

            logger.info(f"[cloudflare_adapter.scrape_date] Fetched {len(feed.entries)} entries from RSS")

            for entry in feed.entries:
                article = self._parse_rss_entry(entry, target_date, excluded_set)
                if article:
                    articles.append(article)

            logger.info(f"[cloudflare_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[cloudflare_adapter.scrape_date] Error fetching RSS feed: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': 'Cloudflare Blog',
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _parse_rss_entry(self, entry: dict, target_date: datetime.date, excluded_set: set) -> dict | None:
        """Parse RSS entry into article dict if it matches the target date.

        Args:
            entry: feedparser entry dict
            target_date: Target date to filter by
            excluded_set: Set of canonical URLs to exclude

        Returns:
            Article dictionary or None if entry should be skipped
        """
        if not entry.get('link'):
            return None

        canonical_url = util.canonicalize_url(entry['link'])
        if canonical_url in excluded_set:
            return None

        published_parsed = entry.get('published_parsed')
        if not published_parsed:
            return None

        entry_date = datetime(*published_parsed[:3]).date()

        if entry_date != target_date:
            return None

        title = entry.get('title', 'Untitled')
        description = entry.get('description', '')

        description_text = self._strip_html(description) if description else ''

        if description_text:
            if len(description_text) > 150:
                description_text = description_text[:150] + '...'
            article_meta = description_text
        else:
            article_meta = ""

        return {
            "title": title,
            "article_meta": article_meta,
            "url": entry['link'],
            "category": "Cloudflare Blog",
            "date": util.format_date_for_url(entry_date),
            "newsletter_type": "blog",
            "removed": False,
        }

    def _strip_html(self, html: str) -> str:
        """Strip HTML tags from text.

        >>> from newsletter_config import NEWSLETTER_CONFIGS
        >>> adapter = CloudflareAdapter(NEWSLETTER_CONFIGS['cloudflare'])
        >>> adapter._strip_html("<p>Hello <b>world</b></p>")
        'Hello world'
        """
        text = re.sub(r'<[^>]+>', '', html)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

</file>
<file path="danluu_adapter.py">
"""
Dan Luu blog adapter implementation using RSS feed.

This adapter implements the NewsletterAdapter interface for Dan Luu's blog,
fetching articles from the RSS feed and filtering them by publication date.
"""

import logging
import xml.etree.ElementTree as ET
from datetime import datetime

import requests

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("danluu_adapter")


class DanLuuAdapter(NewsletterAdapter):
    """Adapter for Dan Luu's blog using RSS feed."""

    def __init__(self, config):
        """Initialize with config.

        Note: We don't use the HTML-to-markdown functionality from base class.
        """
        super().__init__(config)
        self.rss_url = "https://danluu.com/atom.xml"

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch Dan Luu blog articles from RSS feed for a specific date.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date = datetime.fromisoformat(util.format_date_for_url(date))

        logger.info(f"[danluu_adapter.scrape_date] Fetching articles for {date} from RSS feed (excluding {len(excluded_urls)} URLs)")

        try:
            feed_items = self._fetch_rss_feed()

            logger.info(f"[danluu_adapter.scrape_date] Fetched {len(feed_items)} total items from RSS feed")

            # Filter items by date and excluded URLs
            filtered_items = []
            for item in feed_items:
                pub_date = self._parse_pub_date(item.get('pubDate', ''))
                if pub_date is None:
                    continue

                # Check if article was published on target date
                if pub_date.date() != target_date.date():
                    continue

                # Check if URL is excluded
                url = item.get('link', '')
                if not url:
                    continue

                canonical_url = util.canonicalize_url(url)
                if canonical_url not in excluded_set:
                    filtered_items.append(item)

            logger.info(f"[danluu_adapter.scrape_date] {len(filtered_items)} articles match date {date}")

            # Convert to article format
            for item in filtered_items:
                article = self._rss_item_to_article(item, date)
                if article:
                    articles.append(article)

            logger.info(f"[danluu_adapter.scrape_date] Converted {len(articles)} items to articles")

        except Exception as e:
            logger.error(f"[danluu_adapter.scrape_date] Error fetching RSS feed: {e}", exc_info=True)

        # Create issue metadata if we have articles
        issues = []
        if articles:
            issues.append({
                'date': util.format_date_for_url(date),
                'source_id': self.config.source_id,
                'category': 'Dan Luu',
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _fetch_rss_feed(self) -> list[dict]:
        """Fetch and parse the RSS feed.

        Returns:
            List of item dictionaries with title, link, pubDate, description
        """
        response = requests.get(self.rss_url, timeout=30, headers={
            'User-Agent': self.config.user_agent
        })
        response.raise_for_status()

        root = ET.fromstring(response.content)
        channel = root.find('channel')

        if channel is None:
            logger.warning("[danluu_adapter._fetch_rss_feed] No channel found in RSS feed")
            return []

        items = []
        for item_elem in channel.findall('item'):
            title = item_elem.find('title')
            link = item_elem.find('link')
            pub_date = item_elem.find('pubDate')
            description = item_elem.find('description')

            items.append({
                'title': title.text if title is not None else '',
                'link': link.text if link is not None else '',
                'pubDate': pub_date.text if pub_date is not None else '',
                'description': description.text if description is not None else '',
            })

        return items

    def _parse_pub_date(self, pub_date_str: str) -> datetime | None:
        """Parse RSS pubDate format to datetime.

        Args:
            pub_date_str: Date string in RSS format (RFC 822/2822)

        Returns:
            datetime object or None if parsing fails
        """
        if not pub_date_str:
            return None

        try:
            # RSS 2.0 uses RFC 822/2822 format: "Mon, 28 Oct 2024 00:00:00 +0000"
            return datetime.strptime(pub_date_str, '%a, %d %b %Y %H:%M:%S %z')
        except ValueError:
            logger.warning(f"[danluu_adapter._parse_pub_date] Failed to parse date: {pub_date_str}")
            return None

    def _rss_item_to_article(self, item: dict, date: str) -> dict | None:
        """Convert RSS item to article dict.

        Args:
            item: RSS item dictionary
            date: Date string

        Returns:
            Article dictionary or None if item should be skipped
        """
        title = item.get('title', '').strip()
        url = item.get('link', '').strip()

        if not title or not url:
            return None

        # Extract excerpt from description (strip HTML tags)
        description = item.get('description', '')
        excerpt = self._extract_text_from_html(description)[:200]

        return {
            "title": title,
            "article_meta": f"{len(excerpt)} char excerpt",
            "url": url,
            "category": "Dan Luu",
            "date": util.format_date_for_url(date),
            "newsletter_type": "blog",
            "removed": False,
        }

    @staticmethod
    def _extract_text_from_html(html: str) -> str:
        """Extract plain text from HTML description.

        Args:
            html: HTML content

        Returns:
            Plain text with tags stripped
        """
        if not html:
            return ""

        # Simple tag removal (good enough for excerpts)
        import re
        text = re.sub(r'<[^>]+>', '', html)
        # Decode HTML entities
        text = text.replace('&quot;', '"')
        text = text.replace('&amp;', '&')
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')
        text = text.replace('&#39;', "'")

        return text.strip()

</file>
<file path="deepmind_adapter.py">
"""
Google DeepMind blog adapter using HTML scraping.

This adapter fetches articles from the Google DeepMind blog by scraping
the blog listing page, filtering by date and extracting article metadata.
"""

import logging
import re
from datetime import datetime
import requests
from bs4 import BeautifulSoup

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("deepmind_adapter")


class DeepMindAdapter(NewsletterAdapter):
    """Adapter for Google DeepMind blog using HTML scraping."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)
        self.blog_url = "https://deepmind.google/discover/blog/"

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch blog posts for a specific date from blog listing page.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date = datetime.fromisoformat(util.format_date_for_url(date))
        target_month_year = target_date.strftime("%B %Y")

        logger.info(f"[deepmind_adapter.scrape_date] Fetching articles for {target_month_year} (excluding {len(excluded_urls)} URLs)")

        try:
            response = requests.get(self.blog_url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')
            article_cards = soup.find_all('article', class_='card-blog')

            logger.info(f"[deepmind_adapter.scrape_date] Found {len(article_cards)} total articles on blog page")

            for card in article_cards:
                time_elem = card.find('time')
                if not time_elem:
                    continue

                article_date_str = time_elem.get('datetime', '').strip()

                if article_date_str != target_month_year:
                    continue

                link_elem = card.find('a', class_='button')
                if not link_elem:
                    continue

                href = link_elem.get('href', '')
                if not href.startswith('/'):
                    continue

                full_url = 'https://deepmind.google' + href
                canonical_url = util.canonicalize_url(full_url)

                if canonical_url in excluded_set:
                    continue

                article = self._card_to_article(card, target_date.strftime("%Y-%m-%d"))
                if article:
                    articles.append(article)

            logger.info(f"[deepmind_adapter.scrape_date] Found {len(articles)} articles for {target_month_year}")

        except Exception as e:
            logger.error(f"[deepmind_adapter.scrape_date] Error fetching blog: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date.strftime("%Y-%m-%d"),
                'source_id': self.config.source_id,
                'category': self.config.category_display_names.get('blog', 'Google DeepMind'),
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _card_to_article(self, card, date: str) -> dict | None:
        """Convert blog article card to article dict.

        Args:
            card: BeautifulSoup article card element
            date: Date string in YYYY-MM-DD format

        Returns:
            Article dictionary or None if card should be skipped
        """
        title_elem = card.find('h3', class_='card__title')
        if not title_elem:
            return None

        title = title_elem.get_text(strip=True)
        if not title:
            return None

        link_elem = card.find('a', class_='button')
        if not link_elem:
            return None

        href = link_elem.get('href', '')
        if not href.startswith('/'):
            return None

        full_url = 'https://deepmind.google' + href

        category_elem = card.find('span', class_='meta__category')
        category_tag = category_elem.get_text(strip=True) if category_elem else ''

        article_meta = f"Category: {category_tag}" if category_tag else ""

        return {
            "title": title,
            "article_meta": article_meta,
            "url": full_url,
            "category": self.config.category_display_names.get('blog', 'Google DeepMind'),
            "date": date,
            "newsletter_type": "blog",
            "removed": False,
        }

</file>
<file path="hackernews_adapter.py">
"""
HackerNews adapter implementation using Algolia HN Search API.

This adapter implements the NewsletterAdapter interface for HackerNews,
using the Algolia HN Search API for efficient server-side filtering.

Benefits over previous haxor library approach:
- 67% fewer API requests (1 vs 3 per date)
- 77% less data transferred (50 vs 216 stories fetched)
- 70% faster response times (server-side filtering)
- Better quality results (searches entire date range, not just first 100)
"""

import logging
from datetime import datetime
import requests

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("hackernews_adapter")

# Algolia HN Search API endpoint
ALGOLIA_API_BASE = "http://hn.algolia.com/api/v1"


class HackerNewsAdapter(NewsletterAdapter):
    """Adapter for HackerNews using Algolia HN Search API."""

    def __init__(self, config):
        """Initialize with config.

        Note: We don't use the HTML-to-markdown functionality from base class.
        """
        super().__init__(config)

        # Quality thresholds for filtering
        # These ensure we only get high-quality, engaging posts
        self.min_points = 30
        self.min_comments = 5
        self.max_stories = 50  # Fetch up to 50 pre-filtered stories

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch HackerNews stories using Algolia API with server-side filtering.

        Strategy: Single combined query for all story types (story, ask_hn, show_hn)
        with quality filters applied server-side. This is ~67% fewer requests and
        ~77% less data than the previous approach.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        # Parse target date and get timestamp range
        target_date = datetime.fromisoformat(util.format_date_for_url(date))
        start_of_day = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_of_day = target_date.replace(hour=23, minute=59, second=59, microsecond=0)

        start_timestamp = int(start_of_day.timestamp())
        end_timestamp = int(end_of_day.timestamp())

        logger.info(f"[hackernews_adapter.scrape_date] Fetching stories for {date} using Algolia API (excluding {len(excluded_urls)} URLs)")

        # Fetch stories using Algolia API with server-side filtering
        try:
            stories = self._fetch_stories_algolia(
                start_timestamp=start_timestamp,
                end_timestamp=end_timestamp,
                min_points=self.min_points,
                min_comments=self.min_comments,
                limit=self.max_stories
            )

            logger.info(f"[hackernews_adapter.scrape_date] Fetched {len(stories)} pre-filtered stories for {date}")

            # Filter out excluded URLs before scoring
            filtered_stories = []
            for story in stories:
                if not story.get('url'):
                    continue
                canonical_url = util.canonicalize_url(story['url'])
                if canonical_url not in excluded_set:
                    filtered_stories.append(story)

            logger.info(f"[hackernews_adapter.scrape_date] {len(filtered_stories)} stories after filtering excluded URLs")

            # Sort by leading score (already have points and comments from API)
            stories_with_scores = []
            for story in filtered_stories:
                points = story.get('points', 0)
                comments = story.get('num_comments', 0)
                leading_score = (2 * points) + comments

                stories_with_scores.append({
                    **story,
                    'leading_score': leading_score
                })

            # Sort by leading score descending
            stories_with_scores.sort(key=lambda s: s['leading_score'], reverse=True)

            # Convert to article format
            for story in stories_with_scores:
                article = self._algolia_story_to_article(story, date)
                if article:
                    articles.append(article)

            logger.info(f"[hackernews_adapter.scrape_date] Converted {len(articles)} stories to articles")

        except Exception as e:
            logger.error(f"[hackernews_adapter.scrape_date] Error fetching stories: {e}", exc_info=True)

        # Create issues for each category that has articles
        categories_in_articles = {article['category'] for article in articles}
        issues = [
            {
                'date': util.format_date_for_url(date),
                'source_id': self.config.source_id,
                'category': category,
                'title': None,
                'subtitle': None
            }
            for category in sorted(categories_in_articles)
        ]

        return self._normalize_response(articles, issues)

    def _fetch_stories_algolia(
        self,
        start_timestamp: int,
        end_timestamp: int,
        min_points: int = 30,
        min_comments: int = 5,
        limit: int = 50
    ) -> list:
        """Fetch stories from Algolia HN Search API with server-side filtering.

        Args:
            start_timestamp: Unix timestamp for start of date range
            end_timestamp: Unix timestamp for end of date range
            min_points: Minimum points (upvotes) required
            min_comments: Minimum comment count required
            limit: Maximum number of stories to return

        Returns:
            List of story dictionaries from Algolia API
        """
        url = f"{ALGOLIA_API_BASE}/search_by_date"

        # Option B: Combined query for all story types with same quality threshold
        # This includes story, ask_hn, and show_hn in a single request
        params = {
            "tags": "(story,ask_hn,show_hn)",
            "numericFilters": f"created_at_i>{start_timestamp},created_at_i<{end_timestamp},points>={min_points},num_comments>={min_comments}",
            "hitsPerPage": limit
        }

        logger.info(f"[hackernews_adapter._fetch_stories_algolia] Querying Algolia API with filters: {params['numericFilters']}")

        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()

        data = response.json()
        hits = data.get('hits', [])

        logger.info(f"[hackernews_adapter._fetch_stories_algolia] Received {len(hits)} stories from Algolia")

        return hits

    def _algolia_story_to_article(self, story: dict, date: str) -> dict | None:
        """Convert Algolia HN story to article dict.

        Args:
            story: Algolia HN story dictionary
            date: Date string

        Returns:
            Article dictionary or None if story should be skipped
        """
        # Skip stories without URLs (Ask HN text posts, polls, etc.)
        if not story.get('url'):
            return None

        # Determine story type from tags
        tags = story.get('_tags', [])
        story_type = None

        if 'ask_hn' in tags:
            story_type = 'ask'
            category = 'HN Ask'
        elif 'show_hn' in tags:
            story_type = 'show'
            category = 'HN Show'
        else:
            story_type = 'top'  # Default to 'top' for regular stories
            category = 'HN Top'

        # Get category display name from config if available
        if story_type and story_type in self.config.category_display_names:
            category = self.config.category_display_names[story_type]

        base_title = story.get('title', f"HN Story {story.get('objectID', '')}")
        points = story.get('points', 0)
        comments = story.get('num_comments', 0)
        article_meta = f"{points} upvotes, {comments} comments"

        return {
            "title": base_title,
            "article_meta": article_meta,
            "url": story['url'],
            "category": category,
            "date": util.format_date_for_url(date),
            "newsletter_type": story_type,
            "removed": False,
        }

</file>
<file path="hillel_wayne_adapter.py">
"""
Hillel Wayne blog adapter using RSS feed.

This adapter fetches articles from Hillel Wayne's blog via the RSS feed,
filtering by date and extracting article metadata.
"""

import logging
import re
from datetime import datetime
import requests
import feedparser

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("hillel_wayne_adapter")


class HillelWayneAdapter(NewsletterAdapter):
    """Adapter for Hillel Wayne's blog using RSS feed."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)
        self.feed_url = "https://www.hillelwayne.com/index.xml"

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch blog posts for a specific date from RSS feed.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date = datetime.fromisoformat(util.format_date_for_url(date))
        target_date_str = target_date.strftime("%Y-%m-%d")

        logger.info(f"[hillel_wayne_adapter.scrape_date] Fetching articles for {target_date_str} (excluding {len(excluded_urls)} URLs)")

        try:
            response = requests.get(self.feed_url, timeout=10)
            response.raise_for_status()

            feed = feedparser.parse(response.content)

            logger.info(f"[hillel_wayne_adapter.scrape_date] Fetched {len(feed.entries)} total entries from feed")

            for entry in feed.entries:
                if not entry.get('published_parsed'):
                    continue

                entry_date = datetime(*entry.published_parsed[:6])
                entry_date_str = entry_date.strftime("%Y-%m-%d")

                if entry_date_str != target_date_str:
                    continue

                link = entry.get('link', '')
                if not link:
                    continue

                canonical_url = util.canonicalize_url(link)

                if canonical_url in excluded_set:
                    continue

                article = self._entry_to_article(entry, target_date_str)
                if article:
                    articles.append(article)

            logger.info(f"[hillel_wayne_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[hillel_wayne_adapter.scrape_date] Error fetching feed: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': self.config.category_display_names.get('blog', 'Hillel Wayne'),
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _strip_html(self, html: str) -> str:
        """Strip HTML tags from text.

        >>> adapter = HillelWayneAdapter(None)
        >>> adapter._strip_html("<p>Hello <b>world</b></p>")
        'Hello world'
        """
        text = re.sub(r'<[^>]+>', '', html)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _entry_to_article(self, entry: dict, date: str) -> dict | None:
        """Convert RSS feed entry to article dict.

        Args:
            entry: feedparser entry dictionary
            date: Date string

        Returns:
            Article dictionary or None if entry should be skipped
        """
        title = entry.get('title', '')
        if not title:
            return None

        link = entry.get('link', '')
        if not link:
            return None

        summary = entry.get('summary', '')
        summary_text = self._strip_html(summary) if summary else ''

        if summary_text:
            if len(summary_text) > 200:
                summary_text = summary_text[:200] + '...'

        tags = [tag.term for tag in entry.get('tags', [])]
        if tags:
            tags_str = ', '.join(tags[:5])
            article_meta = f"Tags: {tags_str}"
        else:
            article_meta = ""

        return {
            "title": title,
            "article_meta": article_meta,
            "url": link,
            "category": self.config.category_display_names.get('blog', 'Hillel Wayne'),
            "date": date,
            "newsletter_type": "blog",
            "removed": False,
        }

</file>
<file path="infoq_adapter.py">
"""
InfoQ adapter using RSS feed.

This adapter fetches articles from InfoQ via the RSS feed,
filtering by date and extracting article metadata.
InfoQ focuses on software architecture for senior engineers/architects.
"""

import logging
import re
from datetime import datetime
import requests
import feedparser

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("infoq_adapter")


class InfoQAdapter(NewsletterAdapter):
    """Adapter for InfoQ using RSS feed."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)
        self.feed_url = "https://www.infoq.com/feed"

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch articles for a specific date from RSS feed.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date = datetime.fromisoformat(util.format_date_for_url(date))
        target_date_str = target_date.strftime("%Y-%m-%d")

        logger.info(f"[infoq_adapter.scrape_date] Fetching articles for {target_date_str} (excluding {len(excluded_urls)} URLs)")

        try:
            response = requests.get(self.feed_url, timeout=10)
            response.raise_for_status()

            feed = feedparser.parse(response.content)

            logger.info(f"[infoq_adapter.scrape_date] Fetched {len(feed.entries)} total entries from feed")

            for entry in feed.entries:
                if not entry.get('published_parsed'):
                    continue

                entry_date = datetime(*entry.published_parsed[:6])
                entry_date_str = entry_date.strftime("%Y-%m-%d")

                if entry_date_str != target_date_str:
                    continue

                link = entry.get('link', '')
                if not link:
                    continue

                canonical_url = util.canonicalize_url(link)

                if canonical_url in excluded_set:
                    continue

                article = self._entry_to_article(entry, target_date_str)
                if article:
                    articles.append(article)

            logger.info(f"[infoq_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[infoq_adapter.scrape_date] Error fetching feed: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': self.config.category_display_names.get('articles', 'InfoQ'),
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _strip_html(self, html: str) -> str:
        """Strip HTML tags from text.

        >>> adapter = InfoQAdapter(None)
        >>> adapter._strip_html("<p>Hello <b>world</b></p>")
        'Hello world'
        """
        text = re.sub(r'<[^>]+>', '', html)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _entry_to_article(self, entry: dict, date: str) -> dict | None:
        """Convert RSS feed entry to article dict.

        Args:
            entry: feedparser entry dictionary
            date: Date string

        Returns:
            Article dictionary or None if entry should be skipped
        """
        title = entry.get('title', '')
        if not title:
            return None

        link = entry.get('link', '')
        if not link:
            return None

        summary = entry.get('summary', entry.get('description', ''))
        summary_text = self._strip_html(summary) if summary else ''

        if summary_text:
            if len(summary_text) > 200:
                summary_text = summary_text[:200] + '...'

        article_meta = summary_text if summary_text else ""

        tags = [tag.get('term', '') for tag in entry.get('tags', [])]
        if tags:
            tags_str = ', '.join(tags[:3])
            if article_meta:
                article_meta = f"{article_meta} | Tags: {tags_str}"
            else:
                article_meta = f"Tags: {tags_str}"

        return {
            "title": title,
            "article_meta": article_meta,
            "url": link,
            "category": self.config.category_display_names.get('articles', 'InfoQ'),
            "date": date,
            "newsletter_type": "articles",
            "removed": False,
        }

</file>
<file path="jessitron_adapter.py">
"""
Jessitron blog adapter using RSS feed.

Jessica Kerr (jessitron.com) writes about resilience engineering, DDD, DevOps,
symmathecist philosophy, and systems thinking. Known for systems thinking
workshops with Kent Beck.
"""

import logging
import re
from datetime import datetime
import feedparser

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("jessitron_adapter")

RSS_FEED_URL = "https://jessitron.com/rss"


class JessitronAdapter(NewsletterAdapter):
    """Adapter for Jessitron's blog using RSS feed."""

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch Jessitron blog posts for a specific date using RSS feed.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary with articles and issues
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date_str = util.format_date_for_url(date)
        target_date = datetime.fromisoformat(target_date_str).date()

        logger.info(f"[jessitron_adapter.scrape_date] Fetching RSS feed for {target_date_str}")

        try:
            feed = feedparser.parse(RSS_FEED_URL)

            if not feed.entries:
                logger.warning(f"[jessitron_adapter.scrape_date] No entries found in RSS feed")
                return self._normalize_response([], [])

            logger.info(f"[jessitron_adapter.scrape_date] Fetched {len(feed.entries)} entries from RSS")

            for entry in feed.entries:
                article = self._parse_rss_entry(entry, target_date, excluded_set)
                if article:
                    articles.append(article)

            logger.info(f"[jessitron_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[jessitron_adapter.scrape_date] Error fetching RSS feed: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': 'Jessitron',
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _parse_rss_entry(self, entry: dict, target_date: datetime.date, excluded_set: set) -> dict | None:
        """Parse RSS entry into article dict if it matches the target date.

        Args:
            entry: feedparser entry dict
            target_date: Target date to filter by
            excluded_set: Set of canonical URLs to exclude

        Returns:
            Article dictionary or None if entry should be skipped
        """
        if not entry.get('link'):
            return None

        canonical_url = util.canonicalize_url(entry['link'])
        if canonical_url in excluded_set:
            return None

        published_parsed = entry.get('published_parsed')
        if not published_parsed:
            return None

        entry_date = datetime(*published_parsed[:3]).date()

        if entry_date != target_date:
            return None

        title = entry.get('title', 'Untitled')
        summary = entry.get('summary', '')

        summary_text = self._strip_html(summary) if summary else ''
        if summary_text and len(summary_text) > 150:
            summary_text = summary_text[:150] + '...'

        tags = entry.get('tags', [])
        tag_terms = [tag.get('term', '') for tag in tags]

        if tag_terms:
            tags_str = ', '.join(tag_terms[:5])
            article_meta = f"Tags: {tags_str}"
        else:
            article_meta = summary_text

        return {
            "title": title,
            "article_meta": article_meta,
            "url": entry['link'],
            "category": "Jessitron",
            "date": util.format_date_for_url(entry_date),
            "newsletter_type": "blog",
            "removed": False,
        }

    def _strip_html(self, html: str) -> str:
        """Strip HTML tags and entities from text.

        >>> adapter = JessitronAdapter(None)
        >>> adapter._strip_html("&quot;Hello&quot; <b>world</b>")
        '"Hello" world'
        """
        text = re.sub(r'&#8220;', '"', html)
        text = re.sub(r'&#8221;', '"', text)
        text = re.sub(r'&#8217;', "'", text)
        text = re.sub(r'&quot;', '"', text)
        text = re.sub(r'&amp;', '&', text)
        text = re.sub(r'&lt;', '<', text)
        text = re.sub(r'&gt;', '>', text)
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

</file>
<file path="lenny_newsletter_adapter.py">
"""
Lenny's Newsletter adapter using Substack RSS feed.

This adapter fetches articles from Lenny's Newsletter via the Substack RSS feed,
filtering by date and extracting article metadata.
"""

import logging
import re
from datetime import datetime
import requests
import feedparser

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("lenny_newsletter_adapter")


class LennyNewsletterAdapter(NewsletterAdapter):
    """Adapter for Lenny's Newsletter using Substack RSS feed."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)
        self.feed_url = "https://www.lennysnewsletter.com/feed"

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch newsletter posts for a specific date from RSS feed.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date = datetime.fromisoformat(util.format_date_for_url(date))
        target_date_str = target_date.strftime("%Y-%m-%d")

        logger.info(f"[lenny_newsletter_adapter.scrape_date] Fetching articles for {target_date_str} (excluding {len(excluded_urls)} URLs)")

        try:
            response = requests.get(self.feed_url, timeout=10)
            response.raise_for_status()

            feed = feedparser.parse(response.content)

            logger.info(f"[lenny_newsletter_adapter.scrape_date] Fetched {len(feed.entries)} total entries from feed")

            for entry in feed.entries:
                if not entry.get('published_parsed'):
                    continue

                entry_date = datetime(*entry.published_parsed[:6])
                entry_date_str = entry_date.strftime("%Y-%m-%d")

                if entry_date_str != target_date_str:
                    continue

                link = entry.get('link', '')
                if not link:
                    continue

                canonical_url = util.canonicalize_url(link)

                if canonical_url in excluded_set:
                    continue

                article = self._entry_to_article(entry, target_date_str)
                if article:
                    articles.append(article)

            logger.info(f"[lenny_newsletter_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[lenny_newsletter_adapter.scrape_date] Error fetching feed: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': self.config.category_display_names.get('newsletter', "Lenny's Newsletter"),
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _strip_html(self, html: str) -> str:
        """Strip HTML tags from text.

        >>> adapter = LennyNewsletterAdapter(None)
        >>> adapter._strip_html("<p>Hello <b>world</b></p>")
        'Hello world'
        """
        text = re.sub(r'<[^>]+>', '', html)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _entry_to_article(self, entry: dict, date: str) -> dict | None:
        """Convert RSS feed entry to article dict.

        Args:
            entry: feedparser entry dictionary
            date: Date string

        Returns:
            Article dictionary or None if entry should be skipped
        """
        title = entry.get('title', '')
        if not title:
            return None

        link = entry.get('link', '')
        if not link:
            return None

        summary = entry.get('summary', '')
        summary_text = self._strip_html(summary) if summary else ''

        if summary_text:
            if len(summary_text) > 300:
                summary_text = summary_text[:300] + '...'

        article_meta = summary_text if summary_text else ""

        author = entry.get('author', 'Lenny Rachitsky')

        return {
            "title": title,
            "article_meta": article_meta,
            "url": link,
            "category": self.config.category_display_names.get('newsletter', "Lenny's Newsletter"),
            "date": date,
            "newsletter_type": "newsletter",
            "removed": False,
        }

</file>
<file path="martin_fowler_adapter.py">
"""
Martin Fowler's blog adapter using Atom RSS feed.

This adapter fetches articles from Martin Fowler's blog via the Atom feed,
filtering by date and extracting article metadata.
"""

import logging
import re
from datetime import datetime
import requests
import feedparser

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("martin_fowler_adapter")


class MartinFowlerAdapter(NewsletterAdapter):
    """Adapter for Martin Fowler's blog using Atom RSS feed."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)
        self.feed_url = "https://martinfowler.com/feed.atom"

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch blog posts for a specific date from RSS feed.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date = datetime.fromisoformat(util.format_date_for_url(date))
        target_date_str = target_date.strftime("%Y-%m-%d")

        logger.info(f"[martin_fowler_adapter.scrape_date] Fetching articles for {target_date_str} (excluding {len(excluded_urls)} URLs)")

        try:
            response = requests.get(self.feed_url, timeout=10)
            response.raise_for_status()

            feed = feedparser.parse(response.content)

            logger.info(f"[martin_fowler_adapter.scrape_date] Fetched {len(feed.entries)} total entries from feed")

            for entry in feed.entries:
                if not entry.get('updated_parsed'):
                    continue

                entry_date = datetime(*entry.updated_parsed[:6])
                entry_date_str = entry_date.strftime("%Y-%m-%d")

                if entry_date_str != target_date_str:
                    continue

                link = entry.get('link', '')
                if not link:
                    continue

                canonical_url = util.canonicalize_url(link)

                if canonical_url in excluded_set:
                    continue

                article = self._entry_to_article(entry, target_date_str)
                if article:
                    articles.append(article)

            logger.info(f"[martin_fowler_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[martin_fowler_adapter.scrape_date] Error fetching feed: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': self.config.category_display_names.get('blog', 'Martin Fowler'),
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _strip_html(self, html: str) -> str:
        """Strip HTML tags from text.

        >>> adapter = MartinFowlerAdapter(None)
        >>> adapter._strip_html("<p>Hello <b>world</b></p>")
        'Hello world'
        """
        text = re.sub(r'<[^>]+>', '', html)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _entry_to_article(self, entry: dict, date: str) -> dict | None:
        """Convert RSS feed entry to article dict.

        Args:
            entry: feedparser entry dictionary
            date: Date string

        Returns:
            Article dictionary or None if entry should be skipped
        """
        title = entry.get('title', '')
        if not title:
            return None

        link = entry.get('link', '')
        if not link:
            return None

        summary = entry.get('summary', entry.get('content', [{}])[0].get('value', ''))
        summary_text = self._strip_html(summary) if summary else ''

        if summary_text:
            if len(summary_text) > 200:
                summary_text = summary_text[:200] + '...'

        article_meta = summary_text if summary_text else ""

        return {
            "title": title,
            "article_meta": article_meta,
            "url": link,
            "category": self.config.category_display_names.get('blog', 'Martin Fowler'),
            "date": date,
            "newsletter_type": "blog",
            "removed": False,
        }

</file>
<file path="netflix_adapter.py">
"""
Netflix Tech Blog adapter implementation using RSS feed.

This adapter implements the NewsletterAdapter interface for Netflix Tech Blog,
fetching articles from the Medium RSS feed and filtering them by publication date.
"""

import logging
import xml.etree.ElementTree as ET
from datetime import datetime

import requests

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("netflix_adapter")


class NetflixAdapter(NewsletterAdapter):
    """Adapter for Netflix Tech Blog using Medium RSS feed."""

    def __init__(self, config):
        """Initialize with config.

        Note: We don't use the HTML-to-markdown functionality from base class.
        """
        super().__init__(config)
        self.rss_url = "https://medium.com/feed/netflix-techblog"

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch Netflix Tech Blog articles from RSS feed for a specific date.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date = datetime.fromisoformat(util.format_date_for_url(date))

        logger.info(f"[netflix_adapter.scrape_date] Fetching articles for {date} from RSS feed (excluding {len(excluded_urls)} URLs)")

        try:
            feed_items = self._fetch_rss_feed()

            logger.info(f"[netflix_adapter.scrape_date] Fetched {len(feed_items)} total items from RSS feed")

            # Filter items by date and excluded URLs
            filtered_items = []
            for item in feed_items:
                pub_date = self._parse_pub_date(item.get('pubDate', ''))
                if pub_date is None:
                    continue

                # Check if article was published on target date
                if pub_date.date() != target_date.date():
                    continue

                # Check if URL is excluded
                url = item.get('link', '')
                if not url:
                    continue

                canonical_url = util.canonicalize_url(url)
                if canonical_url not in excluded_set:
                    filtered_items.append(item)

            logger.info(f"[netflix_adapter.scrape_date] {len(filtered_items)} articles match date {date}")

            # Convert to article format
            for item in filtered_items:
                article = self._rss_item_to_article(item, date)
                if article:
                    articles.append(article)

            logger.info(f"[netflix_adapter.scrape_date] Converted {len(articles)} items to articles")

        except Exception as e:
            logger.error(f"[netflix_adapter.scrape_date] Error fetching RSS feed: {e}", exc_info=True)

        # Create issue metadata if we have articles
        issues = []
        if articles:
            issues.append({
                'date': util.format_date_for_url(date),
                'source_id': self.config.source_id,
                'category': 'Netflix Tech',
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _fetch_rss_feed(self) -> list[dict]:
        """Fetch and parse the RSS feed.

        Returns:
            List of item dictionaries with title, link, pubDate, categories, description
        """
        response = requests.get(self.rss_url, timeout=30, headers={
            'User-Agent': self.config.user_agent
        })
        response.raise_for_status()

        root = ET.fromstring(response.content)
        channel = root.find('channel')

        if channel is None:
            logger.warning("[netflix_adapter._fetch_rss_feed] No channel found in RSS feed")
            return []

        items = []
        for item_elem in channel.findall('item'):
            title = item_elem.find('title')
            link = item_elem.find('link')
            pub_date = item_elem.find('pubDate')

            # Medium RSS feeds include content:encoded with full HTML
            content_ns = '{http://purl.org/rss/1.0/modules/content/}'
            content_encoded = item_elem.find(f'{content_ns}encoded')

            # Get all categories
            categories = [cat.text for cat in item_elem.findall('category') if cat.text]

            items.append({
                'title': title.text if title is not None else '',
                'link': link.text if link is not None else '',
                'pubDate': pub_date.text if pub_date is not None else '',
                'content': content_encoded.text if content_encoded is not None else '',
                'categories': categories,
            })

        return items

    def _parse_pub_date(self, pub_date_str: str) -> datetime | None:
        """Parse RSS pubDate format to datetime.

        Args:
            pub_date_str: Date string in RSS format (RFC 822/2822)

        Returns:
            datetime object or None if parsing fails
        """
        if not pub_date_str:
            return None

        try:
            # RSS 2.0 uses RFC 822/2822 format: "Tue, 04 Nov 2025 20:33:44 GMT"
            return datetime.strptime(pub_date_str, '%a, %d %b %Y %H:%M:%S %Z')
        except ValueError:
            logger.warning(f"[netflix_adapter._parse_pub_date] Failed to parse date: {pub_date_str}")
            return None

    def _rss_item_to_article(self, item: dict, date: str) -> dict | None:
        """Convert RSS item to article dict.

        Args:
            item: RSS item dictionary
            date: Date string

        Returns:
            Article dictionary or None if item should be skipped
        """
        title = item.get('title', '').strip()
        url = item.get('link', '').strip()

        if not title or not url:
            return None

        # Get categories for metadata
        categories = item.get('categories', [])
        category_str = ', '.join(categories[:3]) if categories else ''

        # Extract excerpt from content (strip HTML tags)
        content = item.get('content', '')
        excerpt = self._extract_text_from_html(content)[:150]

        # Build article_meta with categories and excerpt length
        article_meta_parts = []
        if category_str:
            article_meta_parts.append(category_str)
        if excerpt:
            article_meta_parts.append(f"{len(excerpt)} char excerpt")
        article_meta = ' | '.join(article_meta_parts) if article_meta_parts else ''

        return {
            "title": title,
            "article_meta": article_meta,
            "url": url,
            "category": "Netflix Tech",
            "date": util.format_date_for_url(date),
            "newsletter_type": "blog",
            "removed": False,
        }

    @staticmethod
    def _extract_text_from_html(html: str) -> str:
        """Extract plain text from HTML content.

        Args:
            html: HTML content

        Returns:
            Plain text with tags stripped
        """
        if not html:
            return ""

        # Simple tag removal (good enough for excerpts)
        import re
        text = re.sub(r'<[^>]+>', '', html)
        # Decode common HTML entities
        text = text.replace('&quot;', '"')
        text = text.replace('&amp;', '&')
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')
        text = text.replace('&#39;', "'")
        text = text.replace('&nbsp;', ' ')

        return text.strip()

</file>
<file path="newsletter_adapter.py">
"""
Abstract base class for newsletter source adapters.

This module defines the interface that all newsletter adapters must implement,
providing a template method pattern for fetching, parsing, and normalizing
newsletter content from different sources.
"""

from io import BytesIO

from bs4 import BeautifulSoup
from markitdown import MarkItDown

from newsletter_config import NewsletterSourceConfig
import util


class NewsletterAdapter:
    """Base adapter for newsletter sources.

    Subclasses can either:
    1. Implement fetch_issue, parse_articles, and extract_issue_metadata for HTML-based sources
    2. Override scrape_date() entirely for API-based sources or custom workflows
    """

    def __init__(self, config: NewsletterSourceConfig):
        """Initialize adapter with source configuration.

        Args:
            config: Configuration object defining source-specific settings
        """
        self.config = config
        self.md = MarkItDown()

    def fetch_issue(self, date: str, newsletter_type: str) -> str | None:
        """Fetch raw HTML for a specific issue.

        Override this method for HTML-based sources.
        For API-based sources, override scrape_date() instead.

        Args:
            date: Date string in format used by source
            newsletter_type: Type/category within source (e.g., "tech", "ai")

        Returns:
            HTML content as string, or None if issue not found
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} must implement fetch_issue() or override scrape_date()"
        )

    def parse_articles(
        self, markdown: str, date: str, newsletter_type: str
    ) -> list[dict]:
        """Parse articles from markdown content.

        Override this method for HTML-based sources.
        For API-based sources, override scrape_date() instead.

        Args:
            markdown: Converted markdown content
            date: Date string for the issue
            newsletter_type: Type/category within source

        Returns:
            List of article dictionaries with keys: title, url, category, date, etc.
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} must implement parse_articles() or override scrape_date()"
        )

    def extract_issue_metadata(
        self, markdown: str, date: str, newsletter_type: str
    ) -> dict | None:
        """Extract issue metadata (title, subtitle, sections).

        Override this method for HTML-based sources.
        For API-based sources, override scrape_date() instead.

        Args:
            markdown: Converted markdown content
            date: Date string for the issue
            newsletter_type: Type/category within source

        Returns:
            Dictionary with issue metadata, or None if no metadata found
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} must implement extract_issue_metadata() or override scrape_date()"
        )

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Template method - orchestrates fetch + parse + normalize.

        This default implementation follows the HTML scraping workflow:
        1. Fetch HTML for each type configured for this source
        2. Convert HTML to markdown
        3. Parse articles and extract metadata
        4. Filter out excluded URLs
        5. Normalize response with source_id

        Subclasses can override this entire method for different workflows
        (e.g., API-based sources that don't use HTML conversion).

        Args:
            date: Date string to scrape
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary with source_id, articles, and issues
        """
        articles = []
        issues = []
        excluded_set = set(excluded_urls)

        for newsletter_type in self.config.types:
            # Fetch raw HTML
            html = self.fetch_issue(date, newsletter_type)
            if html is None:
                continue

            # Convert to markdown
            markdown = self._html_to_markdown(html)

            # Parse articles and metadata
            parsed_articles = self.parse_articles(markdown, date, newsletter_type)

            # Filter out excluded URLs
            for article in parsed_articles:
                canonical_url = util.canonicalize_url(article['url'])
                if canonical_url not in excluded_set:
                    articles.append(article)

            issue_meta = self.extract_issue_metadata(markdown, date, newsletter_type)
            if issue_meta:
                issues.append(issue_meta)

        return self._normalize_response(articles, issues)

    def _html_to_markdown(self, html: str) -> str:
        """Convert HTML to markdown using BeautifulSoup and MarkItDown.

        Args:
            html: Raw HTML content

        Returns:
            Markdown string
        """
        soup = BeautifulSoup(html, "html.parser")
        newsletter_content = soup.body or soup
        content_html = str(newsletter_content)
        content_stream = BytesIO(content_html.encode("utf-8"))
        result = self.md.convert_stream(content_stream, file_extension=".html")
        return result.text_content

    def _normalize_response(self, articles: list[dict], issues: list[dict]) -> dict:
        """Convert to standardized format with source_id.

        This ensures every article and issue includes the source_id to prevent
        identity collisions when multiple sources are aggregated.

        Args:
            articles: List of parsed articles
            issues: List of parsed issue metadata

        Returns:
            Normalized response with source_id added to all items
        """
        return {
            "source_id": self.config.source_id,
            "articles": [
                {**article, "source_id": self.config.source_id} for article in articles
            ],
            "issues": [
                {**issue, "source_id": self.config.source_id} for issue in issues
            ],
        }

</file>
<file path="newsletter_config.py">
"""
Newsletter source configuration schema and registered sources.

This module defines the declarative configuration for newsletter sources,
enabling the addition of new sources without modifying core scraper logic.
"""

from dataclasses import dataclass


@dataclass
class NewsletterSourceConfig:
    """Configuration for a newsletter source."""

    source_id: str  # Unique identifier: "tldr_tech", "tldr_ai", "hackernews"
    display_name: str  # Human-readable name: "TLDR Tech", "Hacker News Daily"
    base_url: str  # Base URL: "https://tldr.tech"
    url_pattern: str  # URL template: "{base_url}/{type}/{date}"
    types: list[str]  # Subtypes within source: ["tech", "ai"] or ["daily"]
    user_agent: str  # User-Agent header (neutral default)

    # Parsing rules
    article_pattern: str  # Regex to identify articles

    # Display preferences
    category_display_names: dict[str, str]  # {"tech": "TLDR Tech"}
    sort_order: int  # For multi-source ordering (lower = higher priority)


# Registered newsletter sources
NEWSLETTER_CONFIGS = {
    "tldr_tech": NewsletterSourceConfig(
        source_id="tldr_tech",
        display_name="TLDR Tech",
        base_url="https://tldr.tech",
        url_pattern="{base_url}/tech/{date}",
        types=["tech"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern=r"\((\d+)\s+minute\s+read\)|\(GitHub\s+Repo\)",
        category_display_names={"tech": "TLDR Tech"},
        sort_order=2,
    ),
    "tldr_ai": NewsletterSourceConfig(
        source_id="tldr_ai",
        display_name="TLDR AI",
        base_url="https://tldr.tech",
        url_pattern="{base_url}/ai/{date}",
        types=["ai"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern=r"\((\d+)\s+minute\s+read\)|\(GitHub\s+Repo\)",
        category_display_names={"ai": "TLDR AI"},
        sort_order=1,  # AI comes before Tech
    ),
    "hackernews": NewsletterSourceConfig(
        source_id="hackernews",
        display_name="Hacker News",
        base_url="http://hn.algolia.com/api/v1",  # Using Algolia HN Search API
        url_pattern="",  # Not used (Algolia API-based)
        types=["top", "ask", "show"],  # Combined in single query via Algolia
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",  # Not used for API-based sources
        category_display_names={
            "top": "HN Top",
            "ask": "HN Ask",
            "show": "HN Show",
        },
        sort_order=3,  # After TLDR AI (1) and TLDR Tech (2)
    ),
    "xeiaso": NewsletterSourceConfig(
        source_id="xeiaso",
        display_name="Xe Iaso",
        base_url="https://xeiaso.net",
        url_pattern="",
        types=["blog"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"blog": "Xe Iaso"},
        sort_order=4,
    ),
    "simon_willison": NewsletterSourceConfig(
        source_id="simon_willison",
        display_name="Simon Willison",
        base_url="https://simonwillison.net",
        url_pattern="",
        types=["blog"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"blog": "Simon Willison"},
        sort_order=5,
    ),
    "danluu": NewsletterSourceConfig(
        source_id="danluu",
        display_name="Dan Luu",
        base_url="https://danluu.com",
        url_pattern="",
        types=["blog"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"blog": "Dan Luu"},
        sort_order=5,
    ),
    "will_larson": NewsletterSourceConfig(
        source_id="will_larson",
        display_name="Irrational Exuberance",
        base_url="https://lethain.com",
        url_pattern="",
        types=["blog"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"blog": "Engineering Leadership"},
        sort_order=6,
    ),
    "cloudflare": NewsletterSourceConfig(
        source_id="cloudflare",
        display_name="Cloudflare Blog",
        base_url="https://blog.cloudflare.com",
        url_pattern="",
        types=["blog"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"blog": "Cloudflare Blog"},
        sort_order=7,
    ),
    "lenny_newsletter": NewsletterSourceConfig(
        source_id="lenny_newsletter",
        display_name="Lenny's Newsletter",
        base_url="https://www.lennysnewsletter.com",
        url_pattern="",
        types=["newsletter"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"newsletter": "Lenny's Newsletter"},
        sort_order=8,
    ),
    "pragmatic_engineer": NewsletterSourceConfig(
        source_id="pragmatic_engineer",
        display_name="The Pragmatic Engineer",
        base_url="https://newsletter.pragmaticengineer.com",
        url_pattern="",
        types=["newsletter"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"newsletter": "The Pragmatic Engineer"},
        sort_order=9,
    ),
    "jessitron": NewsletterSourceConfig(
        source_id="jessitron",
        display_name="Jessitron",
        base_url="https://jessitron.com",
        url_pattern="",
        types=["blog"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"blog": "Jessitron"},
        sort_order=10,
    ),
    "stripe_engineering": NewsletterSourceConfig(
        source_id="stripe_engineering",
        display_name="Stripe Engineering",
        base_url="https://stripe.com/blog/engineering",
        url_pattern="",
        types=["engineering"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"engineering": "Stripe Engineering"},
        sort_order=11,
    ),
    "deepmind": NewsletterSourceConfig(
        source_id="deepmind",
        display_name="Google DeepMind",
        base_url="https://deepmind.google",
        url_pattern="",
        types=["blog"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"blog": "Google DeepMind"},
        sort_order=12,
    ),
    "pointer": NewsletterSourceConfig(
        source_id="pointer",
        display_name="Pointer",
        base_url="https://www.pointer.io",
        url_pattern="",
        types=["newsletter"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"newsletter": "Pointer"},
        sort_order=13,
    ),
    "netflix": NewsletterSourceConfig(
        source_id="netflix",
        display_name="Netflix Tech Blog",
        base_url="https://medium.com/netflix-techblog",
        url_pattern="",
        types=["blog"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"blog": "Netflix Tech"},
        sort_order=13,
    ),
    "anthropic": NewsletterSourceConfig(
        source_id="anthropic",
        display_name="Anthropic Research",
        base_url="https://www.anthropic.com",
        url_pattern="",
        types=["research"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"research": "Anthropic Research"},
        sort_order=14,
    ),
    "softwareleadweekly": NewsletterSourceConfig(
        source_id="softwareleadweekly",
        display_name="Software Lead Weekly",
        base_url="https://softwareleadweekly.com",
        url_pattern="",
        types=["newsletter"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"newsletter": "Software Lead Weekly"},
        sort_order=8,
    ),
    "hillel_wayne": NewsletterSourceConfig(
        source_id="hillel_wayne",
        display_name="Hillel Wayne",
        base_url="https://www.hillelwayne.com",
        url_pattern="",
        types=["blog"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"blog": "Hillel Wayne"},
        sort_order=15,
    ),
    "infoq": NewsletterSourceConfig(
        source_id="infoq",
        display_name="InfoQ",
        base_url="https://www.infoq.com",
        url_pattern="",
        types=["articles"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"articles": "InfoQ"},
        sort_order=16,
    ),
    "bytebytego": NewsletterSourceConfig(
        source_id="bytebytego",
        display_name="ByteByteGo",
        base_url="https://blog.bytebytego.com",
        url_pattern="",
        types=["newsletter"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"newsletter": "ByteByteGo"},
        sort_order=17,
    ),
    "martin_fowler": NewsletterSourceConfig(
        source_id="martin_fowler",
        display_name="Martin Fowler",
        base_url="https://martinfowler.com",
        url_pattern="",
        types=["blog"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"blog": "Martin Fowler"},
        sort_order=18,
    ),
    "react_status": NewsletterSourceConfig(
        source_id="react_status",
        display_name="React Status",
        base_url="https://react.statuscode.com",
        url_pattern="",
        types=["newsletter"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"newsletter": "React Status"},
        sort_order=19,
    ),
    "node_weekly": NewsletterSourceConfig(
        source_id="node_weekly",
        display_name="Node Weekly",
        base_url="https://nodeweekly.com",
        url_pattern="",
        types=["newsletter"],
        user_agent="Mozilla/5.0 (compatible; Newsletter-Aggregator/1.0)",
        article_pattern="",
        category_display_names={"newsletter": "Node Weekly"},
        sort_order=20,
    ),
}

</file>
<file path="newsletter_merger.py">
"""
Source-agnostic newsletter response merger.

This module provides functions to merge responses from multiple newsletter
sources into a single normalized response, without any knowledge of specific
newsletter types or sources.
"""

import re

from newsletter_config import NEWSLETTER_CONFIGS
import util


def build_markdown_output(
    start_date, end_date, grouped_articles: dict[str, list[dict]], issues_by_key: dict
) -> str:
    """Generate neutral markdown output from grouped articles.

    Args:
        start_date: Start date for the range
        end_date: End date for the range
        grouped_articles: Articles grouped by date
        issues_by_key: Issue metadata indexed by (date, source_id, category)

    Returns:
        Markdown formatted string
    """
    start_str = util.format_date_for_url(start_date)
    end_str = util.format_date_for_url(end_date)

    # Neutral header (NO TLDR BRANDING)
    output = f"# Newsletter Articles ({start_str} to {end_str})\n\n"

    # List included sources dynamically
    if issues_by_key:
        sources = sorted(set(issue.get("source_id") for issue in issues_by_key.values() if issue.get("source_id")))
        if sources:
            source_names = [
                NEWSLETTER_CONFIGS[s].display_name
                for s in sources
                if s in NEWSLETTER_CONFIGS
            ]
            if source_names:
                output += f"**Sources:** {', '.join(source_names)}\n\n"

    sorted_dates = sorted(grouped_articles.keys(), reverse=True)

    def build_article_lines(article_list):
        if not article_list:
            return ""

        lines: list[str] = []
        for index, article in enumerate(article_list, 1):
            domain_name = util.get_domain_name(article["url"])
            is_removed = article.get("removed", False)

            if is_removed:
                title = article["title"]
                title = re.sub(
                    r"\s*\(\d+\s+minutes?\s+read\)", "", title, flags=re.IGNORECASE
                )
                if len(title) > 10:
                    title = title[:10] + "..."
                title_with_domain = f"{title} ({domain_name})"
            else:
                title_with_domain = f"{article['title']} ({domain_name})"

            removed_marker = "?data-removed=true" if is_removed else ""
            lines.append(
                f"{index}. [{title_with_domain}]({article['url']}{removed_marker})"
            )

        return "\n".join(lines) + "\n\n"

    for date_str in sorted_dates:
        articles = grouped_articles[date_str]

        output += f"### {date_str}\n\n"

        # Group by category
        category_groups = {}
        for article in articles:
            category = article["category"]
            if category not in category_groups:
                category_groups[category] = []
            category_groups[category].append(article)

        # Sort categories by source sort_order
        def get_category_sort_key(category):
            # Find the source_id for this category by looking at articles
            for article in category_groups[category]:
                if article.get("source_id") in NEWSLETTER_CONFIGS:
                    return NEWSLETTER_CONFIGS[article["source_id"]].sort_order
            return 999

        sorted_categories = sorted(category_groups.keys(), key=get_category_sort_key)

        for category in sorted_categories:
            category_articles = category_groups[category]

            output += f"#### {category}\n\n"

            # Find issue metadata for this category
            # Build key as (date, source_id, category) or (date, category) for backwards compat
            issue = None
            if category_articles:
                first_article = category_articles[0]
                source_id = first_article.get("source_id")
                if source_id:
                    issue = issues_by_key.get((date_str, source_id, category))
                if not issue:
                    # Fallback to old key format
                    issue = issues_by_key.get((date_str, category))

            issue_title = (issue or {}).get("title")
            issue_subtitle = (issue or {}).get("subtitle")

            if issue_title:
                output += f"_{issue_title}_\n\n"
            if issue_subtitle and issue_subtitle != issue_title:
                output += f"_{issue_subtitle}_\n\n"

            sections = (issue or {}).get("sections") or []
            sections_by_order = {
                section.get("order"): section
                for section in sections
                if section.get("order") is not None
            }

            if sections_by_order:
                articles_by_order: dict[int, list] = {}
                remaining_articles: list[dict] = []

                for article in category_articles:
                    order = article.get("section_order")
                    if order is None or order not in sections_by_order:
                        remaining_articles.append(article)
                        continue
                    articles_by_order.setdefault(order, []).append(article)

                sorted_sections = sorted(
                    sections,
                    key=lambda section: section.get("order", 0),
                )

                for section in sorted_sections:
                    order = section.get("order")
                    section_articles = articles_by_order.get(order, [])
                    if not section_articles:
                        continue

                    header_text = section.get("title") or ""
                    emoji = section.get("emoji")
                    if emoji:
                        header_text = f"{emoji} {header_text}".strip()

                    if header_text:
                        output += f"##### {header_text}\n\n"

                    output += build_article_lines(section_articles)

                if remaining_articles:
                    output += build_article_lines(remaining_articles)
            else:
                output += build_article_lines(category_articles)

    return output

</file>
<file path="newsletter_scraper.py">
import logging
import json
import time
from datetime import datetime

from newsletter_config import NEWSLETTER_CONFIGS
from tldr_adapter import TLDRAdapter
from newsletter_merger import build_markdown_output

import util

logger = logging.getLogger("newsletter_scraper")


def _get_adapter_for_source(config):
    """Factory pattern - returns appropriate adapter for source.

    Args:
        config: NewsletterSourceConfig instance

    Returns:
        NewsletterAdapter instance

    Raises:
        ValueError: If no adapter exists for the source
    """
    if config.source_id.startswith("tldr_"):
        return TLDRAdapter(config)
    elif config.source_id == "hackernews":
        from hackernews_adapter import HackerNewsAdapter
        return HackerNewsAdapter(config)
    elif config.source_id == "xeiaso":
        from xeiaso_adapter import XeIasoAdapter
        return XeIasoAdapter(config)
    elif config.source_id == "simon_willison":
        from simon_willison_adapter import SimonWillisonAdapter
        return SimonWillisonAdapter(config)
    elif config.source_id == "danluu":
        from danluu_adapter import DanLuuAdapter
        return DanLuuAdapter(config)
    elif config.source_id == "will_larson":
        from will_larson_adapter import WillLarsonAdapter
        return WillLarsonAdapter(config)
    elif config.source_id == "lenny_newsletter":
        from lenny_newsletter_adapter import LennyNewsletterAdapter
        return LennyNewsletterAdapter(config)
    elif config.source_id == "pragmatic_engineer":
        from pragmatic_engineer_adapter import PragmaticEngineerAdapter
        return PragmaticEngineerAdapter(config)
    elif config.source_id == "cloudflare":
        from cloudflare_adapter import CloudflareAdapter
        return CloudflareAdapter(config)
    elif config.source_id == "jessitron":
        from jessitron_adapter import JessitronAdapter
        return JessitronAdapter(config)
    elif config.source_id == "stripe_engineering":
        from stripe_engineering_adapter import StripeEngineeringAdapter
        return StripeEngineeringAdapter(config)
    elif config.source_id == "deepmind":
        from deepmind_adapter import DeepMindAdapter
        return DeepMindAdapter(config)
    elif config.source_id == "pointer":
        from pointer_adapter import PointerAdapter
        return PointerAdapter(config)
    elif config.source_id == "softwareleadweekly":
        from softwareleadweekly_adapter import SoftwareLeadWeeklyAdapter
        return SoftwareLeadWeeklyAdapter(config)
    elif config.source_id == "anthropic":
        from anthropic_adapter import AnthropicAdapter
        return AnthropicAdapter(config)
    elif config.source_id == "netflix":
        from netflix_adapter import NetflixAdapter
        return NetflixAdapter(config)
    elif config.source_id == "hillel_wayne":
        from hillel_wayne_adapter import HillelWayneAdapter
        return HillelWayneAdapter(config)
    elif config.source_id == "infoq":
        from infoq_adapter import InfoQAdapter
        return InfoQAdapter(config)
    elif config.source_id == "bytebytego":
        from bytebytego_adapter import ByteByteGoAdapter
        return ByteByteGoAdapter(config)
    elif config.source_id == "martin_fowler":
        from martin_fowler_adapter import MartinFowlerAdapter
        return MartinFowlerAdapter(config)
    elif config.source_id == "react_status":
        from react_status_adapter import ReactStatusAdapter
        return ReactStatusAdapter(config)
    elif config.source_id == "node_weekly":
        from node_weekly_adapter import NodeWeeklyAdapter
        return NodeWeeklyAdapter(config)
    else:
        raise ValueError(f"No adapter registered for source: {config.source_id}")


def _normalize_article_payload(article: dict) -> dict:
    """Normalize article dict into API payload format.

    >>> article = {"url": "https://example.com", "title": "Test", "date": "2024-01-01", "category": "Tech", "removed": None}
    >>> result = _normalize_article_payload(article)
    >>> result["removed"]
    False
    """
    payload = {
        "url": article["url"],
        "title": article["title"],
        "article_meta": article.get("article_meta", ""),
        "date": article["date"],
        "category": article["category"],
        "removed": bool(article.get("removed", False)),
    }

    if article.get("source_id"):
        payload["source_id"] = article["source_id"]
    if article.get("section_title"):
        payload["section_title"] = article["section_title"]
    if article.get("section_emoji"):
        payload["section_emoji"] = article["section_emoji"]
    if article.get("section_order") is not None:
        payload["section_order"] = article["section_order"]
    if article.get("newsletter_type"):
        payload["newsletter_type"] = article["newsletter_type"]

    return payload


def _group_articles_by_date(articles: list[dict]) -> dict[str, list[dict]]:
    """Group articles by date string.

    >>> articles = [{"date": "2024-01-01", "title": "Test"}]
    >>> result = _group_articles_by_date(articles)
    >>> "2024-01-01" in result
    True
    """
    grouped_articles: dict[str, list[dict]] = {}
    for article in articles:
        date_value = article["date"]
        if isinstance(date_value, str):
            article_date = date_value
        else:
            article_date = util.format_date_for_url(date_value)

        grouped_articles.setdefault(article_date, []).append(article)

    return grouped_articles


def _sort_issues(issues: list[dict]) -> list[dict]:
    """Sort issues by date DESC, source sort_order ASC, category ASC.

    >>> issues = [{"date": "2024-01-01", "source_id": "tldr_tech", "category": "Tech"}]
    >>> result = _sort_issues(issues)
    >>> len(result) == 1
    True
    """
    def _issue_sort_key(issue: dict) -> tuple:
        date_text = issue.get("date", "") or ""
        try:
            date_ordinal = datetime.fromisoformat(date_text).toordinal()
        except Exception:
            date_ordinal = 0

        source_id = issue.get("source_id")
        sort_order = (
            NEWSLETTER_CONFIGS[source_id].sort_order
            if source_id in NEWSLETTER_CONFIGS
            else 999
        )

        return (-date_ordinal, sort_order, issue.get("category", ""))

    return sorted(issues, key=_issue_sort_key)


def _compute_stats(
    articles: list[dict],
    url_set: set[str],
    dates: list,
    grouped_articles: dict[str, list[dict]],
    network_fetches: int,
) -> dict:
    """Compute scrape statistics.

    >>> stats = _compute_stats([], set(), [], {}, 0)
    >>> stats["total_articles"]
    0
    """
    return {
        "total_articles": len(articles),
        "unique_urls": len(url_set),
        "dates_processed": len(dates),
        "dates_with_content": len(grouped_articles),
        "network_fetches": network_fetches,
        "cache_mode": "read_write",
    }


def _build_scrape_response(
    start_date,
    end_date,
    dates,
    all_articles,
    url_set,
    issue_metadata_by_key,
    network_fetches,
):
    """Orchestrate building the complete scrape response."""
    articles_data = [_normalize_article_payload(a) for a in all_articles]
    grouped_articles = _group_articles_by_date(all_articles)
    output = build_markdown_output(
        start_date, end_date, grouped_articles, issue_metadata_by_key
    )
    issues_output = _sort_issues(list(issue_metadata_by_key.values()))
    stats = _compute_stats(
        all_articles, url_set, dates, grouped_articles, network_fetches
    )

    return {
        "success": True,
        "output": output,
        "articles": articles_data,
        "issues": issues_output,
        "stats": stats,
    }






def _collect_newsletters_for_date_from_source(
    source_id,
    config,
    date,
    date_str,
    processed_count,
    total_count,
    url_set,
    all_articles,
    issue_metadata_by_key,
    excluded_urls,
):
    """Collect newsletters for a date using source adapter.

    Args:
        source_id: Source identifier
        config: NewsletterSourceConfig instance
        date: Date object
        date_str: Date string
        processed_count: Current progress counter
        total_count: Total items to process
        url_set: Set of URLs for deduplication
        all_articles: List to append articles to
        issue_metadata_by_key: Dict to store issue metadata
        excluded_urls: List of canonical URLs to exclude

    Returns:
        Tuple of (updated_processed_count, network_articles_count)
    """
    day_articles: list[dict] = []
    network_articles = 0
    current_processed = processed_count

    current_processed += 1
    logger.info(
        f"[newsletter_scraper] Processing {config.display_name} for {date_str} ({current_processed}/{total_count})",
    )

    try:
        # Get adapter and scrape
        adapter = _get_adapter_for_source(config)
        result = adapter.scrape_date(date, excluded_urls)

        # Process articles from response
        for article in result.get("articles", []):
            canonical_url = util.canonicalize_url(article["url"])
            article["url"] = canonical_url

            day_articles.append(article)

            if canonical_url not in url_set:
                url_set.add(canonical_url)
                all_articles.append(article)
                network_articles += 1

        # Process issues from response
        for issue in result.get("issues", []):
            issue_copy = json.loads(json.dumps(issue))
            source_id = issue_copy.get("source_id", "")
            category = issue_copy.get("category", "")
            # Use triple-key to prevent collisions
            issue_metadata_by_key[(date_str, source_id, category)] = issue_copy

        # Rate limiting
        if network_articles > 0:
            time.sleep(0.2)

    except Exception as e:
        logger.error(
            f"[newsletter_scraper] Error processing {config.display_name} for {date_str}: {e}",
            exc_info=True,
        )

    return current_processed, network_articles


def scrape_date_range(start_date, end_date, source_ids=None, excluded_urls=None):
    """Scrape newsletters in date range using configured adapters.

    Args:
        start_date: Start date
        end_date: End date
        source_ids: Optional list of source IDs to scrape. If None, scrapes all configured sources.
        excluded_urls: List of canonical URLs to exclude from results

    Returns:
        Response dictionary with articles and issues
    """
    dates = util.get_date_range(start_date, end_date)

    # Default to all configured sources
    if source_ids is None:
        source_ids = list(NEWSLETTER_CONFIGS.keys())

    # Default to empty list for excluded URLs
    if excluded_urls is None:
        excluded_urls = []

    all_articles: list[dict] = []
    url_set: set[str] = set()
    processed_count = 0
    total_count = len(dates) * len(source_ids)
    network_fetches = 0
    issue_metadata_by_key: dict[tuple[str, str, str], dict] = {}  # (date, source_id, category)

    for date in dates:
        date_str = util.format_date_for_url(date)

        for source_id in source_ids:
            if source_id not in NEWSLETTER_CONFIGS:
                logger.warning(
                    f"[newsletter_scraper] Unknown source_id: {source_id}, skipping",
                )
                continue

            config = NEWSLETTER_CONFIGS[source_id]

            processed_count, network_increment = _collect_newsletters_for_date_from_source(
                source_id,
                config,
                date,
                date_str,
                processed_count,
                total_count,
                url_set,
                all_articles,
                issue_metadata_by_key,
                excluded_urls,
            )
            network_fetches += network_increment

    # Ensure all articles have removed field
    for article in all_articles:
        article.setdefault("removed", False)

    return _build_scrape_response(
        start_date,
        end_date,
        dates,
        all_articles,
        url_set,
        issue_metadata_by_key,
        network_fetches,
    )

</file>
<file path="node_weekly_adapter.py">
"""Node Weekly adapter implementation.

This adapter implements the NewsletterAdapter interface for Node Weekly newsletter,
which publishes weekly issues about Node.js news and articles.

The adapter:
1. Fetches the RSS feed to build a date-to-issue mapping (cached)
2. For a given date, finds the closest previous issue
3. Scrapes article titles and URLs from the issue page
"""

import logging
import re
import time
import xml.etree.ElementTree as ET
from datetime import datetime

import requests
from bs4 import BeautifulSoup

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("node_weekly_adapter")

# RSS feed URL
RSS_FEED_URL = "https://cprss.s3.amazonaws.com/nodeweekly.com.xml"


class NodeWeeklyAdapter(NewsletterAdapter):
    """Adapter for Node Weekly newsletter."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)
        self._date_to_issue_cache = None

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch Node Weekly articles for a specific date.

        Strategy:
        1. Fetch RSS feed to get date-to-issue mapping (cached)
        2. Find issue for the requested date (or closest previous)
        3. Scrape articles from issue page

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        date_str = util.format_date_for_url(date)

        logger.info(f"[node_weekly_adapter.scrape_date] Scraping Node Weekly for {date_str} (excluding {len(excluded_urls)} URLs)")

        try:
            issue_data = self._get_issue_for_date(date_str)

            if not issue_data:
                logger.info(f"[node_weekly_adapter.scrape_date] No issue found for {date_str}")
                return self._normalize_response([], [])

            issue_url = issue_data['url']
            issue_date = issue_data['date']
            issue_number = issue_data['issue_number']

            logger.info(f"[node_weekly_adapter.scrape_date] Found issue #{issue_number} at {issue_url} (published {issue_date})")

            scraped_articles = self._scrape_issue(issue_url, issue_date, issue_number)

            for article in scraped_articles:
                canonical_url = util.canonicalize_url(article['url'])
                if canonical_url not in excluded_set:
                    articles.append(article)

            logger.info(f"[node_weekly_adapter.scrape_date] Scraped {len(articles)} articles after filtering")

        except Exception as e:
            logger.error(f"[node_weekly_adapter.scrape_date] Error scraping Node Weekly for {date_str}: {e}", exc_info=True)

        category = self.config.category_display_names.get("newsletter", "Node Weekly")
        issues = []
        if articles:
            issues = [{
                'date': date_str,
                'source_id': self.config.source_id,
                'category': category,
                'title': f"Issue #{issue_number}" if issue_number else None,
                'subtitle': None
            }]

        return self._normalize_response(articles, issues)

    def _get_issue_for_date(self, target_date: str) -> dict | None:
        """Get issue data for a specific date by finding closest previous issue.

        Args:
            target_date: Date string in YYYY-MM-DD format

        Returns:
            Dictionary with issue data or None if not found
        """
        if self._date_to_issue_cache is None:
            self._date_to_issue_cache = self._build_date_to_issue_mapping()

        target_dt = datetime.fromisoformat(target_date)

        # Find the closest previous issue
        closest_issue = None
        closest_date = None

        for issue_date_str, issue_data in self._date_to_issue_cache.items():
            issue_dt = datetime.fromisoformat(issue_date_str)

            # Only consider issues on or before target date
            if issue_dt <= target_dt:
                if closest_date is None or issue_dt > closest_date:
                    closest_date = issue_dt
                    closest_issue = issue_data

        return closest_issue

    def _build_date_to_issue_mapping(self) -> dict[str, dict]:
        """Build mapping of dates to issue data from RSS feed.

        Returns:
            Dictionary mapping YYYY-MM-DD strings to issue data dicts
        """
        logger.info(f"[node_weekly_adapter._build_date_to_issue_mapping] Fetching RSS feed from {RSS_FEED_URL}")

        response = requests.get(RSS_FEED_URL, timeout=30)
        response.raise_for_status()

        root = ET.fromstring(response.content)
        items = root.findall('.//item')

        date_to_issue = {}

        for item in items:
            try:
                link_elem = item.find('link')
                pub_date_elem = item.find('pubDate')

                if link_elem is None or pub_date_elem is None:
                    continue

                link = link_elem.text
                pub_date_str = pub_date_elem.text

                # Parse date: "Tue, 18 Nov 2025 00:00:00 +0000"
                pub_date = datetime.strptime(pub_date_str, '%a, %d %b %Y %H:%M:%S %z')
                formatted_date = pub_date.strftime('%Y-%m-%d')

                # Extract issue number from URL: https://nodeweekly.com/issues/601
                issue_match = re.search(r'/issues/(\d+)', link)
                issue_number = issue_match.group(1) if issue_match else None

                date_to_issue[formatted_date] = {
                    'url': link,
                    'date': formatted_date,
                    'issue_number': issue_number
                }

            except Exception as e:
                logger.warning(f"[node_weekly_adapter._build_date_to_issue_mapping] Error parsing RSS item: {e}")
                continue

        logger.info(f"[node_weekly_adapter._build_date_to_issue_mapping] Built mapping for {len(date_to_issue)} issues")

        return date_to_issue

    def _scrape_issue(self, issue_url: str, date_str: str, issue_number: str) -> list[dict]:
        """Scrape articles from a Node Weekly issue page.

        Args:
            issue_url: URL of the issue page
            date_str: Date string in YYYY-MM-DD format
            issue_number: Issue number

        Returns:
            List of article dictionaries
        """
        time.sleep(0.2)  # Rate limiting

        response = requests.get(
            issue_url,
            timeout=30,
            headers={"User-Agent": self.config.user_agent}
        )
        response.raise_for_status()

        # Convert to markdown
        markdown = self._html_to_markdown(response.text)

        # Parse articles from markdown
        articles = self._parse_articles_from_markdown(markdown, date_str, issue_number)

        return articles

    def _parse_articles_from_markdown(self, markdown: str, date_str: str, issue_number: str) -> list[dict]:
        """Parse articles from markdown content.

        Args:
            markdown: Markdown content
            date_str: Date string in YYYY-MM-DD format
            issue_number: Issue number

        Returns:
            List of article dictionaries
        """
        articles = []
        category = self.config.category_display_names.get("newsletter", "Node Weekly")

        # Extract all markdown links: [title](url)
        link_pattern = re.compile(r'\[([^\]]+)\]\(([^)]+)\)')
        matches = link_pattern.findall(markdown)

        for title, url in matches:
            # Clean URL - remove any trailing quoted text from markdown title syntax
            url = url.split()[0] if ' ' in url else url

            # Skip if not a tracking link
            if 'nodeweekly.com/link/' not in url:
                continue

            # Skip meta links
            if any(skip in title.lower() for skip in [
                'read on the web',
                'unsubscribe',
                'archive',
                'view in browser',
                'sponsor',
                'node weekly',
                'node.js weekly',
                'prev',
                'next',
                '« prev',
                'next »'
            ]):
                continue

            # Skip very short titles (likely inline links)
            if len(title) < 10:
                continue

            # Clean up title - remove quotes and trailing URLs that might be in markdown link definitions
            cleaned_title = title.strip()

            # Remove trailing quoted URLs like ' "example.com"' from markdown links
            cleaned_title = re.sub(r'\s+"[^"]+"\s*$', '', cleaned_title)

            # Skip if title is just punctuation or numbers
            if re.match(r'^[\d\s\-\.\,]+$', cleaned_title):
                continue

            articles.append({
                "title": cleaned_title,
                "article_meta": "",
                "url": url,
                "category": category,
                "date": date_str,
                "newsletter_type": "newsletter",
                "removed": False,
            })

        # Deduplicate by URL
        seen_urls = set()
        unique_articles = []
        for article in articles:
            if article['url'] not in seen_urls:
                seen_urls.add(article['url'])
                unique_articles.append(article)

        return unique_articles

</file>
<file path="pointer_adapter.py">
"""Pointer newsletter adapter implementation.

This adapter implements the NewsletterAdapter interface for Pointer.io newsletter,
which provides curated reading for engineering leaders.

The adapter:
1. Fetches the archives page to build a date-to-URL mapping
2. For a given date, finds the matching issue URL
3. Scrapes article titles and URLs from the issue page
"""

import logging
import re
from datetime import datetime

import requests
from bs4 import BeautifulSoup

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("pointer_adapter")


class PointerAdapter(NewsletterAdapter):
    """Adapter for Pointer newsletter."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)
        self._date_to_url_cache = None

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch Pointer articles for a specific date.

        Strategy:
        1. Fetch archives page to get date-to-URL mapping (cached)
        2. Find issue URL for the requested date
        3. Scrape articles from issue page

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        date_str = util.format_date_for_url(date)

        logger.info(f"[pointer_adapter.scrape_date] Scraping Pointer for {date_str} (excluding {len(excluded_urls)} URLs)")

        try:
            issue_url = self._get_issue_url_for_date(date_str)

            if not issue_url:
                logger.info(f"[pointer_adapter.scrape_date] No issue found for {date_str}")
                return self._normalize_response([], [])

            logger.info(f"[pointer_adapter.scrape_date] Found issue URL: {issue_url}")

            scraped_articles = self._scrape_issue(issue_url, date_str)

            for article in scraped_articles:
                canonical_url = util.canonicalize_url(article['url'])
                if canonical_url not in excluded_set:
                    articles.append(article)

            logger.info(f"[pointer_adapter.scrape_date] Scraped {len(articles)} articles after filtering")

        except Exception as e:
            logger.error(f"[pointer_adapter.scrape_date] Error scraping Pointer for {date_str}: {e}", exc_info=True)

        category = self.config.category_display_names.get("newsletter", "Pointer")
        issues = []
        if articles:
            issues = [{
                'date': date_str,
                'source_id': self.config.source_id,
                'category': category,
                'title': None,
                'subtitle': None
            }]

        return self._normalize_response(articles, issues)

    def _get_issue_url_for_date(self, target_date: str) -> str | None:
        """Get issue URL for a specific date by parsing archives page.

        Args:
            target_date: Date string in YYYY-MM-DD format

        Returns:
            Issue URL or None if not found
        """
        if self._date_to_url_cache is None:
            self._date_to_url_cache = self._build_date_to_url_mapping()

        return self._date_to_url_cache.get(target_date)

    def _build_date_to_url_mapping(self) -> dict[str, str]:
        """Build mapping of dates to issue URLs from archives page.

        Returns:
            Dictionary mapping YYYY-MM-DD strings to issue URLs
        """
        archives_url = f"{self.config.base_url}/archives"

        logger.info(f"[pointer_adapter._build_date_to_url_mapping] Fetching archives from {archives_url}")

        response = requests.get(
            archives_url,
            timeout=30,
            headers={"User-Agent": self.config.user_agent}
        )
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a', href=True)
        issue_links = [link for link in links if '/archives/post_' in link['href']]

        date_to_url = {}
        for link in issue_links:
            href = link['href']
            text = link.get_text(strip=True)

            match = re.match(r'Issue #(\d+)(.+)', text)
            if match:
                date_str = match.group(2).strip()
                try:
                    date_obj = datetime.strptime(date_str, '%B %d, %Y')
                    formatted_date = date_obj.strftime('%Y-%m-%d')
                    full_url = f"{self.config.base_url}{href}" if not href.startswith('http') else href
                    date_to_url[formatted_date] = full_url
                except Exception:
                    continue

        logger.info(f"[pointer_adapter._build_date_to_url_mapping] Built mapping for {len(date_to_url)} issues")

        return date_to_url

    def _scrape_issue(self, issue_url: str, date_str: str) -> list[dict]:
        """Scrape articles from a Pointer issue page.

        Args:
            issue_url: URL of the issue page
            date_str: Date string in YYYY-MM-DD format

        Returns:
            List of article dictionaries
        """
        response = requests.get(
            issue_url,
            timeout=30,
            headers={"User-Agent": self.config.user_agent}
        )
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        h1_tags = soup.find_all('h1')

        articles = []
        category = self.config.category_display_names.get("newsletter", "Pointer")

        for h1 in h1_tags:
            link = h1.find('a', href=True)
            if not link or not link['href'].startswith('http'):
                continue

            url = link['href']

            if self._should_skip_url(url):
                continue

            title = link.get_text(strip=True)
            if not title:
                continue

            article_meta = self._extract_author(h1)

            articles.append({
                "title": title,
                "article_meta": article_meta,
                "url": url,
                "category": category,
                "date": date_str,
                "newsletter_type": "newsletter",
                "removed": False,
            })

        return articles

    def _should_skip_url(self, url: str) -> bool:
        """Check if URL should be skipped (ads, internal links).

        Args:
            url: URL to check

        Returns:
            True if URL should be skipped
        """
        skip_domains = [
            'pointer.io',
            'beehiiv.com',
            'getunblocked.com',
            'forms.gle',
            'goo.gl',
        ]

        return any(domain in url for domain in skip_domains)

    def _extract_author(self, h1_tag) -> str:
        """Extract author from the element following the h1 tag.

        Args:
            h1_tag: BeautifulSoup h1 tag

        Returns:
            Author string or empty string
        """
        next_p = h1_tag.find_next('p')
        if next_p:
            author_text = next_p.get_text(strip=True)
            if author_text.startswith('—'):
                return author_text
        return ""

</file>
<file path="pragmatic_engineer_adapter.py">
"""
The Pragmatic Engineer newsletter adapter using RSS feed.

This adapter fetches articles from The Pragmatic Engineer newsletter (by Gergely Orosz)
via the Substack RSS feed. The newsletter covers senior engineering topics, industry insights,
scaling teams, tech trends, and the popular "The Scoop" series.
"""

import logging
import re
from datetime import datetime
import requests
import feedparser

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("pragmatic_engineer_adapter")

RSS_FEED_URL = "https://newsletter.pragmaticengineer.com/feed"


class PragmaticEngineerAdapter(NewsletterAdapter):
    """Adapter for The Pragmatic Engineer newsletter using Substack RSS feed."""

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch newsletter articles for a specific date using RSS feed.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary with articles and issues
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date_str = util.format_date_for_url(date)
        target_date = datetime.fromisoformat(target_date_str).date()

        logger.info(f"[pragmatic_engineer_adapter.scrape_date] Fetching RSS feed for {target_date_str}")

        try:
            headers = {
                'User-Agent': self.config.user_agent
            }
            response = requests.get(RSS_FEED_URL, headers=headers, timeout=10)
            response.raise_for_status()

            feed = feedparser.parse(response.content)

            if not feed.entries:
                logger.warning(f"[pragmatic_engineer_adapter.scrape_date] No entries found in RSS feed")
                return self._normalize_response([], [])

            logger.info(f"[pragmatic_engineer_adapter.scrape_date] Fetched {len(feed.entries)} total entries from RSS")

            for entry in feed.entries:
                article = self._parse_rss_entry(entry, target_date, excluded_set)
                if article:
                    articles.append(article)

            logger.info(f"[pragmatic_engineer_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[pragmatic_engineer_adapter.scrape_date] Error fetching RSS feed: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': 'The Pragmatic Engineer',
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _parse_rss_entry(self, entry: dict, target_date: datetime.date, excluded_set: set) -> dict | None:
        """Parse RSS entry into article dict if it matches the target date.

        Args:
            entry: feedparser entry dict
            target_date: Target date to filter by
            excluded_set: Set of canonical URLs to exclude

        Returns:
            Article dictionary or None if entry should be skipped
        """
        if not entry.get('link'):
            return None

        canonical_url = util.canonicalize_url(entry['link'])
        if canonical_url in excluded_set:
            return None

        published_parsed = entry.get('published_parsed')
        if not published_parsed:
            return None

        entry_date = datetime(*published_parsed[:3]).date()

        if entry_date != target_date:
            return None

        title = entry.get('title', 'Untitled')
        summary = entry.get('summary', '')

        article_meta = self._extract_article_meta(summary)

        return {
            "title": title,
            "article_meta": article_meta,
            "url": entry['link'],
            "category": "The Pragmatic Engineer",
            "date": util.format_date_for_url(entry_date),
            "newsletter_type": "newsletter",
            "removed": False,
        }

    def _extract_article_meta(self, summary: str) -> str:
        """Extract clean article metadata from HTML summary.

        Args:
            summary: HTML summary from RSS feed

        Returns:
            Clean text excerpt for article_meta
        """
        clean_text = re.sub(r'<[^>]+>', '', summary)
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()

        if len(clean_text) > 200:
            clean_text = clean_text[:200] + '...'

        return clean_text

</file>
<file path="react_status_adapter.py">
"""React Status newsletter adapter implementation using RSS feed.

React Status is a weekly React newsletter from Cooper Press with 40k subscribers.
It covers React library updates, tutorials, tools, and community news.

The adapter:
1. Fetches the RSS feed which contains recent issues
2. For a given date, finds the matching issue
3. Parses article links from the HTML content in the RSS feed
4. Resolves tracking links to actual article URLs
"""

import logging
import re
from datetime import datetime
import feedparser
import requests
from bs4 import BeautifulSoup

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("react_status_adapter")

RSS_FEED_URL = "https://react.statuscode.com/rss"


class ReactStatusAdapter(NewsletterAdapter):
    """Adapter for React Status newsletter using RSS feed."""

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch React Status articles for a specific date using RSS feed.

        React Status publishes weekly on Wednesdays. This method:
        1. Fetches the RSS feed
        2. Finds the issue matching the target date
        3. Parses articles from the issue's HTML content
        4. Resolves tracking links to actual URLs

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary with articles and issues
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date_str = util.format_date_for_url(date)
        target_date = datetime.fromisoformat(target_date_str).date()

        logger.info(f"[react_status_adapter.scrape_date] Fetching RSS feed for {target_date_str} (excluding {len(excluded_urls)} URLs)")

        try:
            feed = feedparser.parse(RSS_FEED_URL)

            if not feed.entries:
                logger.warning("[react_status_adapter.scrape_date] No entries found in RSS feed")
                return self._normalize_response([], [])

            logger.info(f"[react_status_adapter.scrape_date] Fetched {len(feed.entries)} issues from RSS")

            # Find the issue for the target date
            matching_entry = self._find_matching_issue(feed.entries, target_date)

            if not matching_entry:
                logger.info(f"[react_status_adapter.scrape_date] No issue found for {target_date_str}")
                return self._normalize_response([], [])

            # Parse articles from the issue
            parsed_articles = self._parse_issue_articles(
                matching_entry, target_date_str, excluded_set
            )

            articles.extend(parsed_articles)

            logger.info(f"[react_status_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[react_status_adapter.scrape_date] Error fetching RSS feed: {e}", exc_info=True)

        issues = []
        if articles:
            category = self.config.category_display_names.get("newsletter", "React Status")
            issue_title = matching_entry.get('title', '') if matching_entry else None
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': category,
                'title': issue_title,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _find_matching_issue(self, entries: list, target_date: datetime.date) -> dict | None:
        """Find the RSS entry matching the target date.

        React Status publishes weekly. This finds the issue published on or
        immediately before the target date.

        Args:
            entries: List of RSS feed entries
            target_date: Target date to match

        Returns:
            Matching RSS entry or None if not found
        """
        for entry in entries:
            published_parsed = entry.get('published_parsed')
            if not published_parsed:
                continue

            entry_date = datetime(*published_parsed[:3]).date()

            # For weekly newsletters, use the exact date or find the most recent issue
            if entry_date == target_date:
                return entry

        return None

    def _parse_issue_articles(
        self, entry: dict, date_str: str, excluded_set: set
    ) -> list[dict]:
        """Parse articles from an RSS feed entry.

        Args:
            entry: RSS feed entry dictionary
            date_str: Date string in YYYY-MM-DD format
            excluded_set: Set of canonical URLs to exclude

        Returns:
            List of article dictionaries
        """
        summary_html = entry.get('summary', '')
        if not summary_html:
            return []

        soup = BeautifulSoup(summary_html, 'html.parser')
        articles = []

        # Find article titles - they have bold/large font styling
        title_spans = soup.find_all('span', style=re.compile(r'font-weight:\s*600'))

        category = self.config.category_display_names.get("newsletter", "React Status")

        for span in title_spans:
            link = span.find('a', href=True)
            if not link:
                continue

            tracking_url = link.get('href', '')
            title = link.get_text(strip=True)
            domain = link.get('title', '')

            if not title or len(title) < 5:
                continue

            # Find metadata (author/description)
            parent_p = span.find_parent('p')
            article_meta = ""

            if parent_p:
                next_p = parent_p.find_next_sibling('p')
                if next_p:
                    meta_text = next_p.get_text(strip=True)
                    if 'sponsor' not in meta_text.lower():
                        article_meta = meta_text

            # Resolve tracking link to actual URL
            try:
                actual_url = self._resolve_tracking_link(tracking_url)
                if not actual_url:
                    continue

                canonical_url = util.canonicalize_url(actual_url)

                # Skip excluded URLs and sponsor articles
                if canonical_url in excluded_set:
                    continue
                if 'sponsor' in (article_meta.lower() or '') or 'sponsor' in title.lower():
                    continue

                articles.append({
                    "title": title,
                    "article_meta": article_meta,
                    "url": canonical_url,
                    "category": category,
                    "date": date_str,
                    "newsletter_type": "newsletter",
                    "removed": False,
                })

            except Exception as e:
                logger.warning(f"[react_status_adapter._parse_issue_articles] Error processing article '{title}': {e}")
                continue

        return articles

    def _resolve_tracking_link(self, tracking_url: str) -> str | None:
        """Resolve a tracking link to the actual destination URL.

        Args:
            tracking_url: Tracking URL from the newsletter

        Returns:
            Actual destination URL or None if resolution fails
        """
        try:
            response = requests.head(
                tracking_url,
                allow_redirects=True,
                timeout=10,
                headers={"User-Agent": self.config.user_agent}
            )
            return response.url
        except Exception as e:
            logger.warning(f"[react_status_adapter._resolve_tracking_link] Error resolving {tracking_url}: {e}")
            return None

</file>
<file path="serve.py">
#!/usr/bin/env python3
"""
TLDR Newsletter Scraper backend with a proxy.
"""

from flask import Flask, request, jsonify, send_from_directory
import logging
import requests
import os
import subprocess
import pathlib

import util
import tldr_app
import storage_service
from summarizer import DEFAULT_MODEL, DEFAULT_TLDR_REASONING_EFFORT

# Configure Flask to serve React build output
app = Flask(
    __name__,
    static_folder='static/dist/assets',
    static_url_path='/assets'
)
logging.basicConfig(level=util.resolve_env_var("LOG_LEVEL", "INFO"))
logger = logging.getLogger("serve")


@app.route("/")
def index():
    """Serve the React app"""
    static_dist = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'static', 'dist')
    return send_from_directory(static_dist, 'index.html')


@app.route("/api/scrape", methods=["POST"])
def scrape_newsletters_in_date_range():
    """Backend proxy to scrape newsletters. Expects start_date, end_date, excluded_urls, and optionally sources in the request body."""
    try:
        data = request.get_json()
        if data is None:
            return jsonify({"success": False, "error": "No JSON data received"}), 400

        # Extract sources parameter (optional)
        sources = data.get("sources")
        if sources is not None and not isinstance(sources, list):
            return (
                jsonify(
                    {"success": False, "error": "sources must be an array of source IDs"}
                ),
                400,
            )

        result = tldr_app.scrape_newsletters(
            data.get("start_date"),
            data.get("end_date"),
            source_ids=sources,
            excluded_urls=data.get("excluded_urls", []),
        )
        return jsonify(result)

    except ValueError as error:
        return jsonify({"success": False, "error": str(error)}), 400
    except Exception as error:
        logger.exception(
            "[serve.scrape_newsletters_in_date_range] Failed to scrape newsletters: %s",
            error,
        )
        return jsonify({"success": False, "error": str(error)}), 500


@app.route("/api/tldr-url", methods=["POST"])
def tldr_url(model: str = DEFAULT_MODEL):
    """Create a TLDR of the content at a URL.

    Requires 'url'. Optional: 'summary_effort' to set the reasoning effort level, 'model' query param to specify OpenAI model.
    """
    try:
        data = request.get_json() or {}
        model_param = request.args.get("model", DEFAULT_MODEL)
        result = tldr_app.tldr_url(
            data.get("url", ""),
            summary_effort=data.get("summary_effort", DEFAULT_TLDR_REASONING_EFFORT),
            model=model_param,
        )

        return jsonify(result)

    except ValueError as error:
        return jsonify({"success": False, "error": str(error)}), 400
    except requests.RequestException as e:
        logger.error(
            "[serve.tldr_url] request error error=%s",
            repr(e),
            exc_info=True,
        )
        return jsonify({"success": False, "error": f"Network error: {repr(e)}"}), 502

    except Exception as e:
        logger.error(
            "[serve.tldr_url] error error=%s",
            repr(e),
            exc_info=True,
        )
        return jsonify({"success": False, "error": repr(e)}), 500


@app.route("/api/storage/setting/<key>", methods=["GET"])
def get_storage_setting(key):
    """Get setting value by key."""
    try:
        value = storage_service.get_setting(key)
        if value is None:
            return jsonify({"success": False, "error": "Setting not found"}), 404

        return jsonify({"success": True, "value": value})

    except Exception as e:
        logger.error(
            "[serve.get_storage_setting] error key=%s error=%s",
            key, repr(e),
            exc_info=True,
        )
        return jsonify({"success": False, "error": repr(e)}), 500

@app.route("/api/storage/setting/<key>", methods=["POST"])
def set_storage_setting(key):
    """Set setting value by key."""
    try:
        data = request.get_json()
        value = data['value']

        result = storage_service.set_setting(key, value)
        return jsonify({"success": True, "data": result})

    except Exception as e:
        logger.error(
            "[serve.set_storage_setting] error key=%s error=%s",
            key, repr(e),
            exc_info=True,
        )
        return jsonify({"success": False, "error": repr(e)}), 500

@app.route("/api/storage/daily/<date>", methods=["GET"])
def get_storage_daily(date):
    """Get cached payload for a specific date."""
    try:
        payload = storage_service.get_daily_payload(date)
        if payload is None:
            return jsonify({"success": False, "error": "Date not found"}), 404

        return jsonify({"success": True, "payload": payload})

    except Exception as e:
        logger.error(
            "[serve.get_storage_daily] error date=%s error=%s",
            date, repr(e),
            exc_info=True,
        )
        return jsonify({"success": False, "error": repr(e)}), 500

@app.route("/api/storage/daily/<date>", methods=["POST"])
def set_storage_daily(date):
    """Save or update daily payload."""
    try:
        data = request.get_json()
        payload = data['payload']

        result = storage_service.set_daily_payload(date, payload)
        return jsonify({"success": True, "data": result})

    except Exception as e:
        logger.error(
            "[serve.set_storage_daily] error date=%s error=%s",
            date, repr(e),
            exc_info=True,
        )
        return jsonify({"success": False, "error": repr(e)}), 500

@app.route("/api/storage/daily-range", methods=["POST"])
def get_storage_daily_range():
    """Get all cached payloads in date range."""
    try:
        data = request.get_json()
        start_date = data['start_date']
        end_date = data['end_date']

        payloads = storage_service.get_daily_payloads_range(start_date, end_date)
        return jsonify({"success": True, "payloads": payloads})

    except Exception as e:
        logger.error(
            "[serve.get_storage_daily_range] error error=%s",
            repr(e),
            exc_info=True,
        )
        return jsonify({"success": False, "error": repr(e)}), 500

@app.route("/api/storage/is-cached/<date>", methods=["GET"])
def check_storage_is_cached(date):
    """Check if a specific date exists in cache."""
    try:
        is_cached = storage_service.is_date_cached(date)
        return jsonify({"success": True, "is_cached": is_cached})

    except Exception as e:
        logger.error(
            "[serve.check_storage_is_cached] error date=%s error=%s",
            date, repr(e),
            exc_info=True,
        )
        return jsonify({"success": False, "error": repr(e)}), 500

@app.route("/api/generate-context", methods=["POST"])
def generate_context():
    """Generate context for server, client, docs, or all."""
    try:
        data = request.get_json()
        context_type = data.get('context_type')

        if context_type not in ['server', 'client', 'docs', 'all']:
            return jsonify({"success": False, "error": "Invalid context_type. Must be 'server', 'client', 'docs', or 'all'"}), 400

        root_dir = pathlib.Path(__file__).parent
        script_path = root_dir / 'scripts' / 'generate_context.py'

        if context_type == 'all':
            contents = []
            for ctx in ['docs', 'server', 'client']:
                cmd = ['python3', str(script_path), ctx]
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    cwd=root_dir
                )
                if result.returncode != 0:
                    logger.error(
                        "[serve.generate_context] script failed for %s stderr=%s",
                        ctx, result.stderr
                    )
                    return jsonify({"success": False, "error": f"Failed to generate {ctx} context: {result.stderr}"}), 500
                contents.append(result.stdout)

            combined_content = '\n\n'.join(contents)
            return jsonify({"success": True, "content": combined_content})
        else:
            cmd = ['python3', str(script_path), context_type]

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=root_dir
            )

            if result.returncode != 0:
                logger.error(
                    "[serve.generate_context] script failed stderr=%s",
                    result.stderr
                )
                return jsonify({"success": False, "error": result.stderr}), 500

            return jsonify({"success": True, "content": result.stdout})

    except Exception as e:
        logger.error(
            "[serve.generate_context] error error=%s",
            repr(e),
            exc_info=True,
        )
        return jsonify({"success": False, "error": repr(e)}), 500


if __name__ == "__main__":
    app.run(
        host="0.0.0.0",
        port=5001,
        debug=True,
        threaded=False,
        use_reloader=True,
        use_evalex=True,
        processes=1,
        use_debugger=True,
    )

</file>
<file path="simon_willison_adapter.py">
"""
Simon Willison's blog adapter using Atom RSS feed.

This adapter fetches articles from Simon Willison's blog via the Atom feed,
filtering by date and extracting article metadata.
"""

import logging
import re
from datetime import datetime
import requests
import feedparser

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("simon_willison_adapter")


class SimonWillisonAdapter(NewsletterAdapter):
    """Adapter for Simon Willison's blog using Atom RSS feed."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)
        self.feed_url = "https://simonwillison.net/atom/everything/"

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch blog posts for a specific date from RSS feed.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date = datetime.fromisoformat(util.format_date_for_url(date))
        target_date_str = target_date.strftime("%Y-%m-%d")

        logger.info(f"[simon_willison_adapter.scrape_date] Fetching articles for {target_date_str} (excluding {len(excluded_urls)} URLs)")

        try:
            response = requests.get(self.feed_url, timeout=10)
            response.raise_for_status()

            feed = feedparser.parse(response.content)

            logger.info(f"[simon_willison_adapter.scrape_date] Fetched {len(feed.entries)} total entries from feed")

            for entry in feed.entries:
                if not entry.get('published_parsed'):
                    continue

                entry_date = datetime(*entry.published_parsed[:6])
                entry_date_str = entry_date.strftime("%Y-%m-%d")

                if entry_date_str != target_date_str:
                    continue

                link = entry.get('link', '')
                if not link:
                    continue

                link = self._clean_url(link)
                canonical_url = util.canonicalize_url(link)

                if canonical_url in excluded_set:
                    continue

                article = self._entry_to_article(entry, target_date_str)
                if article:
                    articles.append(article)

            logger.info(f"[simon_willison_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[simon_willison_adapter.scrape_date] Error fetching feed: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': self.config.category_display_names.get('blog', 'Simon Willison'),
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _clean_url(self, url: str) -> str:
        """Remove feed-specific fragments from URL.

        >>> adapter = SimonWillisonAdapter(None)
        >>> adapter._clean_url("https://example.com/post/#atom-everything")
        'https://example.com/post/'
        """
        if '#atom-everything' in url:
            url = url.replace('#atom-everything', '')
        return url

    def _strip_html(self, html: str) -> str:
        """Strip HTML tags from text.

        >>> adapter = SimonWillisonAdapter(None)
        >>> adapter._strip_html("<p>Hello <b>world</b></p>")
        'Hello world'
        """
        text = re.sub(r'<[^>]+>', '', html)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def _entry_to_article(self, entry: dict, date: str) -> dict | None:
        """Convert RSS feed entry to article dict.

        Args:
            entry: feedparser entry dictionary
            date: Date string

        Returns:
            Article dictionary or None if entry should be skipped
        """
        title = entry.get('title', '')
        if not title:
            return None

        link = self._clean_url(entry.get('link', ''))
        if not link:
            return None

        summary = entry.get('summary', '')
        summary_text = self._strip_html(summary) if summary else ''

        if summary_text:
            if len(summary_text) > 200:
                summary_text = summary_text[:200] + '...'

        tags = [tag.term for tag in entry.get('tags', [])]
        if tags:
            tags_str = ', '.join(tags[:5])
            article_meta = f"Tags: {tags_str}"
        else:
            article_meta = ""

        return {
            "title": title,
            "article_meta": article_meta,
            "url": link,
            "category": self.config.category_display_names.get('blog', 'Simon Willison'),
            "date": date,
            "newsletter_type": "blog",
            "removed": False,
        }

</file>
<file path="softwareleadweekly_adapter.py">
"""
Software Lead Weekly adapter implementation.

This adapter fetches curated articles from Software Lead Weekly newsletter,
which publishes weekly issues every Friday. The adapter calculates the
appropriate issue number based on the target date and parses articles
from the HTML content.
"""

import logging
import re
from datetime import datetime, timedelta
import requests

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("softwareleadweekly_adapter")

# Reference point for issue number calculation
# Issue 677 was published on 2025-11-14 (Friday)
REFERENCE_ISSUE = 677
REFERENCE_DATE = datetime(2025, 11, 14)


class SoftwareLeadWeeklyAdapter(NewsletterAdapter):
    """Adapter for Software Lead Weekly newsletter."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch articles for a specific date.

        Software Lead Weekly publishes every Friday. This method:
        1. Calculates which Friday the target date falls on
        2. Determines the issue number based on the reference point
        3. Fetches and parses that issue's content

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date = datetime.fromisoformat(util.format_date_for_url(date))
        target_date_str = target_date.strftime("%Y-%m-%d")

        logger.info(f"[softwareleadweekly_adapter.scrape_date] Fetching articles for {target_date_str} (excluding {len(excluded_urls)} URLs)")

        # Calculate which Friday to use
        issue_date = self._get_issue_date_for_target(target_date)
        issue_number = self._calculate_issue_number(issue_date)

        logger.info(f"[softwareleadweekly_adapter.scrape_date] Target date {target_date_str} maps to issue {issue_number} ({issue_date.strftime('%Y-%m-%d')})")

        try:
            html = self.fetch_issue(str(issue_number), "newsletter")
            if html is None:
                logger.info(f"[softwareleadweekly_adapter.scrape_date] No content found for issue {issue_number}")
                return self._normalize_response([], [])

            markdown = self._html_to_markdown(html)
            parsed_articles = self.parse_articles(markdown, issue_date.strftime("%Y-%m-%d"), "newsletter")

            for article in parsed_articles:
                canonical_url = util.canonicalize_url(article['url'])
                if canonical_url not in excluded_set:
                    articles.append(article)

            logger.info(f"[softwareleadweekly_adapter.scrape_date] Found {len(articles)} articles for issue {issue_number}")

        except Exception as e:
            logger.error(f"[softwareleadweekly_adapter.scrape_date] Error fetching issue {issue_number}: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': issue_date.strftime("%Y-%m-%d"),
                'source_id': self.config.source_id,
                'category': self.config.category_display_names.get('newsletter', 'Software Lead Weekly'),
                'title': f"Issue #{issue_number}",
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _get_issue_date_for_target(self, target_date: datetime) -> datetime:
        """Get the Friday that corresponds to the target date.

        Software Lead Weekly publishes every Friday. This returns:
        - The target date if it's a Friday
        - The previous Friday if target is not a Friday

        >>> adapter = SoftwareLeadWeeklyAdapter(None)
        >>> result = adapter._get_issue_date_for_target(datetime(2025, 11, 14))
        >>> result.strftime("%Y-%m-%d")
        '2025-11-14'
        >>> result = adapter._get_issue_date_for_target(datetime(2025, 11, 16))
        >>> result.strftime("%Y-%m-%d")
        '2025-11-14'
        """
        weekday = target_date.weekday()
        if weekday == 4:
            return target_date
        days_since_friday = (weekday - 4) % 7
        return target_date - timedelta(days=days_since_friday)

    def _calculate_issue_number(self, issue_date: datetime) -> int:
        """Calculate issue number based on date.

        Uses the reference point (issue 677 = 2025-11-14) and calculates
        the issue number based on weekly intervals.

        >>> adapter = SoftwareLeadWeeklyAdapter(None)
        >>> adapter._calculate_issue_number(datetime(2025, 11, 14))
        677
        >>> adapter._calculate_issue_number(datetime(2025, 11, 7))
        676
        >>> adapter._calculate_issue_number(datetime(2025, 11, 21))
        678
        """
        days_diff = (issue_date - REFERENCE_DATE).days
        weeks_diff = days_diff // 7
        return REFERENCE_ISSUE + weeks_diff

    def fetch_issue(self, issue_number: str, newsletter_type: str) -> str | None:
        """Fetch raw HTML for a specific issue.

        Args:
            issue_number: Issue number as string
            newsletter_type: Type (not used, included for interface compatibility)

        Returns:
            HTML content as string, or None if issue not found
        """
        url = f"https://softwareleadweekly.com/issues/{issue_number}"

        try:
            response = requests.get(
                url,
                headers={"User-Agent": self.config.user_agent},
                timeout=10
            )

            if response.status_code == 404:
                logger.info(f"[softwareleadweekly_adapter.fetch_issue] Issue {issue_number} not found (404)")
                return None

            response.raise_for_status()
            return response.text

        except Exception as e:
            logger.error(f"[softwareleadweekly_adapter.fetch_issue] Error fetching issue {issue_number}: {e}", exc_info=True)
            return None

    def parse_articles(self, markdown: str, date: str, newsletter_type: str) -> list[dict]:
        """Parse articles from markdown content.

        Args:
            markdown: Converted markdown content
            date: Date string for the issue
            newsletter_type: Type (not used, included for interface compatibility)

        Returns:
            List of article dictionaries
        """
        articles = []
        current_section = None

        lines = markdown.split('\n')
        i = 0

        while i < len(lines):
            line = lines[i].strip()

            if line.startswith('### ') and not line.startswith('### Subscribe'):
                current_section = line[4:].strip()
                logger.info(f"[softwareleadweekly_adapter.parse_articles] Found section: {current_section}")
                i += 1
                continue

            link_match = re.match(r'\[(.*?)\]\((https?://[^\s\)]+)', line)
            if link_match:
                title = link_match.group(1)
                url = link_match.group(2)

                if any(domain in url for domain in ['getpocket.com', 'instapaper.com', 'twitter.com/share']):
                    i += 1
                    continue

                read_time = ""
                description = ""

                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    read_time_match = re.match(r'(\d+)\s+minutes?\s+read\.?', next_line)
                    if read_time_match:
                        read_time = read_time_match.group(0)

                        desc_lines = []
                        j = i + 2
                        while j < len(lines):
                            desc_line = lines[j].strip()
                            if not desc_line:
                                j += 1
                                continue
                            if desc_line.startswith('**Read**') or desc_line.startswith('['):
                                break
                            desc_lines.append(desc_line)
                            j += 1

                        description = ' '.join(desc_lines)

                article_meta = read_time if read_time else ""
                category = current_section or "Software Lead Weekly"

                article = {
                    "title": title,
                    "article_meta": article_meta,
                    "url": url,
                    "category": category,
                    "date": date,
                    "newsletter_type": "newsletter",
                    "removed": False,
                }

                articles.append(article)

            i += 1

        return articles

    def extract_issue_metadata(self, markdown: str, date: str, newsletter_type: str) -> dict | None:
        """Extract issue metadata from markdown.

        Args:
            markdown: Converted markdown content
            date: Date string for the issue
            newsletter_type: Type (not used, included for interface compatibility)

        Returns:
            Dictionary with issue metadata
        """
        title_match = re.search(r'Issue #(\d+),\s+(.+)', markdown)
        if title_match:
            issue_number = title_match.group(1)
            issue_date = title_match.group(2)
            return {
                'date': date,
                'source_id': self.config.source_id,
                'category': self.config.category_display_names.get('newsletter', 'Software Lead Weekly'),
                'title': f"Issue #{issue_number}",
                'subtitle': issue_date
            }

        return None

</file>
<file path="storage_service.py">
import supabase_client

def get_setting(key):
    """
    Get setting value by key.

    >>> get_setting('cache:enabled')
    True
    """
    supabase = supabase_client.get_supabase_client()
    result = supabase.table('settings').select('value').eq('key', key).execute()

    if result.data:
        return result.data[0]['value']
    return None

def set_setting(key, value):
    """
    Set setting value by key (upsert).

    >>> set_setting('cache:enabled', False)
    {'key': 'cache:enabled', 'value': False, ...}
    """
    supabase = supabase_client.get_supabase_client()
    result = supabase.table('settings').upsert({
        'key': key,
        'value': value
    }).execute()

    return result.data[0] if result.data else None

def get_daily_payload(date):
    """
    Get cached payload for a specific date.

    >>> get_daily_payload('2025-11-09')
    {'date': '2025-11-09', 'articles': [...], ...}
    """
    supabase = supabase_client.get_supabase_client()
    result = supabase.table('daily_cache').select('payload').eq('date', date).execute()

    if result.data:
        return result.data[0]['payload']
    return None

def set_daily_payload(date, payload):
    """
    Save or update daily payload (upsert).

    >>> set_daily_payload('2025-11-09', {'date': '2025-11-09', 'articles': [...]})
    {'date': '2025-11-09', 'payload': {...}, ...}
    """
    supabase = supabase_client.get_supabase_client()
    result = supabase.table('daily_cache').upsert({
        'date': date,
        'payload': payload
    }).execute()

    return result.data[0] if result.data else None

def get_daily_payloads_range(start_date, end_date):
    """
    Get all cached payloads in date range (inclusive).

    >>> get_daily_payloads_range('2025-11-07', '2025-11-09')
    [{'date': '2025-11-09', ...}, {'date': '2025-11-08', ...}, ...]
    """
    supabase = supabase_client.get_supabase_client()
    result = supabase.table('daily_cache') \
        .select('payload') \
        .gte('date', start_date) \
        .lte('date', end_date) \
        .order('date', desc=True) \
        .execute()

    return [row['payload'] for row in result.data]

def is_date_cached(date):
    """
    Check if a specific date exists in cache.

    >>> is_date_cached('2025-11-09')
    True
    """
    supabase = supabase_client.get_supabase_client()
    result = supabase.table('daily_cache').select('date').eq('date', date).execute()

    return len(result.data) > 0
</file>
<file path="stripe_engineering_adapter.py">
"""
Stripe Engineering Blog adapter using Firecrawl API.

This adapter fetches articles from Stripe's Engineering Blog via Firecrawl,
filtering by date and extracting article metadata.
"""

import logging
import re
import os
from datetime import datetime
from firecrawl import FirecrawlApp

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("stripe_engineering_adapter")


class StripeEngineeringAdapter(NewsletterAdapter):
    """Adapter for Stripe Engineering Blog using Firecrawl API."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)
        self.blog_url = "https://stripe.com/blog/engineering"
        api_key = util.resolve_env_var("FIRECRAWL_API_KEY")
        self.firecrawl = FirecrawlApp(api_key=api_key)

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch blog posts for a specific date from Stripe Engineering Blog.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date = datetime.fromisoformat(util.format_date_for_url(date))
        target_date_str = target_date.strftime("%Y-%m-%d")

        logger.info(f"[stripe_engineering_adapter.scrape_date] Fetching articles for {target_date_str} (excluding {len(excluded_urls)} URLs)")

        try:
            result = self.firecrawl.scrape(self.blog_url)
            markdown = result.markdown

            logger.info(f"[stripe_engineering_adapter.scrape_date] Successfully scraped blog page")

            parsed_articles = self._parse_articles_from_markdown(markdown)

            logger.info(f"[stripe_engineering_adapter.scrape_date] Parsed {len(parsed_articles)} total articles from blog")

            for article in parsed_articles:
                article_date_str = article.get('date', '')
                if article_date_str != target_date_str:
                    continue

                url = article.get('url', '')
                if not url:
                    continue

                canonical_url = util.canonicalize_url(url)

                if canonical_url in excluded_set:
                    continue

                article['url'] = canonical_url
                articles.append(article)

            logger.info(f"[stripe_engineering_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[stripe_engineering_adapter.scrape_date] Error fetching blog: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': self.config.category_display_names.get('engineering', 'Stripe Engineering'),
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _parse_articles_from_markdown(self, markdown: str) -> list[dict]:
        """Parse articles from Stripe blog markdown.

        >>> adapter = StripeEngineeringAdapter(None)
        >>> md = '## [Engineering]\\n\\n# [Test Article](https://stripe.com/blog/test)\\n\\n[January 15, 2024](https://stripe.com/blog/test)\\n\\nTest summary.'
        >>> articles = adapter._parse_articles_from_markdown(md)
        >>> len(articles) > 0
        True
        """
        articles = []

        article_pattern = r'## \[Engineering\][^\n]*\n+# \[([^\]]+)\]\(([^\)]+)\)[^\n]*\n+\[([^\]]+)\]\([^\)]+\)[^\n]*\n+(.*?)(?=\n## \[Engineering\]|$)'

        matches = re.findall(article_pattern, markdown, re.DOTALL)

        for title, url, date_str, summary_section in matches:
            try:
                parsed_date = datetime.strptime(date_str, "%B %d, %Y")
                formatted_date = parsed_date.strftime("%Y-%m-%d")
            except Exception:
                logger.warning(f"[stripe_engineering_adapter._parse_articles_from_markdown] Could not parse date: {date_str}")
                continue

            author_name = self._extract_author(summary_section)
            summary_text = self._extract_summary(summary_section)

            article_meta_parts = []
            if author_name:
                article_meta_parts.append(f"By {author_name}")
            if summary_text:
                article_meta_parts.append(summary_text)

            article_meta = " | ".join(article_meta_parts) if article_meta_parts else ""

            articles.append({
                "title": title,
                "article_meta": article_meta,
                "url": url,
                "category": self.config.category_display_names.get('engineering', 'Stripe Engineering'),
                "date": formatted_date,
                "newsletter_type": "engineering",
                "removed": False,
            })

        return articles

    def _extract_author(self, summary_section: str) -> str:
        """Extract author name from summary section.

        >>> adapter = StripeEngineeringAdapter(None)
        >>> summary = '](link)[John Doe](author_link) Engineering Team'
        >>> adapter._extract_author(summary)
        'John Doe'
        """
        author_match = re.search(r'\]\([^\)]+\)\[([^\]]+)\]', summary_section)
        if author_match:
            return author_match.group(1)
        return ""

    def _extract_summary(self, summary_section: str) -> str:
        """Extract summary text from summary section.

        >>> adapter = StripeEngineeringAdapter(None)
        >>> summary = 'Some author info\\n\\n![image](url)\\n\\nActual summary text here.\\n\\n[Read more](url)'
        >>> result = adapter._extract_summary(summary)
        >>> 'Actual summary text' in result
        True
        """
        summary_lines = [
            line.strip()
            for line in summary_section.split('\n')
            if line.strip()
            and not line.strip().startswith('![')
            and not line.strip().startswith('[Read more')
            and not line.strip().startswith('[![')
            and len(line.strip()) > 30
        ]

        if summary_lines:
            summary_text = summary_lines[-1]
            if len(summary_text) > 200:
                summary_text = summary_text[:200] + '...'
            return summary_text

        return ""

</file>
<file path="summarizer.py">
import logging
import json
import re
from requests.models import Response
from typing import Optional

import requests
from curl_cffi import requests as curl_requests
from markitdown import MarkItDown

import util
import urllib.parse as urlparse

logger = logging.getLogger("summarizer")
md = MarkItDown()

_TLDR_PROMPT_CACHE = None

SUMMARY_EFFORT_OPTIONS = ("minimal", "low", "medium", "high")
DEFAULT_TLDR_REASONING_EFFORT = "low"
DEFAULT_MODEL = "gpt-5"


def normalize_summary_effort(value: str) -> str:
    """Normalize summary effort value to a supported option."""
    if not isinstance(value, str):
        return DEFAULT_TLDR_REASONING_EFFORT

    normalized = value.strip().lower()
    if normalized in SUMMARY_EFFORT_OPTIONS:
        return normalized

    return DEFAULT_TLDR_REASONING_EFFORT


def _is_github_repo_url(url: str) -> bool:
    """Check if URL is a GitHub repository URL."""
    pattern = r"^https?://(?:www\.)?github\.com/([^/]+)/([^/?#]+)/?(?:\?.*)?(?:#.*)?$"
    return bool(re.match(pattern, url))


def _build_jina_reader_url(url: str) -> str:
    """Build r.jina.ai reader URL for a target page.

    >>> _build_jina_reader_url('https://openai.com/index/introducing-agentkit')
    'https://r.jina.ai/http://openai.com/index/introducing-agentkit'
    >>> _build_jina_reader_url('https://example.com/path?x=1')
    'https://r.jina.ai/http://example.com/path?x=1'
    """
    parsed = urlparse.urlparse(url)
    # Use http scheme within the reader path; it will follow redirects as needed
    target = f"http://{parsed.netloc}{parsed.path}"
    if parsed.query:
        target += f"?{parsed.query}"
    return f"https://r.jina.ai/{target}"


def _scrape_with_curl_cffi(
    url: str, *, timeout: int = 10, allow_redirects: bool = True
) -> requests.Response:
    response = curl_requests.get(
        url,
        impersonate="chrome131",
        timeout=timeout,
        allow_redirects=allow_redirects,
        headers={
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://www.google.com/",
        },
    )
    response.raise_for_status()

    def response_iter_content_stub(self, *args, **kwargs):
        return [response.content]

    response.__class__.iter_content = response_iter_content_stub
    return response


def _scrape_with_jina_reader(url: str, *, timeout: int) -> requests.Response:
    reader_url = _build_jina_reader_url(url)
    logger.info(
        f"[summarizer.scrape_url] Scraping with Jina reader url={url}",
    )
    response = requests.get(
        reader_url,
        timeout=timeout,
        headers={"User-Agent": "Mozilla/5.0 (compatible; TLDR-Newsletter/1.0)"},
    )
    response.raise_for_status()
    text = response.text
    if re.search(r"error \d+", text, flags=re.IGNORECASE):
        raise requests.HTTPError(
            "Jina reader returned an error page", response=response
        )

    return response


def _scrape_with_firecrawl(url: str, *, timeout: int) -> requests.Response:
    api_key = util.resolve_env_var("FIRECRAWL_API_KEY", "")
    if not api_key:
        raise RuntimeError("FIRECRAWL_API_KEY not configured")

    logger.info(
        f"[summarizer.scrape_url] Scraping with Firecrawl url={url}",
    )

    response = requests.post(
        "https://api.firecrawl.dev/v1/scrape",
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        },
        json={
            "url": url,
            "formats": ["markdown", "html"],
        },
        timeout=timeout,
    )
    response.raise_for_status()

    data = response.json()
    if not data.get("success"):
        raise requests.HTTPError("Firecrawl scraping failed", response=response)

    html_content = data.get("data", {}).get("html", "")
    if not html_content:
        raise RuntimeError("Firecrawl returned empty content")

    # Create a mock Response object with the HTML content
    # (MarkItDown will convert it to markdown)
    from io import BytesIO
    from urllib3.response import HTTPResponse

    mock_response = requests.Response()
    mock_response.status_code = 200
    mock_response._content = html_content.encode("utf-8")
    mock_response.headers["Content-Type"] = "text/html"

    # Create a proper raw response for iter_content to work
    raw = HTTPResponse(
        body=BytesIO(html_content.encode("utf-8")),
        headers={"Content-Type": "text/html"},
        status=200,
        preload_content=False,
    )
    mock_response.raw = raw

    return mock_response


def scrape_url(url: str, *, timeout: int = 10) -> Response:
    scraping_methods = [
        ("curl_cffi", _scrape_with_curl_cffi),
        ("jina_reader", _scrape_with_jina_reader),
    ]

    # Add Firecrawl as fallback if API key is configured
    firecrawl_api_key = util.resolve_env_var("FIRECRAWL_API_KEY", "")
    if firecrawl_api_key:
        scraping_methods.append(("firecrawl", _scrape_with_firecrawl))

    last_status_error: Optional[requests.HTTPError] = None
    errors = []

    for name, scrape in scraping_methods:
        try:
            # Use extended timeout for Firecrawl since it does full browser rendering
            method_timeout = 60 if name == "firecrawl" else timeout
            result = scrape(url, timeout=method_timeout)
            # Only log intermediate failures if all methods fail
            if errors:
                logger.info(
                    f"[summarizer.scrape_url] {name} succeeded after {len(errors)} failed attempts for url={url}",
                )
            return result
        except requests.HTTPError as status_error:
            last_status_error = status_error
            errors.append(f"{name}: {status_error}")
            continue
        except Exception as e:
            errors.append(f"{name}: {e}")
            continue

    # Only log errors if all methods failed
    if errors:
        logger.error(
            f"[summarizer.scrape_url] All methods failed for url={url}. Errors: {'; '.join(errors)}",
        )

    if last_status_error is not None:
        raise last_status_error

    raise RuntimeError(f"Failed to scrape {url}")


def _fetch_github_readme(url: str) -> str:
    """Fetch README.md content from a GitHub repository URL."""
    match = re.match(
        r"^https?://(?:www\.)?github\.com/([^/]+)/([^/?#]+)/?(?:\?.*)?(?:#.*)?$", url
    )
    if not match:
        raise ValueError(f"Invalid GitHub repo URL: {url}")

    owner, repo = match.groups()

    raw_url = f"https://raw.githubusercontent.com/{owner}/{repo}/main/README.md"
    logger.info(
        f"[summarizer._fetch_github_readme] Trying raw fetch from {raw_url}",
    )
    github_api_token = util.resolve_env_var("GITHUB_API_TOKEN", "")
    auth_headers = {
        "Authorization": f"token {github_api_token}",
        "User-Agent": "Mozilla/5.0 (compatible; TLDR-Newsletter/1.0)",
    }

    try:
        response = requests.get(
            raw_url,
            timeout=10,
            headers=auth_headers,
        )
        response.raise_for_status()
        logger.info(
            f"[summarizer._fetch_github_readme] Raw fetch succeeded for {raw_url}",
        )
        return md.convert_response(response).markdown
    except requests.HTTPError as e:
        if e.response and e.response.status_code == 404:
            master_url = (
                f"https://raw.githubusercontent.com/{owner}/{repo}/master/README.md"
            )
            logger.info(
                f"[summarizer._fetch_github_readme] Main branch not found, trying master: {master_url}",
            )
            try:
                response = requests.get(
                    master_url,
                    timeout=10,
                    headers=auth_headers,
                )
                response.raise_for_status()
                logger.info(
                    f"[summarizer._fetch_github_readme] Master branch fetch succeeded for {master_url}",
                )
                return md.convert_response(response).markdown
            except Exception:
                logger.warning(
                    f"[summarizer._fetch_github_readme] Master branch fetch failed for {master_url}",
                )
                raise

    response = scrape_url(url)
    result = md.convert_response(response)
    content = result.markdown

    logger.info(
        f"[summarizer._fetch_github_readme] Direct fetch succeeded for {url}",
    )
    return content


def url_to_markdown(url: str) -> str:
    """Fetch URL and convert to markdown. For GitHub repos, fetches README.md."""
    logger.info(
        f"[summarizer.url_to_markdown] Fetching and converting to markdown {url}",
    )

    if _is_github_repo_url(url):
        return _fetch_github_readme(url)

    response = scrape_url(url)
    markdown = md.convert_response(response).markdown

    return markdown


def tldr_url(url: str, summary_effort: str = DEFAULT_TLDR_REASONING_EFFORT, model: str = DEFAULT_MODEL) -> str:
    """Get markdown content from URL and create a TLDR with LLM.

    Args:
        url: The URL to TLDR
        summary_effort: OpenAI reasoning effort level
        model: OpenAI model to use

    Returns:
        The TLDR markdown
    """
    effort = normalize_summary_effort(summary_effort)
    markdown = url_to_markdown(url)

    template = _fetch_tldr_prompt()
    prompt = f"{template}\n\n<tldr this>\n{markdown}/n</tldr this>"
    tldr = _call_llm(prompt, summary_effort=effort, model=model)

    return tldr


def _fetch_prompt(
    *,
    owner: str,
    repo: str,
    path: str,
    ref: str,
    cache_attr: str,
) -> str:
    cache_value = globals().get(cache_attr)
    if cache_value:
        return cache_value

    url = f"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={ref}"

    headers = {
        "Accept": "application/vnd.github.v3.raw",
        "User-Agent": "Mozilla/5.0 (compatible; TLDR-Newsletter/1.0)",
    }

    token = util.resolve_env_var("GITHUB_API_TOKEN", "")
    if token:
        headers["Authorization"] = f"token {token}"

    response = requests.get(url, headers=headers, timeout=10)

    if response.status_code == 200:
        globals()[cache_attr] = response.text
        return response.text

    if response.headers.get("Content-Type", "").startswith("application/json"):
        import base64

        data = response.json()
        if isinstance(data, dict) and "content" in data:
            decoded = base64.b64decode(data["content"]).decode(
                "utf-8", errors="replace"
            )
            globals()[cache_attr] = decoded
            return decoded

    if token and response.status_code == 401:
        headers_no_auth = {
            "Accept": "application/vnd.github.v3.raw",
            "User-Agent": "Mozilla/5.0 (compatible; TLDR-Newsletter/1.0)",
        }
        response_no_auth = requests.get(url, headers=headers_no_auth, timeout=10)
        if response_no_auth.status_code == 200:
            globals()[cache_attr] = response_no_auth.text
            return response_no_auth.text

    raise RuntimeError(f"Failed to fetch {path}: {response.status_code}")


def _fetch_tldr_prompt(
    owner: str = "giladbarnea",
    repo: str = "llm-templates",
    path: str = "text/tldr.md",
    ref: str = "main",
) -> str:
    """Fetch TLDR prompt from GitHub (cached in memory)."""
    return _fetch_prompt(
        owner=owner,
        repo=repo,
        path=path,
        ref=ref,
        cache_attr="_TLDR_PROMPT_CACHE",
    )


def _call_llm(prompt: str, summary_effort: str = DEFAULT_TLDR_REASONING_EFFORT, model: str = DEFAULT_MODEL) -> str:
    """Call OpenAI API with prompt."""
    api_key = util.resolve_env_var("OPENAI_API_KEY", "")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY not set")
    if not prompt.strip():
        raise ValueError("Prompt is empty")

    url = "https://api.openai.com/v1/responses"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    body = {
        "model": model,
        "input": prompt,
        "reasoning": {"effort": normalize_summary_effort(summary_effort)},
        "stream": False,
    }

    resp = requests.post(url, headers=headers, data=json.dumps(body), timeout=600)
    resp.raise_for_status()
    data = resp.json()

    if isinstance(data, dict) and "output_text" in data:
        if isinstance(data["output_text"], str):
            return data["output_text"]
        if isinstance(data["output_text"], list):
            return "\n".join([
                str(x) for x in data["output_text"] if isinstance(x, str)
            ])

    outputs = data.get("output") or []
    texts = []
    for item in outputs:
        for c in item.get("content") or []:
            if c.get("type") in ("output_text", "text") and isinstance(
                c.get("text"), str
            ):
                texts.append(c["text"])
    if texts:
        return "\n".join(texts)

    choices = data.get("choices") or []
    if choices:
        msg = choices[0].get("message") or {}
        content = msg.get("content")
        if isinstance(content, str):
            return content
    assert data, "No LLM output found"
    return json.dumps(data)

</file>
<file path="supabase_client.py">
from supabase import create_client
import util
import ssl

_original_create_default_context = ssl.create_default_context

def _create_unverified_context(*args, **kwargs):
    context = _original_create_default_context(*args, **kwargs)
    context.check_hostname = False
    context.verify_mode = ssl.CERT_NONE
    return context

ssl.create_default_context = _create_unverified_context

_supabase_client = None

def get_supabase_client():
    global _supabase_client
    if _supabase_client is None:
        url = util.resolve_env_var("SUPABASE_URL")
        key = util.resolve_env_var("SUPABASE_SERVICE_KEY")
        _supabase_client = create_client(url, key)
    return _supabase_client
</file>
<file path="tldr_adapter.py">
"""
TLDR-specific newsletter adapter implementation.

This adapter implements the NewsletterAdapter interface for TLDR newsletters,
handling the specific URL patterns, parsing rules, and content structure of
TLDR Tech and TLDR AI newsletters.
"""

import logging
import re
import time
import unicodedata
from dataclasses import asdict, dataclass

import requests

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("tldr_adapter")


@dataclass
class NewsletterSection:
    """Represents a section within a newsletter issue."""

    order: int
    title: str
    emoji: str | None = None


@dataclass
class NewsletterIssue:
    """Represents metadata for a newsletter issue."""

    date: str
    newsletter_type: str
    category: str
    title: str | None
    subtitle: str | None
    sections: list[NewsletterSection]


@dataclass
class ParsedMarkdown:
    """Structured result from parsing markdown once."""

    issue_title: str | None
    issue_subtitle: str | None
    sections: list[NewsletterSection]
    sections_by_order: dict[int, NewsletterSection]
    article_candidates: list[dict]


class TLDRAdapter(NewsletterAdapter):
    """Adapter for TLDR newsletter sources (Tech, AI, etc.)."""

    def fetch_issue(self, date: str, newsletter_type: str) -> str | None:
        """Fetch TLDR newsletter HTML for a specific date and type.

        Args:
            date: Date string in YYYY-MM-DD format
            newsletter_type: Newsletter type (e.g., "tech", "ai")

        Returns:
            HTML content as string, or None if not found
        """
        date_str = util.format_date_for_url(date)

        # Build URL from config pattern
        url = self.config.url_pattern.format(
            base_url=self.config.base_url, type=newsletter_type, date=date_str
        )

        try:
            net_start = time.time()
            response = requests.get(
                url,
                timeout=30,
                headers={"User-Agent": self.config.user_agent},
                allow_redirects=False,
            )
            net_ms = int(round((time.time() - net_start) * 1000))

            if response.status_code == 404:
                return None

            response.raise_for_status()

            if response.is_redirect:
                return None

            logger.info(f"[tldr_adapter.fetch_issue] Fetched {newsletter_type} for {date_str} in {net_ms}ms")

            return response.text

        except requests.RequestException:
            logger.error(f"[tldr_adapter.fetch_issue] Request error for url={url}", exc_info=True)
            return None

    def _parse_markdown_structure(
        self, markdown: str, date: str, newsletter_type: str
    ) -> ParsedMarkdown:
        """Parse markdown once into structured format.

        Extracts all structure (headings, sections, links) in a single pass.
        """
        lines = markdown.split("\n")
        heading_pattern = re.compile(r"^(#+)\s*(.*)$")

        issue_title = None
        issue_subtitle = None
        sections: list[NewsletterSection] = []
        sections_by_order: dict[int, NewsletterSection] = {}
        article_candidates: list[dict] = []

        current_section_order: int | None = None
        pending_section_emoji = None
        section_counter = 0
        seen_title = False

        for raw_line in lines:
            line = raw_line.strip()

            if not line:
                continue

            heading_match = heading_pattern.match(line)
            if heading_match:
                level = len(heading_match.group(1))
                text = heading_match.group(2).strip()

                if not text:
                    continue

                if level == 1 and issue_title is None:
                    issue_title = text
                    seen_title = True
                    pending_section_emoji = None
                    continue

                if level <= 2 and seen_title and issue_subtitle is None:
                    issue_subtitle = text

                if level >= 2:
                    if not re.search(r"[A-Za-z0-9]", text):
                        pending_section_emoji = text.strip()
                        continue

                    emoji = None
                    title_text = text

                    split_match = re.match(r"^([^\w\d]+)\s+(.*)$", title_text)
                    if split_match and split_match.group(2).strip():
                        potential_emoji = split_match.group(1).strip()
                        remainder = split_match.group(2).strip()
                        if potential_emoji and not re.search(
                            r"[A-Za-z0-9]", potential_emoji
                        ):
                            emoji = potential_emoji
                            title_text = remainder

                    if pending_section_emoji and not emoji:
                        emoji = pending_section_emoji.strip()

                    pending_section_emoji = None

                    if not title_text:
                        continue

                    section_counter += 1
                    section = NewsletterSection(
                        order=section_counter, title=title_text, emoji=emoji or None
                    )
                    sections.append(section)
                    sections_by_order[section_counter] = section
                    current_section_order = section_counter
                    continue

            if self._is_symbol_only_line(line):
                pending_section_emoji = line.strip()
                continue

            link_matches = re.findall(r"\[([^\]]+)\]\(([^)]+)\)", line)
            for title, url in link_matches:
                if not url.startswith("http"):
                    continue
                if self._is_file_url(url):
                    continue

                article_candidates.append(
                    {
                        "title": title,
                        "url": url,
                        "section_order": current_section_order,
                    }
                )

        return ParsedMarkdown(
            issue_title=issue_title,
            issue_subtitle=issue_subtitle,
            sections=sections,
            sections_by_order=sections_by_order,
            article_candidates=article_candidates,
        )

    def parse_articles(
        self, markdown: str, date: str, newsletter_type: str
    ) -> list[dict]:
        """Parse TLDR articles from markdown content.

        Args:
            markdown: Markdown content converted from HTML
            date: Date string for the issue
            newsletter_type: Newsletter type (e.g., "tech", "ai")

        Returns:
            List of article dictionaries
        """
        parsed = self._parse_markdown_structure(markdown, date, newsletter_type)

        article_pattern = re.compile(self.config.article_pattern, re.IGNORECASE)
        category = self.config.category_display_names.get(
            newsletter_type, f"TLDR {newsletter_type.capitalize()}"
        )

        articles = []
        for candidate in parsed.article_candidates:
            if not article_pattern.search(candidate["title"]):
                continue

            cleaned_title = candidate["title"].strip()
            cleaned_title = re.sub(r"^#+\s*", "", cleaned_title)
            cleaned_title = re.sub(r"^\s*\d+\.\s*", "", cleaned_title)

            meta_match = re.search(r"\s*\((\d+)\s+minutes?\s+read\)|\s*\(GitHub\s+Repo\)", cleaned_title, re.IGNORECASE)
            if meta_match:
                article_meta = meta_match.group(0).strip().strip('()')
                cleaned_title = cleaned_title[:meta_match.start()].strip()
            else:
                article_meta = ""

            article = {
                "title": cleaned_title,
                "article_meta": article_meta,
                "url": candidate["url"].strip(),
                "category": category,
                "date": util.format_date_for_url(date),
                "newsletter_type": newsletter_type,
            }

            section_order = candidate["section_order"]
            if section_order is not None:
                section = parsed.sections_by_order.get(section_order)
                if section is not None:
                    article["section_title"] = section.title
                    if section.emoji:
                        article["section_emoji"] = section.emoji
                    article["section_order"] = section_order

            articles.append(article)

        return articles

    def extract_issue_metadata(
        self, markdown: str, date: str, newsletter_type: str
    ) -> dict | None:
        """Extract TLDR issue metadata (title, subtitle, sections).

        Args:
            markdown: Markdown content converted from HTML
            date: Date string for the issue
            newsletter_type: Newsletter type (e.g., "tech", "ai")

        Returns:
            Dictionary with issue metadata, or None if no metadata found
        """
        parsed = self._parse_markdown_structure(markdown, date, newsletter_type)

        if not parsed.issue_title and not parsed.issue_subtitle and not parsed.sections:
            return None

        category = self.config.category_display_names.get(
            newsletter_type, f"TLDR {newsletter_type.capitalize()}"
        )

        metadata_sections = parsed.sections
        if parsed.issue_subtitle and parsed.sections:
            subtitle_text = parsed.issue_subtitle
            if parsed.sections[0].title == subtitle_text:
                metadata_sections = parsed.sections[1:]

        issue = NewsletterIssue(
            date=util.format_date_for_url(date),
            newsletter_type=newsletter_type,
            category=category,
            title=parsed.issue_title,
            subtitle=parsed.issue_subtitle,
            sections=metadata_sections,
        )

        return asdict(issue)

    @staticmethod
    def _is_symbol_only_line(text: str) -> bool:
        """Check if a line contains only symbols/emoji (no alphanumeric chars).

        Args:
            text: Text to check

        Returns:
            True if line contains only symbols
        """
        stripped = text.strip()
        if not stripped:
            return False

        if any(character.isalnum() for character in stripped):
            return False

        has_symbol = False

        for character in stripped:
            category = unicodedata.category(character)

            # Punctuation means not a pure symbol line
            if category.startswith("P"):
                return False

            # Symbol characters
            if category in {"So", "Sk"}:
                has_symbol = True
                continue

            # Allow modifiers and control chars
            if category in {"Mn", "Me", "Cf", "Cc"}:
                continue

            # Allow whitespace
            if category.startswith("Z"):
                continue

            return False

        return has_symbol

    @staticmethod
    def _is_file_url(url: str) -> bool:
        """Check if URL points to a file (image, PDF, etc.) rather than a web page.

        Args:
            url: URL to check

        Returns:
            True if URL appears to be a file
        """
        file_extensions = [
            ".png",
            ".jpg",
            ".jpeg",
            ".gif",
            ".webp",
            ".svg",
            ".bmp",
            ".pdf",
            ".doc",
            ".docx",
            ".ppt",
            ".pptx",
            ".xls",
            ".xlsx",
            ".mp4",
            ".mp3",
            ".avi",
            ".mov",
            ".wav",
            ".zip",
            ".tar",
            ".gz",
            ".rar",
        ]

        url_path = url.split("?")[0].lower()
        return any(url_path.endswith(ext) for ext in file_extensions)

</file>
<file path="tldr_app.py">
import logging
from typing import Optional

import tldr_service
from summarizer import DEFAULT_MODEL, DEFAULT_TLDR_REASONING_EFFORT

logger = logging.getLogger("tldr_app")


def scrape_newsletters(
    start_date_text: str, end_date_text: str, source_ids: list[str] | None = None, excluded_urls: list[str] | None = None
) -> dict:
    """Scrape newsletters in date range.

    Args:
        start_date_text: Start date in ISO format
        end_date_text: End date in ISO format
        source_ids: Optional list of source IDs to scrape. Defaults to all configured sources.
        excluded_urls: List of canonical URLs to exclude from results

    Returns:
        Response dictionary with articles and issues
    """
    return tldr_service.scrape_newsletters_in_date_range(
        start_date_text, end_date_text, source_ids=source_ids, excluded_urls=excluded_urls
    )


def tldr_url(
    url: str,
    *,
    summary_effort: str = DEFAULT_TLDR_REASONING_EFFORT,
    model: str = DEFAULT_MODEL,
) -> dict:
    result = tldr_service.tldr_url_content(
        url,
        summary_effort=summary_effort,
        model=model,
    )

    payload: dict[str, Optional[str]] = {
        "success": True,
        "tldr_markdown": result["tldr_markdown"],
    }

    canonical_url = result.get("canonical_url")
    if canonical_url:
        payload["canonical_url"] = canonical_url

    summary_effort_value = result.get("summary_effort")
    if summary_effort_value:
        payload["summary_effort"] = summary_effort_value

    return payload



</file>
<file path="tldr_service.py">
import logging
from datetime import datetime

import requests

import util
from newsletter_scraper import scrape_date_range
from summarizer import (
    DEFAULT_MODEL,
    DEFAULT_TLDR_REASONING_EFFORT,
    _fetch_tldr_prompt,
    normalize_summary_effort,
    tldr_url,
)

logger = logging.getLogger("tldr_service")


def _parse_date_range(
    start_date_text: str, end_date_text: str
) -> tuple[datetime, datetime]:
    """Parse ISO date strings and enforce range limits.

    >>> _parse_date_range("2024-01-01", "2024-01-02")[0].isoformat()
    '2024-01-01T00:00:00'
    """
    if not start_date_text or not end_date_text:
        raise ValueError("start_date and end_date are required")

    try:
        start_date = datetime.fromisoformat(start_date_text)
        end_date = datetime.fromisoformat(end_date_text)
    except ValueError as error:
        raise ValueError("Dates must be ISO formatted (YYYY-MM-DD)") from error

    if start_date > end_date:
        raise ValueError("start_date must be before or equal to end_date")

    if (end_date - start_date).days >= 31:
        raise ValueError("Date range cannot exceed 31 days")

    return start_date, end_date


def scrape_newsletters_in_date_range(
    start_date_text: str, end_date_text: str, source_ids: list[str] | None = None, excluded_urls: list[str] | None = None
) -> dict:
    """Scrape newsletters in date range.

    Args:
        start_date_text: Start date in ISO format
        end_date_text: End date in ISO format
        source_ids: Optional list of source IDs to scrape. Defaults to all configured sources.
        excluded_urls: List of canonical URLs to exclude from results

    Returns:
        Response dictionary with articles and issues
    """
    start_date, end_date = _parse_date_range(start_date_text, end_date_text)

    sources_str = ", ".join(source_ids) if source_ids else "all"
    excluded_count = len(excluded_urls) if excluded_urls else 0
    logger.info(
        f"[tldr_service.scrape_newsletters] start start_date={start_date_text} end_date={end_date_text} sources={sources_str} excluded_count={excluded_count}",
    )

    result = scrape_date_range(start_date, end_date, source_ids=source_ids, excluded_urls=excluded_urls)

    logger.info(
        f"[tldr_service.scrape_newsletters] done dates_processed={result['stats']['dates_processed']} total_articles={result['stats']['total_articles']}",
    )
    return result


def fetch_tldr_prompt_template() -> str:
    return _fetch_tldr_prompt()


def tldr_url_content(
    url: str,
    *,
    summary_effort: str = DEFAULT_TLDR_REASONING_EFFORT,
    model: str = DEFAULT_MODEL,
) -> dict:
    cleaned_url = (url or "").strip()
    if not cleaned_url:
        raise ValueError("Missing url")

    canonical_url = util.canonicalize_url(cleaned_url)
    normalized_effort = normalize_summary_effort(summary_effort)

    try:
        tldr_markdown = tldr_url(
            canonical_url,
            summary_effort=normalized_effort,
            model=model,
        )
    except requests.RequestException as error:
        logger.error(
            "[tldr_service.tldr_url_content] request error error=%s",
            repr(error),
            exc_info=True,
        )
        raise

    return {
        "tldr_markdown": tldr_markdown,
        "canonical_url": canonical_url,
        "summary_effort": normalized_effort,
    }

</file>
<file path="util.py">
import logging
import os
from datetime import timedelta


def resolve_env_var(name: str, default: str = "") -> str:
    """
    Resolve environment variable, trying both direct name and TLDR_SCRAPER_ prefixed version.
    Strips surrounding quotes from the value if present.

    >>> os.environ['TEST_VAR'] = '"value"'
    >>> resolve_env_var('TEST_VAR')
    'value'
    >>> os.environ['TEST_VAR'] = 'value'
    >>> resolve_env_var('TEST_VAR')
    'value'
    """
    value = os.getenv(name) or os.getenv(f"TLDR_SCRAPER_{name}") or default
    return value.strip('"').strip("'") if value else value


def get_date_range(start_date, end_date):
    """Generate list of dates between start and end (inclusive)"""
    dates = []
    current = start_date
    while current <= end_date:
        dates.append(current)
        current += timedelta(days=1)
    return dates


def format_date_for_url(date):
    """Format date as YYYY-MM-DD for TLDR URL"""
    if isinstance(date, str):
        return date
    return date.strftime("%Y-%m-%d")


def canonicalize_url(url) -> str:
    """Canonicalize URL for better deduplication.

    Normalizes:
    - Removes scheme (http:// or https://)
    - Removes www. prefix
    - Removes query parameters
    - Removes URL fragments
    - Removes trailing slashes (including root)
    - Lowercases domain
    """
    import urllib.parse as urlparse

    # Handle protocol-less URLs by adding a temporary scheme for parsing
    if not url.startswith(('http://', 'https://', '//')):
        url = f'https://{url}'

    parsed = urlparse.urlparse(url)

    # Normalize netloc: lowercase and remove www. prefix
    netloc = parsed.netloc.lower()
    if netloc.startswith('www.'):
        netloc = netloc[4:]

    # Build canonical URL without scheme (strips query params and fragments)
    path = parsed.path
    canonical = f"{netloc}{path}"

    # Remove trailing slash (including root to normalize example.com/ → example.com)
    if canonical.endswith('/'):
        canonical = canonical[:-1]

    return canonical


def get_domain_name(url) -> str:
    """Extract a friendly domain name from a URL"""
    import urllib.parse as urlparse

    try:
        parsed = urlparse.urlparse(url)
        hostname = parsed.netloc.lower()

        # Remove www. prefix if present
        if hostname.startswith("www."):
            hostname = hostname[4:]

        # Remove port number if present
        hostname = hostname.split(":")[0]

        # Map common domains to friendly names
        domain_map = {
            "google.com": "Google",
            "youtube.com": "YouTube",
            "github.com": "GitHub",
            "stackoverflow.com": "Stack Overflow",
            "reddit.com": "Reddit",
            "twitter.com": "Twitter",
            "x.com": "X",
            "facebook.com": "Facebook",
            "linkedin.com": "LinkedIn",
            "medium.com": "Medium",
            "techcrunch.com": "TechCrunch",
            "theverge.com": "The Verge",
            "arstechnica.com": "Ars Technica",
            "wired.com": "Wired",
            "engadget.com": "Engadget",
            "reuters.com": "Reuters",
            "bloomberg.com": "Bloomberg",
            "nytimes.com": "New York Times",
            "washingtonpost.com": "Washington Post",
            "bbc.com": "BBC",
            "bbc.co.uk": "BBC",
            "cnn.com": "CNN",
            "theguardian.com": "The Guardian",
            "forbes.com": "Forbes",
            "wsj.com": "Wall Street Journal",
            "arxiv.org": "arXiv",
            "nature.com": "Nature",
            "science.org": "Science",
            "openai.com": "OpenAI",
            "anthropic.com": "Anthropic",
            "deepmind.com": "DeepMind",
            "microsoft.com": "Microsoft",
            "apple.com": "Apple",
            "amazon.com": "Amazon",
            "netflix.com": "Netflix",
            "spotify.com": "Spotify",
            "slack.com": "Slack",
            "discord.com": "Discord",
            "notion.so": "Notion",
            "figma.com": "Figma",
            "vercel.com": "Vercel",
            "netlify.com": "Netlify",
        }

        if hostname in domain_map:
            return domain_map[hostname]

        # For unmapped domains, capitalize the main part
        # e.g., "example.com" -> "Example"
        parts = hostname.split(".")
        if len(parts) >= 2:
            # Use the second-to-last part (main domain name)
            main_part = parts[-2]
            return main_part.capitalize()
        elif len(parts) == 1:
            return parts[0].capitalize()

        return hostname.capitalize()

    except Exception:
        return "Unknown"

</file>
<file path="will_larson_adapter.py">
"""
Will Larson's "Irrational Exuberance" blog adapter implementation using RSS feed.

This adapter implements the NewsletterAdapter interface for Will Larson's blog,
using the RSS feed at lethain.com for efficient article fetching.
"""

import logging
import xml.etree.ElementTree as ET
from datetime import datetime
from email.utils import parsedate_to_datetime

import requests

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("will_larson_adapter")


class WillLarsonAdapter(NewsletterAdapter):
    """Adapter for Will Larson's blog using RSS feed."""

    def __init__(self, config):
        """Initialize with config."""
        super().__init__(config)
        self.rss_url = "https://lethain.com/feeds.xml"

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch articles from Will Larson's blog RSS feed for a specific date.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
        articles = []
        excluded_set = set(excluded_urls)

        # Parse target date
        target_date = datetime.fromisoformat(util.format_date_for_url(date))
        target_date_str = target_date.strftime("%Y-%m-%d")

        logger.info(f"[will_larson_adapter.scrape_date] Fetching articles for {target_date_str} from RSS feed")

        try:
            # Fetch RSS feed
            response = requests.get(self.rss_url, timeout=10)
            response.raise_for_status()

            # Parse XML
            root = ET.fromstring(response.content)
            items = root.findall('.//item')

            logger.info(f"[will_larson_adapter.scrape_date] Found {len(items)} items in RSS feed")

            # Filter items by date
            for item in items:
                title_elem = item.find('title')
                link_elem = item.find('link')
                pubdate_elem = item.find('pubDate')
                description_elem = item.find('description')

                if title_elem is None or link_elem is None or pubdate_elem is None:
                    continue

                # Parse publication date
                try:
                    pub_datetime = parsedate_to_datetime(pubdate_elem.text)
                    article_date_str = pub_datetime.strftime("%Y-%m-%d")
                except Exception as e:
                    logger.warning(f"[will_larson_adapter.scrape_date] Error parsing date '{pubdate_elem.text}': {e}")
                    continue

                # Check if article matches target date
                if article_date_str != target_date_str:
                    continue

                # Get article details
                title = title_elem.text
                url = link_elem.text
                canonical_url = util.canonicalize_url(url)

                # Skip if excluded
                if canonical_url in excluded_set:
                    continue

                # Extract excerpt from description (first 200 chars)
                description = description_elem.text if description_elem is not None else ""
                if description:
                    # Strip HTML tags and decode HTML entities
                    import re
                    import html
                    description_text = re.sub(r'<[^>]+>', '', description)
                    description_text = html.unescape(description_text)
                    excerpt = description_text[:200].strip()
                    if len(description_text) > 200:
                        excerpt += "..."
                else:
                    excerpt = ""

                article = {
                    "title": title,
                    "article_meta": excerpt,
                    "url": canonical_url,
                    "category": "Engineering Leadership",
                    "date": target_date_str,
                    "newsletter_type": "blog",
                    "removed": False,
                }

                articles.append(article)

            logger.info(f"[will_larson_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[will_larson_adapter.scrape_date] Error fetching RSS feed: {e}", exc_info=True)

        # Create issue metadata if we have articles
        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': 'Engineering Leadership',
                'title': 'Irrational Exuberance',
                'subtitle': 'Will Larson'
            })

        return self._normalize_response(articles, issues)

</file>
<file path="xeiaso_adapter.py">
"""
Xe Iaso blog adapter implementation using RSS feed.

Xe Iaso (xeiaso.net) is a technical educator with 400+ blog articles covering
backend systems, NixOS, systems programming, and making tech accessible.
Posts 1-2 articles per week.
"""

import logging
from datetime import datetime
import feedparser

from newsletter_adapter import NewsletterAdapter
import util


logger = logging.getLogger("xeiaso_adapter")

RSS_FEED_URL = "https://xeiaso.net/blog.rss"


class XeIasoAdapter(NewsletterAdapter):
    """Adapter for Xe Iaso's blog using RSS feed."""

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch Xe Iaso blog posts for a specific date using RSS feed.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary with articles and issues
        """
        articles = []
        excluded_set = set(excluded_urls)

        target_date_str = util.format_date_for_url(date)
        target_date = datetime.fromisoformat(target_date_str).date()

        logger.info(f"[xeiaso_adapter.scrape_date] Fetching RSS feed for {target_date_str}")

        try:
            feed = feedparser.parse(RSS_FEED_URL)

            if not feed.entries:
                logger.warning(f"[xeiaso_adapter.scrape_date] No entries found in RSS feed")
                return self._normalize_response([], [])

            logger.info(f"[xeiaso_adapter.scrape_date] Fetched {len(feed.entries)} entries from RSS")

            for entry in feed.entries:
                article = self._parse_rss_entry(entry, target_date, excluded_set)
                if article:
                    articles.append(article)

            logger.info(f"[xeiaso_adapter.scrape_date] Found {len(articles)} articles for {target_date_str}")

        except Exception as e:
            logger.error(f"[xeiaso_adapter.scrape_date] Error fetching RSS feed: {e}", exc_info=True)

        issues = []
        if articles:
            issues.append({
                'date': target_date_str,
                'source_id': self.config.source_id,
                'category': 'Xe Iaso',
                'title': None,
                'subtitle': None
            })

        return self._normalize_response(articles, issues)

    def _parse_rss_entry(self, entry: dict, target_date: datetime.date, excluded_set: set) -> dict | None:
        """Parse RSS entry into article dict if it matches the target date.

        Args:
            entry: feedparser entry dict
            target_date: Target date to filter by
            excluded_set: Set of canonical URLs to exclude

        Returns:
            Article dictionary or None if entry should be skipped
        """
        if not entry.get('link'):
            return None

        canonical_url = util.canonicalize_url(entry['link'])
        if canonical_url in excluded_set:
            return None

        published_parsed = entry.get('published_parsed')
        if not published_parsed:
            return None

        entry_date = datetime(*published_parsed[:3]).date()

        if entry_date != target_date:
            return None

        title = entry.get('title', 'Untitled')
        summary = entry.get('summary', '')

        tags = entry.get('tags', [])
        tag_terms = [tag.get('term', '') for tag in tags]

        is_external = 'external' in tag_terms

        if is_external:
            article_meta = "External post"
        else:
            article_meta = summary[:100] if summary else ""

        return {
            "title": title,
            "article_meta": article_meta,
            "url": entry['link'],
            "category": "Xe Iaso",
            "date": util.format_date_for_url(entry_date),
            "newsletter_type": "blog",
            "removed": False,
        }

</file>
</files>

================================================================================
                              CLIENT CONTEXT
================================================================================

Total: 20 client files
<files>
<file path="client/index.html">
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="color-scheme" content="light" />
    <title>Newsletter Aggregator</title>

    <!-- Vollkorn font for reading surface -->
    <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Vollkorn:wght@400;700&display=swap" rel="stylesheet">
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>

</file>
<file path="client/src/App.css">
/* CSS Variables */
:root {
  --bg: #f6f7f9;
  --surface: #ffffff;
  --text: #0f172a;
  --muted: #475569;
  --border: #e5e7eb;
  --link: #1a73e8;
  --radius: 10px;
  --shadow-sm: 0 1px 2px rgba(0,0,0,0.06), 0 1px 3px rgba(0,0,0,0.10);

  /* Whitey reading surface colors */
  --whitey-text: #333;
  --whitey-link: #2484c1;
  --whitey-divider: #2f2f2f;
  --whitey-rule: #ddd;
  --whitey-s-1: 0.5rem;
  --whitey-s-2: 1rem;
  --whitey-s-3: 1.5rem;
  --whitey-s-4: 2.5rem;
}

/* Global resets and base styles */
* {
  box-sizing: border-box;
}

body {
  font-family: system-ui, -apple-system, Segoe UI, Roboto, Noto Sans, Ubuntu, Cantarell, "Helvetica Neue", Arial, sans-serif;
  max-width: 1000px;
  margin: 0 auto;
  padding: 20px;
  background-color: var(--bg);
  color: var(--text);
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  text-rendering: optimizeLegibility;
  -webkit-text-size-adjust: 100%;
  letter-spacing: 0.003em;
  line-height: 1.6;
}

/* Container styles */
.container {
  background: var(--surface);
  padding: 20px;
  border-radius: var(--radius);
  box-shadow: var(--shadow-sm);
  border: 1px solid var(--border);
}

h1 {
  color: var(--text);
  text-align: center;
  margin-bottom: 30px;
  font-weight: 700;
  letter-spacing: -0.01em;
  line-height: 1.25;
  text-wrap: balance;
}

.context-buttons {
  display: flex;
  gap: 8px;
  margin-bottom: 20px;
  justify-content: stretch;
}

.context-btn {
  flex: 1;
  padding: 6px 12px;
  font-size: 13px;
  color: var(--muted);
  background: transparent;
  border: 1px solid var(--border);
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.15s;
}

.context-btn:hover:not(:disabled) {
  background: var(--bg);
  border-color: var(--muted);
}

.context-btn:disabled {
  cursor: not-allowed;
  opacity: 0.6;
}

.copy-error {
  padding: 8px 12px;
  margin-bottom: 16px;
  background: #fff3cd;
  border: 1px solid #ffc107;
  border-radius: 6px;
  font-size: 13px;
  color: #856404;
}

/* Focus styles */
a:focus-visible,
button:focus-visible,
input:focus-visible {
  outline: 3px solid rgba(26,115,232,0.45);
  outline-offset: 2px;
}

/* Whitey reading surface styles (for #write) */
#write {
  font-size: 19px;
  max-width: 960px;
  margin: 0 auto 2em;
  padding-top: 40px;
  padding-left: 1rem;
  padding-right: 1rem;
  color: var(--whitey-text);
  font-family: "Vollkorn", Palatino, Times, serif;
  line-height: 1.53;
  text-align: left;
  background: transparent;
}

@media only screen and (min-width: 1400px) {
  #write {
    max-width: 1100px;
  }
}

#write h1,
#write h2,
#write h3 {
  text-align: center;
  font-weight: normal;
  color: var(--whitey-text);
}

#write h1 {
  margin-top: 1.6em;
  font-size: 3em;
}

#write h2 {
  margin-top: 2em;
}

#write h3 {
  margin-top: 3em;
  font-style: italic;
}

#write h1 + h2,
#write h2 + h3 {
  margin-top: 0.83em;
}

/* Mobile responsive */
@media (max-width: 42em) {
  #write {
    font-size: clamp(16px, 0.95rem + 0.6vw, 18px);
    line-height: 1.6;
    padding-left: 0;
    padding-right: 0;
  }

  #write h1 {
    font-size: clamp(2rem, 7vw, 2.6rem);
    line-height: 1.15;
    margin: var(--whitey-s-4) 0 var(--whitey-s-2);
  }

  #write h2 {
    font-size: clamp(1.5rem, 5.5vw, 2rem);
    line-height: 1.25;
    margin: var(--whitey-s-4) 0 var(--whitey-s-1);
  }

  #write h3 {
    font-size: clamp(1.25rem, 4.5vw, 1.6rem);
    line-height: 1.3;
    margin: var(--whitey-s-3) 0 var(--whitey-s-1);
  }
}

@media (max-width: 480px) {
  body {
    padding: 8px;
  }
  .container {
    padding: 12px;
  }
}

</file>
<file path="client/src/App.jsx">
import { useState, useEffect } from 'react'
import CacheToggle from './components/CacheToggle'
import ScrapeForm from './components/ScrapeForm'
import ResultsDisplay from './components/ResultsDisplay'
import { loadFromCache } from './lib/scraper'
import './App.css'

function App() {
  const [results, setResults] = useState(null)
  const [copying, setCopying] = useState(null)
  const [downloadError, setDownloadError] = useState(null)

  useEffect(() => {
    const today = new Date()
    const threeDaysAgo = new Date(today)
    threeDaysAgo.setDate(today.getDate() - 3)

    const endDate = today.toISOString().split('T')[0]
    const startDate = threeDaysAgo.toISOString().split('T')[0]

    loadFromCache(startDate, endDate)
      .then(cached => {
        if (cached) {
          setResults(cached)
        }
      })
      .catch(err => {
        console.error('Failed to load cached results:', err)
      })
  }, [])

  const handleContextCopy = async (contextType) => {
    console.log(`[handleContextCopy] Starting download for: ${contextType}`)
    setCopying(contextType)
    setDownloadError(null)

    try {
      const response = await fetch('/api/generate-context', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ context_type: contextType })
      })

      console.log(`[handleContextCopy] Response status: ${response.status}`)
      const result = await response.json()
      console.log(`[handleContextCopy] Result success: ${result.success}, content length: ${result.content?.length || 0}`)

      if (result.success) {
        const blob = new Blob([result.content], { type: 'text/plain' })
        const url = URL.createObjectURL(blob)
        const link = document.createElement('a')
        link.href = url
        link.download = `context-${contextType}.txt`
        document.body.appendChild(link)
        link.click()
        document.body.removeChild(link)
        URL.revokeObjectURL(url)

        console.log(`[handleContextCopy] Download triggered`)
        setTimeout(() => setCopying(null), 1000)
      } else {
        console.error('Failed to generate context:', result.error)
        setDownloadError(`Server error: ${result.error}`)
        setCopying(null)
      }
    } catch (err) {
      console.error('Failed to download context:', err)
      setDownloadError(`Network error: ${err.message}`)
      setCopying(null)
    }
  }

  return (
    <div className="container">
      <h1>Newsletter Aggregator</h1>

      <div className="context-buttons">
        <button
          onClick={() => handleContextCopy('server')}
          disabled={copying === 'server'}
          className="context-btn"
        >
          ⬇ {copying === 'server' ? 'Downloaded!' : 'server'}
        </button>
        <button
          onClick={() => handleContextCopy('client')}
          disabled={copying === 'client'}
          className="context-btn"
        >
          ⬇ {copying === 'client' ? 'Downloaded!' : 'client'}
        </button>
        <button
          onClick={() => handleContextCopy('docs')}
          disabled={copying === 'docs'}
          className="context-btn"
        >
          ⬇ {copying === 'docs' ? 'Downloaded!' : 'docs'}
        </button>
        <button
          onClick={() => handleContextCopy('all')}
          disabled={copying === 'all'}
          className="context-btn"
        >
          ⬇ {copying === 'all' ? 'Downloaded!' : 'all'}
        </button>
      </div>

      {downloadError && (
        <div className="copy-error">
          {downloadError}
        </div>
      )}

      <CacheToggle />

      <ScrapeForm onResults={setResults} />

      {results && <ResultsDisplay results={results} />}
    </div>
  )
}

export default App

</file>
<file path="client/src/components/ArticleCard.css">
.article-card {
  display: flex;
  flex-direction: column;
  background: var(--surface, #ffffff);
  border: 1px solid var(--border, #e5e7eb);
  border-radius: 8px;
  padding: 14px 16px;
  transition: border-color 0.15s ease, box-shadow 0.15s ease, transform 0.15s ease, opacity 0.15s ease;
  position: relative;
  animation: cardFadeIn 0.3s ease backwards;
}

@keyframes cardFadeIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.article-card:hover {
  border-color: var(--link, #1a73e8);
  box-shadow: 0 2px 8px rgba(26, 115, 232, 0.12);
  transform: translateY(-1px);
}

.article-card.unread .article-link {
  font-weight: 600;
}

.article-card.read .article-link {
  font-weight: normal;
  color: var(--muted, #475569);
}

.article-card.tldr-hidden {
  opacity: 0.6;
  background: rgba(156, 163, 175, 0.04);
}

.article-card.tldr-hidden .article-link {
  font-weight: normal;
  color: var(--muted, #475569);
}

.article-card.removed {
  opacity: 0.75;
  border-style: dashed;
  border-color: #d1d5db;
  background: #f3f4f6;
  cursor: pointer;
}

.article-card.removed .article-link {
  text-decoration: line-through;
  color: var(--muted, #475569);
  pointer-events: none;
}

.article-card.removed .article-actions {
  opacity: 1;
}

.article-card.removed .article-actions .article-btn:not(.remove-article-btn) {
  display: none;
}

.article-header {
  display: flex;
  align-items: center;
  gap: 12px;
  width: 100%;
}

.article-number {
  flex-shrink: 0;
  width: 28px;
  height: 28px;
  display: flex;
  align-items: center;
  justify-content: center;
  background: var(--bg, #f6f7f9);
  border-radius: 6px;
  font-weight: 600;
  font-size: 14px;
  color: var(--muted, #475569);
}

.article-content {
  flex: 1;
  min-width: 0;
}

.article-link {
  font-size: 16px;
  line-height: 1.5;
  color: var(--link, #1a73e8);
  text-decoration: none;
  word-wrap: break-word;
  display: block;
  width: 100%;
  cursor: pointer;
}

.article-link:hover {
  text-decoration: underline;
}

.article-link.loading {
  opacity: 0.6;
  cursor: wait;
}

.article-favicon {
  width: 1em;
  height: 1em;
  display: inline-block;
  vertical-align: -0.125em;
  object-fit: contain;
  margin-right: 1ch;
}

.article-actions {
  flex-shrink: 0;
  display: flex;
  align-items: center;
  gap: 6px;
  opacity: 0;
  transition: opacity 0.2s ease;
}

.article-card:hover .article-actions {
  opacity: 1;
}

.article-btn {
  height: 32px;
  display: flex;
  align-items: center;
  justify-content: center;
  background: transparent;
  border: 1px solid var(--border, #e5e7eb);
  border-radius: 6px;
  color: var(--muted, #475569);
  cursor: pointer;
  transition: all 0.15s ease;
  font-size: 13px;
  padding: 0;
  margin: 0;
}

.article-btn:hover {
  background: var(--bg, #f6f7f9);
  border-color: var(--muted, #475569);
  color: var(--text, #0f172a);
}

.article-btn:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.tldr-btn {
  width: auto;
  min-width: 5em;
  padding: 0 10px;
  font-weight: 500;
  white-space: nowrap;
}

.tldr-btn.loaded {
  background: #22c55e;
  color: white;
  border-color: #22c55e;
}

.tldr-btn.loaded:hover {
  background: #16a34a;
  border-color: #16a34a;
}

.tldr-btn.loaded.expanded {
  background: var(--bg, #f6f7f9);
  color: var(--text, #0f172a);
  border-color: var(--border, #e5e7eb);
}

.remove-article-btn {
  width: auto;
  min-width: 6.2em;
  padding: 0 10px;
  font-weight: 500;
  white-space: nowrap;
}

.remove-article-btn:hover {
  background: rgba(220, 53, 69, 0.12);
  border-color: rgba(220, 53, 69, 0.4);
  color: #b91c1c;
}

.article-card.removed .remove-article-btn {
  display: none;
}

.inline-tldr {
  margin-top: 9px;
  padding-top: 9px;
  padding-bottom: 0;
  border-top: 1px dashed var(--border, #e5e7eb);
  font-size: 0.96em;
  line-height: 1.675;
  animation: slideDown 0.2s ease;
}

@keyframes slideDown {
  from {
    opacity: 0;
    transform: translateY(-8px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.inline-tldr > strong {
  display: block;
  margin-bottom: 0.5em;
  color: var(--text, #0f172a);
}

.inline-tldr ul {
  padding-left: 1.2em;  /* Just enough room for bullets */
  margin: 0.5em 0;
}

@media (max-width: 768px) {
  .article-actions {
    opacity: 1;
    width: 100%;
    margin-left: 0;
    margin-top: 8px;
    justify-content: flex-start;
  }

  .article-header {
    flex-wrap: wrap;
  }

}

</file>
<file path="client/src/components/ArticleCard.jsx">
import { useMemo } from 'react'
import { useArticleState } from '../hooks/useArticleState'
import { useSummary } from '../hooks/useSummary'
import './ArticleCard.css'

function ArticleCard({ article, index }) {
  const { isRead, isRemoved, isTldrHidden, toggleRead, toggleRemove, markTldrHidden, unmarkTldrHidden, loading: stateLoading } = useArticleState(
    article.issueDate,
    article.url
  )

  const tldr = useSummary(article.issueDate, article.url, 'tldr')

  const cardClasses = [
    'article-card',
    !isRead && 'unread',
    isRead && 'read',
    isRemoved && 'removed',
    isTldrHidden && 'tldr-hidden'
  ].filter(Boolean).join(' ')

  const fullUrl = useMemo(() => {
    const url = article.url
    if (url.startsWith('http://') || url.startsWith('https://')) {
      return url
    }
    return `https://${url}`
  }, [article.url])

  const faviconUrl = useMemo(() => {
    try {
      const url = new URL(fullUrl)
      return `${url.origin}/favicon.ico`
    } catch {
      return null
    }
  }, [fullUrl])

  const handleLinkClick = (e) => {
    if (isRemoved) return
    if (e.ctrlKey || e.metaKey) return

    if (!isRead) {
      toggleRead()
    }
  }

  const handleTldrClick = () => {
    if (isRemoved) return

    const wasExpanded = tldr.expanded
    tldr.toggle()

    if (!isRead && tldr.expanded) {
      toggleRead()
    }

    if (wasExpanded && !tldr.expanded) {
      markTldrHidden()
    } else if (tldr.expanded) {
      unmarkTldrHidden()
    }
  }

  const handleRemoveClick = () => {
    if (!isRemoved && tldr.expanded) {
      tldr.collapse()
    }
    toggleRemove()
  }

  return (
    <div className={cardClasses} data-original-order={index} onClick={isRemoved ? handleRemoveClick : undefined}>
      <div className="article-header">
        <div className="article-number">{index + 1}</div>

        <div className="article-content">
          <a
            href={fullUrl}
            className={`article-link ${stateLoading ? 'loading' : ''}`}
            target="_blank"
            rel="noopener noreferrer"
            data-url={fullUrl}
            tabIndex={isRemoved ? -1 : 0}
            onClick={handleLinkClick}
          >
            {faviconUrl && (
              <img
                src={faviconUrl}
                className="article-favicon"
                loading="lazy"
                alt=""
                onError={(e) => e.target.style.display = 'none'}
              />
            )}
            <span className="article-link-text">
              {article.title}{!isRemoved && article.articleMeta ? ` (${article.articleMeta})` : ''}
            </span>
          </a>
        </div>

        <div className="article-actions">
          <button
            className={`article-btn tldr-btn ${tldr.isAvailable ? 'loaded' : ''} ${tldr.expanded ? 'expanded' : ''}`}
            disabled={stateLoading || tldr.loading}
            type="button"
            title={tldr.isAvailable ? 'TLDR cached - click to show' : 'Show TLDR'}
            onClick={handleTldrClick}
          >
            {tldr.buttonLabel}
          </button>

          <button
            className="article-btn remove-article-btn"
            type="button"
            title={isRemoved ? 'Restore this article to the list' : 'Remove this article from the list'}
            disabled={stateLoading}
            onClick={handleRemoveClick}
          >
            {isRemoved ? 'Restore' : 'Remove'}
          </button>
        </div>
      </div>

      {tldr.expanded && tldr.html && (
        <div className="inline-tldr">
          <strong>TLDR</strong>
          <div dangerouslySetInnerHTML={{ __html: tldr.html }} />
        </div>
      )}
    </div>
  )
}

export default ArticleCard

</file>
<file path="client/src/components/ArticleList.css">
.article-list {
  display: flex;
  flex-direction: column;
  gap: 12px;
  margin: 20px 0;
  padding: 0;
  list-style: none;
}

.section-title {
  margin: 18px 0 6px;
  text-align: center;
  font-style: italic;
  color: #9ca3af;
  font-size: 0.92em;
  letter-spacing: 0.01em;
}

.article-card.category-first {
  border-top: 3px solid var(--link, #1a73e8);
  padding-top: 16px;
  margin-top: 12px;
}

</file>
<file path="client/src/components/ArticleList.jsx">
import { useMemo } from 'react'
import ArticleCard from './ArticleCard'
import './ArticleList.css'

function ArticleList({ articles }) {
  const sortedArticles = useMemo(() => {
    return [...articles].sort((a, b) => {
      const stateA = a.removed ? 3
        : a.tldrHidden ? 2
        : a.read?.isRead ? 1
        : 0
      const stateB = b.removed ? 3
        : b.tldrHidden ? 2
        : b.read?.isRead ? 1
        : 0

      if (stateA !== stateB) return stateA - stateB

      return (a.originalOrder ?? 0) - (b.originalOrder ?? 0)
    })
  }, [articles])

  const sectionsWithArticles = useMemo(() => {
    const sections = []
    let currentSection = null

    sortedArticles.forEach((article, index) => {
      const sectionTitle = article.section
      const sectionEmoji = article.sectionEmoji
      const sectionKey = sectionTitle ? `${sectionEmoji || ''} ${sectionTitle}`.trim() : null

      if (sectionKey && sectionKey !== currentSection) {
        sections.push({
          type: 'section',
          key: sectionKey,
          label: sectionKey
        })
        currentSection = sectionKey
      } else if (!sectionTitle && currentSection !== null) {
        currentSection = null
      }

      sections.push({
        type: 'article',
        key: article.url,
        article,
        index
      })
    })

    return sections
  }, [sortedArticles])

  return (
    <div className="article-list">
      {sectionsWithArticles.map((item) => (
        item.type === 'section' ? (
          <div key={item.key} className="section-title">
            {item.label}
          </div>
        ) : (
          <ArticleCard
            key={item.key}
            article={item.article}
            index={item.index}
          />
        )
      ))}
    </div>
  )
}

export default ArticleList

</file>
<file path="client/src/components/CacheToggle.css">
.cache-toggle-container {
  display: flex;
  align-items: center;
  justify-content: center;
  padding: 12px;
  margin-bottom: 20px;
  background: var(--bg);
  border-radius: 8px;
  border: 1px solid var(--border);
}

.cache-toggle-label {
  display: flex;
  align-items: center;
  gap: 10px;
  cursor: pointer;
  font-size: 15px;
  color: var(--text);
  user-select: none;
}

.cache-toggle-checkbox {
  position: relative;
  width: 48px;
  height: 26px;
  background: #cbd5e1;
  border-radius: 13px;
  transition: background-color 0.2s ease;
  cursor: pointer;
  border: 2px solid transparent;
}

.cache-toggle-checkbox::after {
  content: '';
  position: absolute;
  top: 2px;
  left: 2px;
  width: 18px;
  height: 18px;
  background: white;
  border-radius: 50%;
  transition: transform 0.2s ease;
  box-shadow: 0 2px 4px rgba(0,0,0,0.2);
}

.cache-toggle-input {
  position: absolute;
  opacity: 0;
  width: 0;
  height: 0;
}

.cache-toggle-input:checked + .cache-toggle-checkbox {
  background: #22c55e;
}

.cache-toggle-input:checked + .cache-toggle-checkbox::after {
  transform: translateX(22px);
}

.cache-toggle-input:focus-visible + .cache-toggle-checkbox {
  outline: 3px solid rgba(26,115,232,0.45);
  outline-offset: 2px;
}

.cache-toggle-text {
  font-weight: 500;
}

.cache-toggle-status {
  font-size: 13px;
  color: var(--muted);
  margin-left: 4px;
}

</file>
<file path="client/src/components/CacheToggle.jsx">
import { useSupabaseStorage } from '../hooks/useSupabaseStorage'
import './CacheToggle.css'

function CacheToggle() {
  const [enabled, setEnabled, , { loading }] = useSupabaseStorage('cache:enabled', true)

  return (
    <div className="cache-toggle-container" data-testid="cache-toggle-container">
      <label className="cache-toggle-label" htmlFor="cacheToggle">
        <input
          id="cacheToggle"
          type="checkbox"
          className="cache-toggle-input"
          data-testid="cache-toggle-input"
          aria-label="Enable cache"
          checked={enabled}
          disabled={loading}
          onChange={(e) => setEnabled(e.target.checked)}
        />
        <span className="cache-toggle-checkbox" data-testid="cache-toggle-switch" />
        <span className="cache-toggle-text">Cache</span>
        <span className="cache-toggle-status" data-testid="cache-toggle-status">
          {enabled ? '(enabled)' : '(disabled)'}
        </span>
      </label>
    </div>
  )
}

export default CacheToggle

</file>
<file path="client/src/components/ResultsDisplay.css">
.result {
  margin-top: 20px;
  padding: 0;
  background-color: transparent;
  border-radius: 0;
  display: block;
  border: none;
}

.stats {
  background-color: #fff3cd;
  border: 1px solid #ffeaa7;
  color: #856404;
  margin-bottom: 15px;
  padding: 10px;
  border-radius: 4px;
}

.date-header-container {
  display: flex;
  align-items: center;
  justify-content: space-between;
  width: 100%;
  margin: 0;
}

.date-header-container h2 {
  text-align: center;
  width: 100%;
  font-weight: normal;
  color: var(--text, #0f172a);
  margin-top: 2em;
}

.date-header-container h2::after {
  border-bottom: 1px solid #2f2f2f;
  content: '';
  width: 100px;
  display: block;
  margin: 0.4em auto 0;
  height: 1px;
}

.issue-header-container h4 {
  text-align: left;
  margin: 1.5em 0 0.5em;
  font-style: normal;
  color: var(--text, #0f172a);
}

.issue-title-block {
  margin: 12px 0 18px;
  text-align: center;
  font-style: italic;
  color: #9ca3af;
  font-size: 0.95em;
  line-height: 1.6;
}

.issue-title-block .issue-title-line + .issue-title-line {
  margin-top: 4px;
}

.copy-toast {
  position: fixed;
  left: 50%;
  bottom: 32px;
  transform: translateX(-50%) translateY(20px);
  background: rgba(255, 255, 255, 0.95);
  backdrop-filter: blur(10px);
  color: var(--text, #0f172a);
  padding: 10px 18px;
  border-radius: 999px;
  font-size: 14px;
  font-weight: 500;
  opacity: 0;
  pointer-events: none;
  transition: opacity 0.25s ease, transform 0.25s ease;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08), 0 2px 4px rgba(0, 0, 0, 0.06);
  border: 1px solid rgba(0, 0, 0, 0.06);
  z-index: 999;
}

.copy-toast.show {
  opacity: 1;
  transform: translateX(-50%) translateY(0);
}

</file>
<file path="client/src/components/ResultsDisplay.jsx">
import { useSupabaseStorage } from '../hooks/useSupabaseStorage'
import { getNewsletterScrapeKey } from '../lib/storageKeys'
import ArticleList from './ArticleList'
import './ResultsDisplay.css'

function ResultsDisplay({ results }) {
  const statsLines = [
    `📊 Stats: ${results.stats.total_articles} articles, ${results.stats.unique_urls} unique URLs`,
    `📅 Dates: ${results.stats.dates_with_content}/${results.stats.dates_processed} with content`,
    results.source && `Source: ${results.source}`
  ].filter(Boolean)

  return (
    <div id="result" className="result success">
      <div className="stats">
        {statsLines.map((line, index) => (
          <div key={index}>{line}</div>
        ))}
      </div>

      <main id="write">
        {(results.payloads || []).map((payload) => (
          <DailyResults
            key={payload.date}
            payload={payload}
          />
        ))}
      </main>
    </div>
  )
}

function DailyResults({ payload }) {
  const [livePayload, , , { loading }] = useSupabaseStorage(
    getNewsletterScrapeKey(payload.date),
    payload
  )

  const date = livePayload?.date ?? payload.date
  const articles = (livePayload?.articles ?? payload.articles).map((article, index) => ({
    ...article,
    originalOrder: index
  }))
  const issues = livePayload?.issues ?? payload.issues ?? []

  return (
    <div className="date-group">
      <div className="date-header-container" data-date={date}>
        <h2>{date}</h2>
        {loading && <span className="loading-indicator"> (loading...)</span>}
      </div>

      {issues.map((issue) => (
        <div
          key={`${date}-${issue.category}`}
          className="issue-section"
        >
          <div className="issue-header-container">
            <h4>{issue.category}</h4>
          </div>

          {(issue.title || issue.subtitle) && (
            <div className="issue-title-block">
              {issue.title && (
                <div className="issue-title-line">{issue.title}</div>
              )}
              {issue.subtitle && issue.subtitle !== issue.title && (
                <div className="issue-title-line">{issue.subtitle}</div>
              )}
            </div>
          )}

          <ArticleList
            articles={articles.filter((article) => article.category === issue.category)}
          />
        </div>
      ))}

      {articles.some((article) => !article.category) && (
        <ArticleList
          articles={articles.filter((article) => !article.category)}
        />
      )}
    </div>
  )
}

export default ResultsDisplay

</file>
<file path="client/src/components/ScrapeForm.css">
.form-group {
  margin-bottom: 20px;
  display: inline-block;
  width: 45%;
  margin-right: 5%;
}

.form-group:last-child {
  margin-right: 0;
}

label {
  display: block;
  margin-bottom: 5px;
  font-weight: bold;
  color: var(--muted);
}

input[type="date"] {
  width: 100%;
  padding: 10px 12px;
  border: 1px solid var(--border);
  border-radius: 8px;
  font-size: 16px;
  box-sizing: border-box;
  background: var(--surface);
  color: var(--text);
  min-height: 44px;
}

button {
  background-color: #007bff;
  color: white;
  padding: 12px 24px;
  border: none;
  border-radius: 8px;
  cursor: pointer;
  font-size: 16px;
  width: 100%;
  margin-top: 20px;
  min-height: 44px;
}

button:hover {
  background-color: #0056b3;
}

button:disabled {
  background-color: #ccc;
  cursor: not-allowed;
}

.progress {
  display: block;
  margin: 20px 0;
  padding: 15px;
  background-color: #e3f2fd;
  border-radius: 4px;
  border-left: 4px solid #2196f3;
}

.progress-bar {
  width: 100%;
  height: 20px;
  background-color: #ddd;
  border-radius: 10px;
  overflow: hidden;
  margin-top: 10px;
}

.progress-fill {
  height: 100%;
  background-color: #2196f3;
  transition: width 0.3s ease;
}

.error {
  background-color: #f8d7da;
  border: 1px solid #f5c6cb;
  color: #721c24;
  padding: 10px 12px;
  border-radius: 6px;
  margin-top: 10px;
}

@media (max-width: 480px) {
  .form-group {
    width: 100%;
    margin-right: 0;
  }
}

</file>
<file path="client/src/components/ScrapeForm.jsx">
import { useActionState, useState, useEffect } from 'react'
import { scrapeNewsletters } from '../lib/scraper'
import { useSupabaseStorage } from '../hooks/useSupabaseStorage'
import './ScrapeForm.css'

function ScrapeForm({ onResults }) {
  const [startDate, setStartDate] = useState('')
  const [endDate, setEndDate] = useState('')
  const [cacheEnabled] = useSupabaseStorage('cache:enabled', true)
  const [progress, setProgress] = useState(0)

  useEffect(() => {
    const today = new Date()
    const threeDaysAgo = new Date(today)
    threeDaysAgo.setDate(today.getDate() - 3)
    setEndDate(today.toISOString().split('T')[0])
    setStartDate(threeDaysAgo.toISOString().split('T')[0])
  }, [])

  const [state, formAction, isPending] = useActionState(
    async (previousState, formData) => {
      const start = formData.get('start_date')
      const end = formData.get('end_date')

      const startDateObj = new Date(start)
      const endDateObj = new Date(end)
      const daysDiff = Math.ceil((endDateObj - startDateObj) / (1000 * 60 * 60 * 24))

      if (startDateObj > endDateObj) {
        return { error: 'Start date must be before or equal to end date.' }
      }
      if (daysDiff >= 31) {
        return { error: 'Date range cannot exceed 31 days. Please select a smaller range.' }
      }

      setProgress(50)

      try {
        const results = await scrapeNewsletters(start, end, cacheEnabled)
        setProgress(100)
        onResults(results)
        return { success: true }
      } catch (err) {
        setProgress(0)
        return { error: err.message || 'Network error' }
      }
    },
    { success: false }
  )

  const validationError = (() => {
    if (!startDate || !endDate) return null

    const start = new Date(startDate)
    const end = new Date(endDate)
    const daysDiff = Math.ceil((end - start) / (1000 * 60 * 60 * 24))

    if (start > end) {
      return 'Start date must be before or equal to end date.'
    }
    if (daysDiff >= 31) {
      return 'Date range cannot exceed 31 days. Please select a smaller range.'
    }
    return null
  })()

  return (
    <div>
      <form id="scrapeForm" action={formAction}>
        <div className="form-group">
          <label htmlFor="start_date">Start Date:</label>
          <input
            id="start_date"
            name="start_date"
            type="date"
            value={startDate}
            onChange={(e) => setStartDate(e.target.value)}
            required
          />
        </div>

        <div className="form-group">
          <label htmlFor="end_date">End Date:</label>
          <input
            id="end_date"
            name="end_date"
            type="date"
            value={endDate}
            onChange={(e) => setEndDate(e.target.value)}
            required
          />
        </div>

        <button
          id="scrapeBtn"
          type="submit"
          disabled={isPending || !!validationError}
          data-testid="scrape-btn"
        >
          {isPending ? 'Scraping...' : 'Scrape Newsletters'}
        </button>
      </form>

      {isPending && (
        <div className="progress">
          <div id="progress-text">
            Scraping newsletters... This may take several minutes.
          </div>
          <div className="progress-bar">
            <div
              className="progress-fill"
              style={{ width: `${progress}%` }}
            />
          </div>
        </div>
      )}

      {validationError && (
        <div className="error" role="alert">
          {validationError}
        </div>
      )}

      {state.error && (
        <div className="error" role="alert">
          Error: {state.error}
        </div>
      )}
    </div>
  )
}

export default ScrapeForm

</file>
<file path="client/src/hooks/useArticleState.js">
import { useCallback, useMemo } from 'react'
import { useSupabaseStorage } from './useSupabaseStorage'
import { getNewsletterScrapeKey } from '../lib/storageKeys'

export function useArticleState(date, url) {
  const storageKey = getNewsletterScrapeKey(date)
  const [payload, setPayload, , { loading, error }] = useSupabaseStorage(storageKey, null)

  const article = useMemo(() => {
    return payload?.articles?.find(a => a.url === url) || null
  }, [payload, url])

  const isRead = article?.read?.isRead ?? false
  const isRemoved = Boolean(article?.removed)
  const isTldrHidden = Boolean(article?.tldrHidden)

  const state = !article ? 0
    : article.removed ? 3
    : article.tldrHidden ? 2
    : article.read?.isRead ? 1
    : 0

  const updateArticle = useCallback((updater) => {
    if (!article) return

    setPayload(current => {
      if (!current) return current

      return {
        ...current,
        articles: current.articles.map(a =>
          a.url === url ? { ...a, ...updater(a) } : a
        )
      }
    })
  }, [article, url, setPayload])

  const markAsRead = useCallback(() => {
    updateArticle(() => ({
      read: { isRead: true, markedAt: new Date().toISOString() }
    }))
  }, [updateArticle])

  const markAsUnread = useCallback(() => {
    updateArticle(() => ({
      read: { isRead: false, markedAt: null }
    }))
  }, [updateArticle])

  const toggleRead = useCallback(() => {
    if (isRead) markAsUnread()
    else markAsRead()
  }, [isRead, markAsRead, markAsUnread])

  const setRemoved = useCallback((removed) => {
    updateArticle(() => ({ removed: Boolean(removed) }))
  }, [updateArticle])

  const toggleRemove = useCallback(() => {
    setRemoved(!isRemoved)
  }, [isRemoved, setRemoved])

  const setTldrHidden = useCallback((hidden) => {
    updateArticle(() => ({ tldrHidden: Boolean(hidden) }))
  }, [updateArticle])

  const markTldrHidden = useCallback(() => {
    setTldrHidden(true)
  }, [setTldrHidden])

  const unmarkTldrHidden = useCallback(() => {
    setTldrHidden(false)
  }, [setTldrHidden])

  return {
    article,
    isRead,
    isRemoved,
    isTldrHidden,
    state,
    loading,
    error,
    markAsRead,
    markAsUnread,
    toggleRead,
    setRemoved,
    toggleRemove,
    setTldrHidden,
    markTldrHidden,
    unmarkTldrHidden,
    updateArticle
  }
}

</file>
<file path="client/src/hooks/useSummary.js">
import { useState, useMemo, useCallback } from 'react'
import { useArticleState } from './useArticleState'
import { marked } from 'marked'
import DOMPurify from 'dompurify'

export function useSummary(date, url, type = 'tldr') {
  if (type === 'summary') {
    throw new Error('Summary feature has been removed. Use type="tldr" instead.')
  }

  const { article, updateArticle } = useArticleState(date, url)
  const [loading, setLoading] = useState(false)
  const [expanded, setExpanded] = useState(false)
  const [effort, setEffort] = useState('low')

  const data = article?.[type]
  const status = data?.status || 'unknown'
  const markdown = data?.markdown || ''

  const html = useMemo(() => {
    if (!markdown) return ''
    try {
      const rawHtml = marked.parse(markdown)
      return DOMPurify.sanitize(rawHtml)
    } catch (error) {
      console.error('Failed to parse markdown:', error)
      return ''
    }
  }, [markdown])

  const errorMessage = data?.errorMessage || null
  const isAvailable = status === 'available' && markdown
  const isLoading = status === 'creating' || loading
  const isError = status === 'error'

  const buttonLabel = useMemo(() => {
    if (isLoading) return 'Loading...'
    if (expanded) return 'Hide'
    if (isAvailable) return 'Available'
    if (isError) return 'Retry'
    return 'TLDR'
  }, [isLoading, expanded, isAvailable, isError, type])

  const fetch = useCallback(async (summaryEffort = effort) => {
    if (!article) return

    setLoading(true)
    setEffort(summaryEffort)

    const endpoint = '/api/tldr-url'

    try {
      const response = await window.fetch(endpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          url,
          summary_effort: summaryEffort
        })
      })

      const result = await response.json()

      if (result.success) {
        updateArticle(() => ({
          [type]: {
            status: 'available',
            markdown: result[`${type}_markdown`] || '',
            effort: summaryEffort,
            checkedAt: new Date().toISOString(),
            errorMessage: null
          }
        }))
        setExpanded(true)
      } else {
        updateArticle((current) => ({
          [type]: {
            ...(current[type] || {}),
            status: 'error',
            errorMessage: result.error || `Failed to fetch ${type}`
          }
        }))
      }
    } catch (error) {
      updateArticle((current) => ({
        [type]: {
          ...(current[type] || {}),
          status: 'error',
          errorMessage: error.message || 'Network error'
        }
      }))
      console.error(`Failed to fetch ${type}:`, error)
    } finally {
      setLoading(false)
    }
  }, [article, url, type, effort, updateArticle])

  const toggle = useCallback((summaryEffort) => {
    if (isAvailable) {
      setExpanded(!expanded)
    } else {
      fetch(summaryEffort)
    }
  }, [isAvailable, expanded, fetch])

  const collapse = useCallback(() => {
    setExpanded(false)
  }, [])

  const expand = useCallback(() => {
    setExpanded(true)
  }, [])

  return {
    data,
    status,
    markdown,
    html,
    errorMessage,
    loading: isLoading,
    expanded,
    effort,
    isAvailable,
    isError,
    buttonLabel,
    fetch,
    toggle,
    collapse,
    expand
  }
}

</file>
<file path="client/src/hooks/useSupabaseStorage.js">
import { useState, useCallback, useEffect, useRef } from 'react'

const changeListenersByKey = new Map()
const readCache = new Map()
const inflightReads = new Map()

function emitChange(key) {
  const listeners = changeListenersByKey.get(key)
  if (listeners) {
    listeners.forEach(listener => {
      try {
        listener()
      } catch (error) {
        console.error(`Storage listener failed: ${error.message}`)
      }
    })
  }

  if (typeof window !== 'undefined') {
    window.dispatchEvent(new CustomEvent('supabase-storage-change', { detail: { key } }))
  }
}

function subscribe(key, listener) {
  if (!changeListenersByKey.has(key)) {
    changeListenersByKey.set(key, new Set())
  }
  changeListenersByKey.get(key).add(listener)

  return () => {
    const listeners = changeListenersByKey.get(key)
    if (listeners) {
      listeners.delete(listener)
      if (listeners.size === 0) {
        changeListenersByKey.delete(key)
      }
    }
  }
}

async function readValue(key, defaultValue) {
  if (typeof window === 'undefined') return defaultValue

  if (readCache.has(key)) {
    return readCache.get(key)
  }

  if (inflightReads.has(key)) {
    return inflightReads.get(key)
  }

  const readPromise = (async () => {
    try {
      let value = defaultValue

      if (key.startsWith('cache:')) {
        const response = await window.fetch(`/api/storage/setting/${key}`)
        const data = await response.json()
        if (data.success) {
          value = data.value
        }
      } else if (key.startsWith('newsletters:scrapes:')) {
        const date = key.split(':')[2]
        const response = await window.fetch(`/api/storage/daily/${date}`)
        const data = await response.json()
        if (data.success) {
          value = data.payload
        }
      } else {
        console.warn(`Unknown storage key pattern: ${key}`)
      }

      readCache.set(key, value)
      return value

    } catch (error) {
      console.error(`Failed to read from storage: ${error.message}`)
      return defaultValue
    } finally {
      inflightReads.delete(key)
    }
  })()

  inflightReads.set(key, readPromise)
  return readPromise
}

async function writeValue(key, value) {
  if (typeof window === 'undefined') return

  try {
    if (key.startsWith('cache:')) {
      const response = await window.fetch(`/api/storage/setting/${key}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ value })
      })

      const data = await response.json()
      if (!data.success) {
        throw new Error(data.error || 'Failed to write setting')
      }

      readCache.set(key, value)
      emitChange(key)
      return
    }

    if (key.startsWith('newsletters:scrapes:')) {
      const date = key.split(':')[2]
      const response = await window.fetch(`/api/storage/daily/${date}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ payload: value })
      })

      const data = await response.json()
      if (!data.success) {
        throw new Error(data.error || 'Failed to write daily cache')
      }

      readCache.set(key, value)
      emitChange(key)
      return
    }

    throw new Error(`Unknown storage key pattern: ${key}`)

  } catch (error) {
    console.error(`Failed to persist to storage: ${error.message}`)
    throw error
  }
}

export function useSupabaseStorage(key, defaultValue) {
  const [value, setValue] = useState(defaultValue)
  const [loading, setLoading] = useState(true)
  const [error, setError] = useState(null)
  const valueRef = useRef(defaultValue)

  useEffect(() => {
    let cancelled = false

    readValue(key, defaultValue).then(loadedValue => {
      if (!cancelled) {
        setValue(loadedValue)
        valueRef.current = loadedValue
        setLoading(false)
      }
    }).catch(err => {
      if (!cancelled) {
        console.error(`Failed to load storage value for ${key}:`, err)
        setError(err)
        setValue(defaultValue)
        valueRef.current = defaultValue
        setLoading(false)
      }
    })

    return () => {
      cancelled = true
    }
  }, [key])

  useEffect(() => {
    const handleChange = () => {
      readValue(key, defaultValue).then(newValue => {
        setValue(newValue)
        valueRef.current = newValue
      }).catch(err => {
        console.error(`Failed to reload storage value for ${key}:`, err)
      })
    }

    return subscribe(key, handleChange)
  }, [key])

  const setValueAsync = useCallback(async (nextValue) => {
    if (typeof window === 'undefined') return

    setLoading(true)
    setError(null)

    try {
      const previous = valueRef.current
      const resolved = typeof nextValue === 'function' ? nextValue(previous) : nextValue

      if (resolved === previous) {
        setLoading(false)
        return
      }

      valueRef.current = resolved
      await writeValue(key, resolved)
      setValue(resolved)
      setLoading(false)

    } catch (err) {
      console.error(`Failed to set storage value for ${key}:`, err)
      setError(err)
      setLoading(false)
      throw err
    }
  }, [key])

  const remove = useCallback(async () => {
    await setValueAsync(undefined)
  }, [setValueAsync])

  return [value, setValueAsync, remove, { loading, error }]
}

</file>
<file path="client/src/lib/scraper.js">
/**
 * Plain JS scraper utilities for React components
 * Extracted from composables/useScraper.js
 */

import { getNewsletterScrapeKey } from './storageKeys'
import * as storageApi from './storageApi'

function computeDateRange(startDate, endDate) {
  const dates = []
  const start = new Date(startDate)
  const end = new Date(endDate)

  if (isNaN(start.getTime()) || isNaN(end.getTime())) {
    return []
  }

  if (start > end) return []

  const current = new Date(end)
  while (current >= start) {
    dates.push(current.toISOString().split('T')[0])
    current.setDate(current.getDate() - 1)
  }

  return dates
}

function buildStatsFromPayloads(payloads) {
  const uniqueUrls = new Set()
  let totalArticles = 0

  payloads.forEach(payload => {
    if (payload.articles) {
      payload.articles.forEach(article => {
        uniqueUrls.add(article.url)
        totalArticles++
      })
    }
  })

  return {
    total_articles: totalArticles,
    unique_urls: uniqueUrls.size,
    dates_processed: payloads.length,
    dates_with_content: payloads.filter(p => p.articles?.length > 0).length
  }
}

async function isRangeCached(startDate, endDate, cacheEnabled) {
  if (!cacheEnabled) return false

  const dates = computeDateRange(startDate, endDate)

  for (const date of dates) {
    const isCached = await storageApi.isDateCached(date)
    if (!isCached) {
      return false
    }
  }

  return true
}

function normalizeIsoDate(value) {
  if (typeof value !== 'string') return null
  const trimmed = value.trim()
  if (!trimmed) return null
  const date = new Date(trimmed)
  if (isNaN(date.getTime())) return null
  return date.toISOString().split('T')[0]
}

function buildDailyPayloadsFromScrape(data) {
  const payloadByDate = new Map()
  const issuesByDate = new Map()

  if (Array.isArray(data.issues)) {
    data.issues.forEach(issue => {
      const date = normalizeIsoDate(issue.date)
      if (!date) return

      if (!issuesByDate.has(date)) {
        issuesByDate.set(date, [])
      }
      issuesByDate.get(date).push(issue)
    })
  }

  if (Array.isArray(data.articles)) {
    data.articles.forEach(article => {
      const date = normalizeIsoDate(article.date)
      if (!date) return

      const articleData = {
        url: article.url,
        title: article.title || article.url,
        articleMeta: article.article_meta || "",
        issueDate: date,
        category: article.category || 'Newsletter',
        sourceId: article.source_id || null,
        section: article.section_title || null,
        sectionEmoji: article.section_emoji || null,
        sectionOrder: article.section_order ?? null,
        newsletterType: article.newsletter_type || null,
        removed: Boolean(article.removed),
        tldrHidden: false,
        tldr: { status: 'unknown', markdown: '', effort: 'low', checkedAt: null, errorMessage: null },
        read: { isRead: false, markedAt: null }
      }

      if (!payloadByDate.has(date)) {
        payloadByDate.set(date, [])
      }
      payloadByDate.get(date).push(articleData)
    })
  }

  const payloads = []
  payloadByDate.forEach((articles, date) => {
    const issues = issuesByDate.get(date) || []
    payloads.push({
      date,
      cachedAt: new Date().toISOString(),
      articles,
      issues
    })
  })

  return payloads.sort((a, b) => (a.date < b.date ? 1 : a.date > b.date ? -1 : 0))
}

async function mergeWithCache(payloads) {
  const merged = []

  for (const payload of payloads) {
    const existing = await storageApi.getDailyPayload(payload.date)

    if (existing) {
      const mergedPayload = {
        ...payload,
        articles: payload.articles.map(article => {
          const existingArticle = existing.articles?.find(a => a.url === article.url)
          if (existingArticle) {
            return {
              ...article,
              tldr: existingArticle.tldr || article.tldr,
              read: existingArticle.read || article.read,
              removed: existingArticle.removed ?? article.removed,
              tldrHidden: existingArticle.tldrHidden ?? article.tldrHidden
            }
          }
          return article
        })
      }

      await storageApi.setDailyPayload(payload.date, mergedPayload)
      merged.push(mergedPayload)
    } else {
      await storageApi.setDailyPayload(payload.date, payload)
      merged.push(payload)
    }
  }

  return merged
}

export async function loadFromCache(startDate, endDate) {
  const payloads = await storageApi.getDailyPayloadsRange(startDate, endDate)

  if (!payloads || payloads.length === 0) {
    return null
  }

  return {
    success: true,
    payloads,
    source: 'local cache',
    stats: buildStatsFromPayloads(payloads)
  }
}

export async function scrapeNewsletters(startDate, endDate, cacheEnabled = true) {
  if (await isRangeCached(startDate, endDate, cacheEnabled)) {
    const cached = await loadFromCache(startDate, endDate)
    if (cached) {
      return cached
    }
  }

  const response = await window.fetch('/api/scrape', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      start_date: startDate,
      end_date: endDate
    })
  })

  const data = await response.json()

  if (data.success) {
    const payloads = buildDailyPayloadsFromScrape(data)
    const mergedPayloads = cacheEnabled ? await mergeWithCache(payloads) : payloads

    return {
      success: true,
      payloads: mergedPayloads,
      source: 'Live scrape',
      stats: data.stats
    }
  } else {
    throw new Error(data.error || 'Scraping failed')
  }
}

</file>
<file path="client/src/lib/storageApi.js">
export async function isDateCached(date) {
  const response = await window.fetch(`/api/storage/is-cached/${date}`)
  const data = await response.json()

  if (data.success) {
    return data.is_cached
  }

  throw new Error(data.error || 'Failed to check cache')
}

export async function getDailyPayload(date) {
  const response = await window.fetch(`/api/storage/daily/${date}`)
  const data = await response.json()

  if (data.success) {
    return data.payload
  }

  return null
}

export async function setDailyPayload(date, payload) {
  const response = await window.fetch(`/api/storage/daily/${date}`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ payload })
  })

  const data = await response.json()

  if (!data.success) {
    throw new Error(data.error || 'Failed to save payload')
  }

  return data.data
}

export async function getDailyPayloadsRange(startDate, endDate) {
  const response = await window.fetch('/api/storage/daily-range', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ start_date: startDate, end_date: endDate })
  })

  const data = await response.json()

  if (data.success) {
    return data.payloads
  }

  throw new Error(data.error || 'Failed to load payloads')
}

</file>
<file path="client/src/lib/storageKeys.js">
export const STORAGE_KEYS = {
  CACHE_ENABLED: 'cache:enabled'
}

/**
 * Get the storage key for newsletter scrape data for a specific date.
 * @param {string} date - ISO date string (YYYY-MM-DD)
 * @returns {string} The storage key used for Supabase API endpoints
 */
export function getNewsletterScrapeKey(date) {
  return `newsletters:scrapes:${date}`
}

</file>
<file path="client/src/main.jsx">
import React from 'react'
import ReactDOM from 'react-dom/client'
import App from './App'

ReactDOM.createRoot(document.getElementById('root')).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
)

</file>
</files>
