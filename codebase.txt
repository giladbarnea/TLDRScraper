<files>
<file path="hackernews_adapter.py">
class HackerNewsAdapter(NewsletterAdapter):
    """Adapter for HackerNews using Algolia HN Search API."""
    ...

    def __init__(self, config):
        """Initialize with config.

        Note: We don't use the HTML-to-markdown functionality from base class.
        """
    ...

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch HackerNews stories using Algolia API with server-side filtering.

        Strategy: Single combined query for all story types (story, ask_hn, show_hn)
        with quality filters applied server-side. This is ~67% fewer requests and
        ~77% less data than the previous approach.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
    ...

    def _fetch_stories_algolia(
        self,
        start_timestamp: int,
        end_timestamp: int,
        min_points: int = 30,
        min_comments: int = 5,
        limit: int = 50
    ) -> list:
        """Fetch stories from Algolia HN Search API with server-side filtering.

        Args:
            start_timestamp: Unix timestamp for start of date range
            end_timestamp: Unix timestamp for end of date range
            min_points: Minimum points (upvotes) required
            min_comments: Minimum comment count required
            limit: Maximum number of stories to return

        Returns:
            List of story dictionaries from Algolia API
        """
    ...

    def _algolia_story_to_article(self, story: dict, date: str) -> dict | None:
        """Convert Algolia HN story to article dict.

        Args:
            story: Algolia HN story dictionary
            date: Date string

        Returns:
            Article dictionary or None if story should be skipped
        """
    ...
</file>
<file path="newsletter_adapter.py">
class NewsletterAdapter:
    """Base adapter for newsletter sources.

    Subclasses can either:
    1. Implement fetch_issue, parse_articles, and extract_issue_metadata for HTML-based sources
    2. Override scrape_date() entirely for API-based sources or custom workflows
    """
    ...

    def __init__(self, config: NewsletterSourceConfig):
        """Initialize adapter with source configuration.

        Args:
            config: Configuration object defining source-specific settings
        """
    ...

    def fetch_issue(self, date: str, newsletter_type: str) -> str | None:
        """Fetch raw HTML for a specific issue.

        Override this method for HTML-based sources.
        For API-based sources, override scrape_date() instead.

        Args:
            date: Date string in format used by source
            newsletter_type: Type/category within source (e.g., "tech", "ai")

        Returns:
            HTML content as string, or None if issue not found
        """
    ...

    def parse_articles(
        self, markdown: str, date: str, newsletter_type: str
    ) -> list[dict]:
        """Parse articles from markdown content.

        Override this method for HTML-based sources.
        For API-based sources, override scrape_date() instead.

        Args:
            markdown: Converted markdown content
            date: Date string for the issue
            newsletter_type: Type/category within source

        Returns:
            List of article dictionaries with keys: title, url, category, date, etc.
        """
    ...

    def extract_issue_metadata(
        self, markdown: str, date: str, newsletter_type: str
    ) -> dict | None:
        """Extract issue metadata (title, subtitle, sections).

        Override this method for HTML-based sources.
        For API-based sources, override scrape_date() instead.

        Args:
            markdown: Converted markdown content
            date: Date string for the issue
            newsletter_type: Type/category within source

        Returns:
            Dictionary with issue metadata, or None if no metadata found
        """
    ...

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Template method - orchestrates fetch + parse + normalize.

        This default implementation follows the HTML scraping workflow:
        1. Fetch HTML for each type configured for this source
        2. Convert HTML to markdown
        3. Parse articles and extract metadata
        4. Filter out excluded URLs
        5. Normalize response with source_id

        Subclasses can override this entire method for different workflows
        (e.g., API-based sources that don't use HTML conversion).

        Args:
            date: Date string to scrape
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary with source_id, articles, and issues
        """
    ...

    def _html_to_markdown(self, html: str) -> str:
        """Convert HTML to markdown using BeautifulSoup and MarkItDown.

        Args:
            html: Raw HTML content

        Returns:
            Markdown string
        """
    ...

    def _normalize_response(self, articles: list[dict], issues: list[dict]) -> dict:
        """Convert to standardized format with source_id.

        This ensures every article and issue includes the source_id to prevent
        identity collisions when multiple sources are aggregated.

        Args:
            articles: List of parsed articles
            issues: List of parsed issue metadata

        Returns:
            Normalized response with source_id added to all items
        """
    ...
</file>
<file path="newsletter_config.py">
class NewsletterSourceConfig:
    """Configuration for a newsletter source."""
    ...
</file>
<file path="newsletter_merger.py">
def build_markdown_output(
    start_date, end_date, grouped_articles: dict[str, list[dict]], issues_by_key: dict
) -> str:
    """Generate neutral markdown output from grouped articles.

    Args:
        start_date: Start date for the range
        end_date: End date for the range
        grouped_articles: Articles grouped by date
        issues_by_key: Issue metadata indexed by (date, source_id, category)

    Returns:
        Markdown formatted string
    """
    ...

    def build_article_lines(article_list):
    ...

        def get_category_sort_key(category):
    ...
</file>
<file path="newsletter_scraper.py">
def _get_adapter_for_source(config):
    """Factory pattern - returns appropriate adapter for source.

    Args:
        config: NewsletterSourceConfig instance

    Returns:
        NewsletterAdapter instance

    Raises:
        ValueError: If no adapter exists for the source
    """
    ...

def _normalize_article_payload(article: dict) -> dict:
    """Normalize article dict into API payload format.

    >>> article = {"url": "https://example.com", "title": "Test", "date": "2024-01-01", "category": "Tech", "removed": None}
    >>> result = _normalize_article_payload(article)
    >>> result["removed"]
    False
    """
    ...

def _group_articles_by_date(articles: list[dict]) -> dict[str, list[dict]]:
    """Group articles by date string.

    >>> articles = [{"date": "2024-01-01", "title": "Test"}]
    >>> result = _group_articles_by_date(articles)
    >>> "2024-01-01" in result
    True
    """
    ...

def _sort_issues(issues: list[dict]) -> list[dict]:
    """Sort issues by date DESC, source sort_order ASC, category ASC.

    >>> issues = [{"date": "2024-01-01", "source_id": "tldr_tech", "category": "Tech"}]
    >>> result = _sort_issues(issues)
    >>> len(result) == 1
    True
    """
    ...

def _compute_stats(
    articles: list[dict],
    url_set: set[str],
    dates: list,
    grouped_articles: dict[str, list[dict]],
    network_fetches: int,
) -> dict:
    """Compute scrape statistics.

    >>> stats = _compute_stats([], set(), [], {}, 0)
    >>> stats["total_articles"]
    0
    """
    ...

def _build_scrape_response(
    start_date,
    end_date,
    dates,
    all_articles,
    url_set,
    issue_metadata_by_key,
    network_fetches,
):
    """Orchestrate building the complete scrape response."""
    ...

def _collect_newsletters_for_date_from_source(
    source_id,
    config,
    date,
    date_str,
    processed_count,
    total_count,
    url_set,
    all_articles,
    issue_metadata_by_key,
    excluded_urls,
):
    """Collect newsletters for a date using source adapter.

    Args:
        source_id: Source identifier
        config: NewsletterSourceConfig instance
        date: Date object
        date_str: Date string
        processed_count: Current progress counter
        total_count: Total items to process
        url_set: Set of URLs for deduplication
        all_articles: List to append articles to
        issue_metadata_by_key: Dict to store issue metadata
        excluded_urls: List of canonical URLs to exclude

    Returns:
        Tuple of (updated_processed_count, network_articles_count)
    """
    ...

def scrape_date_range(start_date, end_date, source_ids=None, excluded_urls=None):
    """Scrape newsletters in date range using configured adapters.

    Args:
        start_date: Start date
        end_date: End date
        source_ids: Optional list of source IDs to scrape. If None, scrapes all configured sources.
        excluded_urls: List of canonical URLs to exclude from results

    Returns:
        Response dictionary with articles and issues
    """
    ...

    def _issue_sort_key(issue: dict) -> tuple:
    ...
</file>
<file path="serve.py">
def index():
    """Serve the React app"""
    ...

def scrape_newsletters_in_date_range():
    """Backend proxy to scrape newsletters. Expects start_date, end_date, excluded_urls, and optionally sources in the request body."""
    ...

def tldr_url(model: str = DEFAULT_MODEL):
    """Create a TLDR of the content at a URL.

    Requires 'url'. Optional: 'summary_effort' to set the reasoning effort level, 'model' query param to specify OpenAI model.
    """
    ...

def get_storage_setting(key):
    """Get setting value by key."""
    ...

def set_storage_setting(key):
    """Set setting value by key."""
    ...

def get_storage_daily(date):
    """Get cached payload for a specific date."""
    ...

def set_storage_daily(date):
    """Save or update daily payload."""
    ...

def get_storage_daily_range():
    """Get all cached payloads in date range."""
    ...

def check_storage_is_cached(date):
    """Check if a specific date exists in cache."""
    ...
</file>
<file path="storage_service.py">
def get_setting(key):
    """
    Get setting value by key.

    >>> get_setting('cache:enabled')
    True
    """
    ...

def set_setting(key, value):
    """
    Set setting value by key (upsert).

    >>> set_setting('cache:enabled', False)
    {'key': 'cache:enabled', 'value': False, ...}
    """
    ...

def get_daily_payload(date):
    """
    Get cached payload for a specific date.

    >>> get_daily_payload('2025-11-09')
    {'date': '2025-11-09', 'articles': [...], ...}
    """
    ...

def set_daily_payload(date, payload):
    """
    Save or update daily payload (upsert).

    >>> set_daily_payload('2025-11-09', {'date': '2025-11-09', 'articles': [...]})
    {'date': '2025-11-09', 'payload': {...}, ...}
    """
    ...

def get_daily_payloads_range(start_date, end_date):
    """
    Get all cached payloads in date range (inclusive).

    >>> get_daily_payloads_range('2025-11-07', '2025-11-09')
    [{'date': '2025-11-09', ...}, {'date': '2025-11-08', ...}, ...]
    """
    ...

def is_date_cached(date):
    """
    Check if a specific date exists in cache.

    >>> is_date_cached('2025-11-09')
    True
    """
    ...
</file>
<file path="summarizer.py">
def normalize_summary_effort(value: str) -> str:
    """Normalize summary effort value to a supported option."""
    ...

def _is_github_repo_url(url: str) -> bool:
    """Check if URL is a GitHub repository URL."""
    ...

def _build_jina_reader_url(url: str) -> str:
    """Build r.jina.ai reader URL for a target page.

    >>> _build_jina_reader_url('https://openai.com/index/introducing-agentkit')
    'https://r.jina.ai/http://openai.com/index/introducing-agentkit'
    >>> _build_jina_reader_url('https://example.com/path?x=1')
    'https://r.jina.ai/http://example.com/path?x=1'
    """
    ...

def _scrape_with_curl_cffi(
    ...

def _scrape_with_jina_reader(url: str, *, timeout: int) -> requests.Response:
    ...

def _scrape_with_firecrawl(url: str, *, timeout: int) -> requests.Response:
    ...

def scrape_url(url: str, *, timeout: int = 10) -> Response:
    ...

def _fetch_github_readme(url: str) -> str:
    """Fetch README.md content from a GitHub repository URL."""
    ...

def url_to_markdown(url: str) -> str:
    """Fetch URL and convert to markdown. For GitHub repos, fetches README.md."""
    ...

def tldr_url(url: str, summary_effort: str = DEFAULT_TLDR_REASONING_EFFORT, model: str = DEFAULT_MODEL) -> str:
    """Get markdown content from URL and create a TLDR with LLM.

    Args:
        url: The URL to TLDR
        summary_effort: OpenAI reasoning effort level
        model: OpenAI model to use

    Returns:
        The TLDR markdown
    """
    ...

def _fetch_prompt(
    ...

def _fetch_tldr_prompt(
    owner: str = "giladbarnea",
    repo: str = "llm-templates",
    path: str = "text/tldr.md",
    ref: str = "main",
) -> str:
    """Fetch TLDR prompt from GitHub (cached in memory)."""
    ...

def _call_llm(prompt: str, summary_effort: str = DEFAULT_TLDR_REASONING_EFFORT, model: str = DEFAULT_MODEL) -> str:
    """Call OpenAI API with prompt."""
    ...

    def response_iter_content_stub(self, *args, **kwargs):
    ...
</file>
<file path="supabase_client.py">
def _create_unverified_context(*args, **kwargs):
    ...

def get_supabase_client():
    ...
</file>
<file path="tldr_adapter.py">
class NewsletterSection:
    """Represents a section within a newsletter issue."""
    ...

class NewsletterIssue:
    """Represents metadata for a newsletter issue."""
    ...

class ParsedMarkdown:
    """Structured result from parsing markdown once."""
    ...

class TLDRAdapter(NewsletterAdapter):
    """Adapter for TLDR newsletter sources (Tech, AI, etc.)."""
    ...

    def fetch_issue(self, date: str, newsletter_type: str) -> str | None:
        """Fetch TLDR newsletter HTML for a specific date and type.

        Args:
            date: Date string in YYYY-MM-DD format
            newsletter_type: Newsletter type (e.g., "tech", "ai")

        Returns:
            HTML content as string, or None if not found
        """
    ...

    def _parse_markdown_structure(
        self, markdown: str, date: str, newsletter_type: str
    ) -> ParsedMarkdown:
        """Parse markdown once into structured format.

        Extracts all structure (headings, sections, links) in a single pass.
        """
    ...

    def parse_articles(
        self, markdown: str, date: str, newsletter_type: str
    ) -> list[dict]:
        """Parse TLDR articles from markdown content.

        Args:
            markdown: Markdown content converted from HTML
            date: Date string for the issue
            newsletter_type: Newsletter type (e.g., "tech", "ai")

        Returns:
            List of article dictionaries
        """
    ...

    def extract_issue_metadata(
        self, markdown: str, date: str, newsletter_type: str
    ) -> dict | None:
        """Extract TLDR issue metadata (title, subtitle, sections).

        Args:
            markdown: Markdown content converted from HTML
            date: Date string for the issue
            newsletter_type: Newsletter type (e.g., "tech", "ai")

        Returns:
            Dictionary with issue metadata, or None if no metadata found
        """
    ...

    def _is_symbol_only_line(text: str) -> bool:
        """Check if a line contains only symbols/emoji (no alphanumeric chars).

        Args:
            text: Text to check

        Returns:
            True if line contains only symbols
        """
    ...

    def _is_file_url(url: str) -> bool:
        """Check if URL points to a file (image, PDF, etc.) rather than a web page.

        Args:
            url: URL to check

        Returns:
            True if URL appears to be a file
        """
    ...
</file>
<file path="tldr_app.py">
def scrape_newsletters(
    start_date_text: str, end_date_text: str, source_ids: list[str] | None = None, excluded_urls: list[str] | None = None
) -> dict:
    """Scrape newsletters in date range.

    Args:
        start_date_text: Start date in ISO format
        end_date_text: End date in ISO format
        source_ids: Optional list of source IDs to scrape. Defaults to all configured sources.
        excluded_urls: List of canonical URLs to exclude from results

    Returns:
        Response dictionary with articles and issues
    """
    ...

def tldr_url(
    ...
</file>
<file path="tldr_service.py">
def _parse_date_range(
    start_date_text: str, end_date_text: str
) -> tuple[datetime, datetime]:
    """Parse ISO date strings and enforce range limits.

    >>> _parse_date_range("2024-01-01", "2024-01-02")[0].isoformat()
    '2024-01-01T00:00:00'
    """
    ...

def scrape_newsletters_in_date_range(
    start_date_text: str, end_date_text: str, source_ids: list[str] | None = None, excluded_urls: list[str] | None = None
) -> dict:
    """Scrape newsletters in date range.

    Args:
        start_date_text: Start date in ISO format
        end_date_text: End date in ISO format
        source_ids: Optional list of source IDs to scrape. Defaults to all configured sources.
        excluded_urls: List of canonical URLs to exclude from results

    Returns:
        Response dictionary with articles and issues
    """
    ...

def fetch_tldr_prompt_template() -> str:
    ...

def tldr_url_content(
    ...
</file>
<file path="util.py">
def log(msg, *args, **kwargs):
    ...

def resolve_env_var(name: str, default: str = "") -> str:
    """
    Resolve environment variable, trying both direct name and TLDR_SCRAPER_ prefixed version.
    Strips surrounding quotes from the value if present.

    >>> os.environ['TEST_VAR'] = '"value"'
    >>> resolve_env_var('TEST_VAR')
    'value'
    >>> os.environ['TEST_VAR'] = 'value'
    >>> resolve_env_var('TEST_VAR')
    'value'
    """
    ...

def get_date_range(start_date, end_date):
    """Generate list of dates between start and end (inclusive)"""
    ...

def format_date_for_url(date):
    """Format date as YYYY-MM-DD for TLDR URL"""
    ...

def canonicalize_url(url) -> str:
    """Canonicalize URL for better deduplication.

    Normalizes:
    - Removes scheme (http:// or https://)
    - Removes www. prefix
    - Removes query parameters
    - Removes URL fragments
    - Removes trailing slashes (including root)
    - Lowercases domain
    """
    ...

def get_domain_name(url) -> str:
    """Extract a friendly domain name from a URL"""
    ...
</file>
</files>
