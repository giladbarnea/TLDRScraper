<documents>
<document path="hackernews_adapter.py">
class HackerNewsAdapter(NewsletterAdapter):
    """Adapter for HackerNews using Algolia HN Search API."""
    ...

    def __init__(self, config):
        """Initialize with config.

        Note: We don't use the HTML-to-markdown functionality from base class.
        """
    ...

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Fetch HackerNews stories using Algolia API with server-side filtering.

        Strategy: Single combined query for all story types (story, ask_hn, show_hn)
        with quality filters applied server-side. This is ~67% fewer requests and
        ~77% less data than the previous approach.

        Args:
            date: Date string in YYYY-MM-DD format
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary
        """
    ...

    def _fetch_stories_algolia(
        self,
        start_timestamp: int,
        end_timestamp: int,
        min_points: int = 30,
        min_comments: int = 5,
        limit: int = 50
    ) -> list:
        """Fetch stories from Algolia HN Search API with server-side filtering.

        Args:
            start_timestamp: Unix timestamp for start of date range
            end_timestamp: Unix timestamp for end of date range
            min_points: Minimum points (upvotes) required
            min_comments: Minimum comment count required
            limit: Maximum number of stories to return

        Returns:
            List of story dictionaries from Algolia API
        """
    ...

    def _algolia_story_to_article(self, story: dict, date: str) -> dict | None:
        """Convert Algolia HN story to article dict.

        Args:
            story: Algolia HN story dictionary
            date: Date string

        Returns:
            Article dictionary or None if story should be skipped
        """
    ...
</document>
<document path="newsletter_adapter.py">
class NewsletterAdapter:
    """Base adapter for newsletter sources.

    Subclasses can either:
    1. Implement fetch_issue, parse_articles, and extract_issue_metadata for HTML-based sources
    2. Override scrape_date() entirely for API-based sources or custom workflows
    """
    ...

    def __init__(self, config: NewsletterSourceConfig):
        """Initialize adapter with source configuration.

        Args:
            config: Configuration object defining source-specific settings
        """
    ...

    def fetch_issue(self, date: str, newsletter_type: str) -> str | None:
        """Fetch raw HTML for a specific issue.

        Override this method for HTML-based sources.
        For API-based sources, override scrape_date() instead.

        Args:
            date: Date string in format used by source
            newsletter_type: Type/category within source (e.g., "tech", "ai")

        Returns:
            HTML content as string, or None if issue not found
        """
    ...

    def parse_articles(
        self, markdown: str, date: str, newsletter_type: str
    ) -> list[dict]:
        """Parse articles from markdown content.

        Override this method for HTML-based sources.
        For API-based sources, override scrape_date() instead.

        Args:
            markdown: Converted markdown content
            date: Date string for the issue
            newsletter_type: Type/category within source

        Returns:
            List of article dictionaries with keys: title, url, category, date, etc.
        """
    ...

    def extract_issue_metadata(
        self, markdown: str, date: str, newsletter_type: str
    ) -> dict | None:
        """Extract issue metadata (title, subtitle, sections).

        Override this method for HTML-based sources.
        For API-based sources, override scrape_date() instead.

        Args:
            markdown: Converted markdown content
            date: Date string for the issue
            newsletter_type: Type/category within source

        Returns:
            Dictionary with issue metadata, or None if no metadata found
        """
    ...

    def scrape_date(self, date: str, excluded_urls: list[str]) -> dict:
        """Template method - orchestrates fetch + parse + normalize.

        This default implementation follows the HTML scraping workflow:
        1. Fetch HTML for each type configured for this source
        2. Convert HTML to markdown
        3. Parse articles and extract metadata
        4. Filter out excluded URLs
        5. Normalize response with source_id

        Subclasses can override this entire method for different workflows
        (e.g., API-based sources that don't use HTML conversion).

        Args:
            date: Date string to scrape
            excluded_urls: List of canonical URLs to exclude from results

        Returns:
            Normalized response dictionary with source_id, articles, and issues
        """
    ...

    def _html_to_markdown(self, html: str) -> str:
        """Convert HTML to markdown using BeautifulSoup and MarkItDown.

        Args:
            html: Raw HTML content

        Returns:
            Markdown string
        """
    ...

    def _normalize_response(self, articles: list[dict], issues: list[dict]) -> dict:
        """Convert to standardized format with source_id.

        This ensures every article and issue includes the source_id to prevent
        identity collisions when multiple sources are aggregated.

        Args:
            articles: List of parsed articles
            issues: List of parsed issue metadata

        Returns:
            Normalized response with source_id added to all items
        """
    ...
</document>
<document path="newsletter_config.py">
class NewsletterSourceConfig:
    """Configuration for a newsletter source."""
    ...
</document>
<document path="newsletter_merger.py">
def build_markdown_output(
    start_date, end_date, grouped_articles: dict[str, list[dict]], issues_by_key: dict
) -> str:
    """Generate neutral markdown output from grouped articles.

    Args:
        start_date: Start date for the range
        end_date: End date for the range
        grouped_articles: Articles grouped by date
        issues_by_key: Issue metadata indexed by (date, source_id, category)

    Returns:
        Markdown formatted string
    """
    ...

    def build_article_lines(article_list):
    ...

        def get_category_sort_key(category):
    ...
</document>
<document path="newsletter_scraper.py">
def _get_adapter_for_source(config):
    """Factory pattern - returns appropriate adapter for source.

    Args:
        config: NewsletterSourceConfig instance

    Returns:
        NewsletterAdapter instance

    Raises:
        ValueError: If no adapter exists for the source
    """
    ...

def _normalize_article_payload(article: dict) -> dict:
    """Normalize article dict into API payload format.

    >>> article = {"url": "https://example.com", "title": "Test", "date": "2024-01-01", "category": "Tech", "removed": None}
    >>> result = _normalize_article_payload(article)
    >>> result["removed"]
    False
    """
    ...

def _group_articles_by_date(articles: list[dict]) -> dict[str, list[dict]]:
    """Group articles by date string.

    >>> articles = [{"date": "2024-01-01", "title": "Test"}]
    >>> result = _group_articles_by_date(articles)
    >>> "2024-01-01" in result
    True
    """
    ...

def _sort_issues(issues: list[dict]) -> list[dict]:
    """Sort issues by date DESC, source sort_order ASC, category ASC.

    >>> issues = [{"date": "2024-01-01", "source_id": "tldr_tech", "category": "Tech"}]
    >>> result = _sort_issues(issues)
    >>> len(result) == 1
    True
    """
    ...

def _compute_stats(
    articles: list[dict],
    url_set: set[str],
    dates: list,
    grouped_articles: dict[str, list[dict]],
    network_fetches: int,
) -> dict:
    """Compute scrape statistics.

    >>> stats = _compute_stats([], set(), [], {}, 0)
    >>> stats["total_articles"]
    0
    """
    ...

def _build_scrape_response(
    start_date,
    end_date,
    dates,
    all_articles,
    url_set,
    issue_metadata_by_key,
    network_fetches,
):
    """Orchestrate building the complete scrape response."""
    ...

def _collect_newsletters_for_date_from_source(
    source_id,
    config,
    date,
    date_str,
    processed_count,
    total_count,
    url_set,
    all_articles,
    issue_metadata_by_key,
    excluded_urls,
):
    """Collect newsletters for a date using source adapter.

    Args:
        source_id: Source identifier
        config: NewsletterSourceConfig instance
        date: Date object
        date_str: Date string
        processed_count: Current progress counter
        total_count: Total items to process
        url_set: Set of URLs for deduplication
        all_articles: List to append articles to
        issue_metadata_by_key: Dict to store issue metadata
        excluded_urls: List of canonical URLs to exclude

    Returns:
        Tuple of (updated_processed_count, network_articles_count)
    """
    ...

def scrape_date_range(start_date, end_date, source_ids=None, excluded_urls=None):
    """Scrape newsletters in date range using configured adapters.

    Args:
        start_date: Start date
        end_date: End date
        source_ids: Optional list of source IDs to scrape. If None, scrapes all configured sources.
        excluded_urls: List of canonical URLs to exclude from results

    Returns:
        Response dictionary with articles and issues
    """
    ...

    def _issue_sort_key(issue: dict) -> tuple:
    ...
</document>
<document path="serve.py">
def index():
    """Serve the React app"""
    ...

def scrape_newsletters_in_date_range():
    """Backend proxy to scrape newsletters. Expects start_date, end_date, excluded_urls, and optionally sources in the request body."""
    ...

def tldr_url(model: str = DEFAULT_MODEL):
    """Create a TLDR of the content at a URL.

    Requires 'url'. Optional: 'summary_effort' to set the reasoning effort level, 'model' query param to specify OpenAI model.
    """
    ...

def get_storage_setting(key):
    """Get setting value by key."""
    ...

def set_storage_setting(key):
    """Set setting value by key."""
    ...

def get_storage_daily(date):
    """Get cached payload for a specific date."""
    ...

def set_storage_daily(date):
    """Save or update daily payload."""
    ...

def get_storage_daily_range():
    """Get all cached payloads in date range."""
    ...

def check_storage_is_cached(date):
    """Check if a specific date exists in cache."""
    ...
</document>
<document path="storage_service.py">
def get_setting(key):
    """
    Get setting value by key.

    >>> get_setting('cache:enabled')
    True
    """
    ...

def set_setting(key, value):
    """
    Set setting value by key (upsert).

    >>> set_setting('cache:enabled', False)
    {'key': 'cache:enabled', 'value': False, ...}
    """
    ...

def get_daily_payload(date):
    """
    Get cached payload for a specific date.

    >>> get_daily_payload('2025-11-09')
    {'date': '2025-11-09', 'articles': [...], ...}
    """
    ...

def set_daily_payload(date, payload):
    """
    Save or update daily payload (upsert).

    >>> set_daily_payload('2025-11-09', {'date': '2025-11-09', 'articles': [...]})
    {'date': '2025-11-09', 'payload': {...}, ...}
    """
    ...

def get_daily_payloads_range(start_date, end_date):
    """
    Get all cached payloads in date range (inclusive).

    >>> get_daily_payloads_range('2025-11-07', '2025-11-09')
    [{'date': '2025-11-09', ...}, {'date': '2025-11-08', ...}, ...]
    """
    ...

def is_date_cached(date):
    """
    Check if a specific date exists in cache.

    >>> is_date_cached('2025-11-09')
    True
    """
    ...
</document>
<document path="summarizer.py">
def normalize_summary_effort(value: str) -> str:
    """Normalize summary effort value to a supported option."""
    ...

def _is_github_repo_url(url: str) -> bool:
    """Check if URL is a GitHub repository URL."""
    ...

def _build_jina_reader_url(url: str) -> str:
    """Build r.jina.ai reader URL for a target page.

    >>> _build_jina_reader_url('https://openai.com/index/introducing-agentkit')
    'https://r.jina.ai/http://openai.com/index/introducing-agentkit'
    >>> _build_jina_reader_url('https://example.com/path?x=1')
    'https://r.jina.ai/http://example.com/path?x=1'
    """
    ...

def _scrape_with_curl_cffi(
    ...

def _scrape_with_jina_reader(url: str, *, timeout: int) -> requests.Response:
    ...

def _scrape_with_firecrawl(url: str, *, timeout: int) -> requests.Response:
    ...

def scrape_url(url: str, *, timeout: int = 10) -> Response:
    ...

def _fetch_github_readme(url: str) -> str:
    """Fetch README.md content from a GitHub repository URL."""
    ...

def url_to_markdown(url: str) -> str:
    """Fetch URL and convert to markdown. For GitHub repos, fetches README.md."""
    ...

def tldr_url(url: str, summary_effort: str = DEFAULT_TLDR_REASONING_EFFORT, model: str = DEFAULT_MODEL) -> str:
    """Get markdown content from URL and create a TLDR with LLM.

    Args:
        url: The URL to TLDR
        summary_effort: OpenAI reasoning effort level
        model: OpenAI model to use

    Returns:
        The TLDR markdown
    """
    ...

def _fetch_prompt(
    ...

def _fetch_tldr_prompt(
    owner: str = "giladbarnea",
    repo: str = "llm-templates",
    path: str = "text/tldr.md",
    ref: str = "main",
) -> str:
    """Fetch TLDR prompt from GitHub (cached in memory)."""
    ...

def _call_llm(prompt: str, summary_effort: str = DEFAULT_TLDR_REASONING_EFFORT, model: str = DEFAULT_MODEL) -> str:
    """Call OpenAI API with prompt."""
    ...

    def response_iter_content_stub(self, *args, **kwargs):
    ...
</document>
<document path="supabase_client.py">
def _create_unverified_context(*args, **kwargs):
    ...

def get_supabase_client():
    ...
</document>
<document path="tldr_adapter.py">
class NewsletterSection:
    """Represents a section within a newsletter issue."""
    ...

class NewsletterIssue:
    """Represents metadata for a newsletter issue."""
    ...

class ParsedMarkdown:
    """Structured result from parsing markdown once."""
    ...

class TLDRAdapter(NewsletterAdapter):
    """Adapter for TLDR newsletter sources (Tech, AI, etc.)."""
    ...

    def fetch_issue(self, date: str, newsletter_type: str) -> str | None:
        """Fetch TLDR newsletter HTML for a specific date and type.

        Args:
            date: Date string in YYYY-MM-DD format
            newsletter_type: Newsletter type (e.g., "tech", "ai")

        Returns:
            HTML content as string, or None if not found
        """
    ...

    def _parse_markdown_structure(
        self, markdown: str, date: str, newsletter_type: str
    ) -> ParsedMarkdown:
        """Parse markdown once into structured format.

        Extracts all structure (headings, sections, links) in a single pass.
        """
    ...

    def parse_articles(
        self, markdown: str, date: str, newsletter_type: str
    ) -> list[dict]:
        """Parse TLDR articles from markdown content.

        Args:
            markdown: Markdown content converted from HTML
            date: Date string for the issue
            newsletter_type: Newsletter type (e.g., "tech", "ai")

        Returns:
            List of article dictionaries
        """
    ...

    def extract_issue_metadata(
        self, markdown: str, date: str, newsletter_type: str
    ) -> dict | None:
        """Extract TLDR issue metadata (title, subtitle, sections).

        Args:
            markdown: Markdown content converted from HTML
            date: Date string for the issue
            newsletter_type: Newsletter type (e.g., "tech", "ai")

        Returns:
            Dictionary with issue metadata, or None if no metadata found
        """
    ...

    def _is_symbol_only_line(text: str) -> bool:
        """Check if a line contains only symbols/emoji (no alphanumeric chars).

        Args:
            text: Text to check

        Returns:
            True if line contains only symbols
        """
    ...

    def _is_file_url(url: str) -> bool:
        """Check if URL points to a file (image, PDF, etc.) rather than a web page.

        Args:
            url: URL to check

        Returns:
            True if URL appears to be a file
        """
    ...
</document>
<document path="tldr_app.py">
def scrape_newsletters(
    start_date_text: str, end_date_text: str, source_ids: list[str] | None = None, excluded_urls: list[str] | None = None
) -> dict:
    """Scrape newsletters in date range.

    Args:
        start_date_text: Start date in ISO format
        end_date_text: End date in ISO format
        source_ids: Optional list of source IDs to scrape. Defaults to all configured sources.
        excluded_urls: List of canonical URLs to exclude from results

    Returns:
        Response dictionary with articles and issues
    """
    ...

def tldr_url(
    ...
</document>
<document path="tldr_service.py">
def _parse_date_range(
    start_date_text: str, end_date_text: str
) -> tuple[datetime, datetime]:
    """Parse ISO date strings and enforce range limits.

    >>> _parse_date_range("2024-01-01", "2024-01-02")[0].isoformat()
    '2024-01-01T00:00:00'
    """
    ...

def scrape_newsletters_in_date_range(
    start_date_text: str, end_date_text: str, source_ids: list[str] | None = None, excluded_urls: list[str] | None = None
) -> dict:
    """Scrape newsletters in date range.

    Args:
        start_date_text: Start date in ISO format
        end_date_text: End date in ISO format
        source_ids: Optional list of source IDs to scrape. Defaults to all configured sources.
        excluded_urls: List of canonical URLs to exclude from results

    Returns:
        Response dictionary with articles and issues
    """
    ...

def fetch_tldr_prompt_template() -> str:
    ...

def tldr_url_content(
    ...
</document>
<document path="util.py">
def log(msg, *args, **kwargs):
    ...

def resolve_env_var(name: str, default: str = "") -> str:
    """
    Resolve environment variable, trying both direct name and TLDR_SCRAPER_ prefixed version.
    Strips surrounding quotes from the value if present.

    >>> os.environ['TEST_VAR'] = '"value"'
    >>> resolve_env_var('TEST_VAR')
    'value'
    >>> os.environ['TEST_VAR'] = 'value'
    >>> resolve_env_var('TEST_VAR')
    'value'
    """
    ...

def get_date_range(start_date, end_date):
    """Generate list of dates between start and end (inclusive)"""
    ...

def format_date_for_url(date):
    """Format date as YYYY-MM-DD for TLDR URL"""
    ...

def canonicalize_url(url) -> str:
    """Canonicalize URL for better deduplication.

    Normalizes:
    - Removes scheme (http:// or https://)
    - Removes www. prefix
    - Removes query parameters
    - Removes URL fragments
    - Removes trailing slashes (including root)
    - Lowercases domain
    """
    ...

def get_domain_name(url) -> str:
    """Extract a friendly domain name from a URL"""
    ...
</document>
<document path="AGENTS.md">
---
last_updated: 2025-11-18 10:35, af7d9f0
---
# Agents Guide

## Project overview

Newsletter aggregator that scrapes tech newsletters from multiple sources, displays them in a unified interface, and provides AI-powered TLDRs.

- Stack:
   * Python: Flask backend, serverless on Vercel
   * React 19 + Vite (frontend) (in `client/`)
   * Supabase PostgreSQL for all data persistence
   * OpenAI GPT-5 for TLDRs
- Storage: Project uses Supabase Database (PostgreSQL) for all data persistence (newsletters, article states, settings, scrape results). Data is stored server-side with client hooks managing async operations.
- Cache mechanism: Server-side storage with cache-first scraping behavior. Daily payloads stored as JSONB in PostgreSQL. 

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed flows & user interactions documentation and [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) for a map of the project structure.

## Environment

The single source of truth for what is available locally is the output of:

```bash
env | grep -E -o '^[A-Z_]+' | grep -e TLDR -e TOKEN -e API -e KEY | sort -u  # Should print the names of all environment variables without values on a need-to-know basis.
```

**Run `source ./setup.sh` first thing to install all server and client dependencies and tooling, build the client, verify your environment and provide you with convenience functions and crucial context for the project.**

### Expected Environment Variables for AI Agents **besides Cursor Background Agents** (for Claude, Codex, etc.)

- FIRECRAWL_API_KEY
- GITHUB_API_TOKEN
- OPENAI_API_KEY
- SUPABASE_API_KEY
- SUPABASE_DATABASE_PASSWORD
- SUPABASE_SERVICE_KEY
- SUPABASE_URL

This is true both for local and production environments.

## Development & Setup

### Running the server and logs watchdog
```bash
# Verify the environment and dependencies are set up correctly.
source ./setup.sh

# Start the server and watchdog in the background. Logs output to file.
start_server_and_watchdog

# Verify the server is running.
print_server_and_watchdog_pids

# Exercise the API with curl requests.
curl http://localhost:5001/api/scrape
curl http://localhost:5001/api/tldr-url
curl ...additional endpoints that may be relevant...

# Stop the server and watchdog.
kill_server_and_watchdog
```


## Client setup

Builds client:
```bash
source setup.sh
```

### Frontend development

For frontend development with hot reload:

```bash
cd client
npm run dev
```

This runs Vite dev server on port 3000 with API proxy to localhost:5001.

#### Testing Client With Playwright

1. Use this browser configuration:
```python
launch_options = {
    'headless': True,
    'args': [
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage',
        '--disable-accelerated-2d-canvas',
        '--no-first-run',
        '--no-zygote',
        '--disable-gpu',
        '--disable-software-rasterizer',
        '--disable-web-security',
        '--disable-features=IsolateOrigins,site-per-process',
        '--disable-blink-features=AutomationControlled',
    ]
}
browser = p.chromium.launch(**launch_options)
context = browser.new_context(
    viewport={"width": 1920, "height": 1080},
    ignore_https_errors=True,
    bypass_csp=True,
)
page = context.new_page()
```

2. Take, download and view screenshots yourself to assess visuals
3. Utilize event monitoring (on "console", "request", "pageerror", ...)
4. Lean on testing real user flows
5. Wait intelligently
    - `wait_until="domcontentloaded"`, not "networkidle"
    - `page.wait_for_selector('body', state="visible")`
    - time.sleep(2~3) for React hydration
6. Leverage CSS classes as distinguishers


### `uv` installation and usage

- Install `uv`:
```bash
source setup.sh
```

Never run Python directly. Always use `uv` to run Python.
Do: `uv run python3 ...`. Do not: `python3 ...`.
Do: `uv run --with=dep1 python3 ...`. Do not: `pip install ...`.

- Use Python via `uv` for quick testing:
```bash
uv run python3 - <<'PY'
import json, sys
print("hello from uv python")
PY
```
- `uv` can transiently install dependencies if you need or consider integrating any:
```bash
uv run --with=dep1,dep2,dep3 python3 - <<'PY'
import dep1, dep2, dep3, os
dep1.do(os.environ["MY_API_KEY"])
PY
```

## Practical guidance

- Trust and Verify: Lean heavily on curling and running transient Python programs in a check-verify-trial-and-error process to make sure you know what you're doing, that you are expecting the right behavior, and to verify assumptions that any particular way of doing something is indeed the right way. This is doubly true when it comes to third-party integrations, third-party libraries, network requests, APIs, the existence and values of environment variables. 
- Run `source ./setup.sh` to verify the environment and dependencies are set up correctly. Use `source setup.sh && start_server_and_watchdog` and `source setup.sh && print_server_and_watchdog_pids` to confirm the local server is running. Generously exercise the API with `curl` requests (e.g., `/api/scrape`, `/api/tldr-url`) throughout the development process to catch regressions early. Use `source setup.sh && kill_server_and_watchdog` for cleanup.
- Verify every new behavior, fix or modification you make by utilizing your shell and Playwright. If possible, execute the modified flow to ensure nothing is broken.


## Development Conventions

1. Do not abbreviate variable, function or class names. Use complete words. Write clean code.
2. Write code that fails early and clearly rather than writing fallbacks to "maybe broken" inputs. Zero "Just in case my inputs are corrupted" code. Fallback-rich code is to be avoided because it explodes complexity and often just silently propagates bugs downstream. Good code assumes that its inputs are valid and complete. It trusts upstream code to have completed its job. This ties closely to separation of concerns. And if something important fails, or an assumption is broken, fail early and clearly. Broken code should be discovered early and loudly and fixed quickly; It should not be tolerated, nor worked around.
3. Write highly cohesive, decoupled logic.
4. Early return from functions when possible.
5. Utilize existing logic when possible. Do not re-implement anything.
6. Write flat, optimized logical branches. Avoid nested, duplicate-y code. Write DRY and elegant logic.
7. Prefer `import modulename` and call `modulename.function()` rather than `from modulename import function`. Namespacing is an easy clarity win. `import os.path; os.path.join(...)` is better than `from os.path import join(...)`.
8. Always use `util.resolve_env_var` to get environment variables.
9. Add a doctest example to pure-ish functions (data in, data out).
10. `util.log` when something is going wrong, even if it is recoverable. Be consistent with existing logging style.

<Bad: fallback-rich, squirmy code>
```py
@app.route("/api/tldr-url", methods=["POST"])
def tldr_url():
    """Requires 'url' in request body"""
    # Unnecessarily defends against broken upstream guarantees.
    data = request.get_json() or {}
    url = data.get("url", "")
    result = tldr_service.tldr_url_content(url) or ""
```
</Bad: fallback-rich, squirmy code>

<Good: straightforward, upstream-trusting code>
```py
@app.route("/api/tldr-url", methods=["POST"])
def tldr_url():
    """Requires 'url' in request body"""
    # Assumes upstream guarantees are upheld (inputs are valid and complete) — thus keeps the state machine simpler.
    # If upstream guarantees are broken (e.g., missing 'url'), we WANT to fail as early as possible (in this case, `data['url']` will throw a KeyError)
    data = request.get_json()
    url = data['url']
    result = tldr_service.tldr_url_content(url)
```
</Good: straightforward, upstream-trusting code>

<Bad: unnecessarily defensive, therefore nested code>
```py
# `MyResponse.words` is an optional list, defaulting to None (`words: list | None = None`).
response: MyResponse = requests.post(...)

# Both checks are redundant:
#  1. `response.words` is guaranteed to exist by the MyResponse model
#  2. `response.words` can be coerced to an empty iterable if it is None instead of checked.
if hasattr(response, 'words') and response.words:
    for word in response.words:
        ...  # Nested indentation level
```
</Bad: unnecessarily defensive, therefore nested code>

<Good: straightforward, confident, flatter code with fewer logical branches>
```py
response: MyResponse = requests.post(...)

# The `or []` is a safe coercion to an empty iterable if `response.words` is None. In the empty case, the loop will not run, which is the desired behavior.
for word in response.words or []:
    ...  # Single indentation level; as safe if not safer than the bad, defensive example above.
```
</Good: straightforward, confident, flatter code with fewer logical branches>

## The Right Engineering Mindset

1. Avoid increasing complexity without a truly justified reason. Each new line of code or logical branch increases complexity. Complexity is the enemy of the project. In your decision-making, ask yourself how might you **REDUCE complexity** in your solution, rather than just solve the immediate problem ad-hoc. Oftentimes, reducing complexity means **removing code**, which is OK. If done right, removing code is beneficial similarly to how clearing Tetris blocks is beneficial — it simplifies and creates more space.
2. Prefer declarative code design over imperative approaches. From a variable to an entire system, if it can be declaratively expressed upfront, do so. People understand things better when they can see the full picture instead of having to dive in. Difficulty arises when flow and logic are embedded implicitly in a sprawling implementation.
3. Avoid over-engineering and excessive abstraction. Abstractions have to be clearly justified. Simplicity and clarity are key.
4. If you're unsure whether your response is correct, that's completely fine—just let me know of your uncertainty and continue responding. We're a team.
5. Do not write comments in code, unless they are critical for understanding. Especially, do not write "journaling" comments saying "modified: foo", "added: bar" or "new implementation", as if to leave a modification trail behind.
6. For simple tasks that could be performed straight away, do not think much. Just do it. For more complex tasks that would benefit from thinking, think deeper, proportionally to the task's complexity. Regardless, always present your final response in a direct and concise manner. No fluff.
7. Do NOT fix linter errors unless instructed by the user to do so.
8. Docstrings should be few and far between. When you do write one, keep it to 1-2 sentences max.

## Crucial Important Rules: How To Approach a Task.

The following points are close to my heart:
1. Before starting your task, you must understand how big the affected scope is. Will the change affect the entire stack & flow, from the db architecture to the client logic? Map out the moving parts and coupling instances before thinking and planning.
2. If you are fixing a bug, hypothesize of the root cause before planning your changes.
3. Plan step-by-step. Account for the moving parts and coupling you found in step (1).
4. When making changes, be absolutely SURGICAL. Every line of code you add incurs a small debt; this debt compounds over time through maintenance costs, potential bugs, and cognitive load for everyone who must understand it later. Therefore, make only laser-focused changes.
4. No band-aid fixes. When encountering a problem, first brainstorm what possible root causes may explain it. band-aid fixes are bad because they increase complexity significantly. Root-cause solutions are good because they reduce complexity.


## Being an Effective AI Agent

1. Know your weaknesses: your eagerness to solve a problem can cause tunnel vision. You may fix the issue but unintentionally create code duplication, deviate from the existing design, or introduce a regression in other coupled parts of the project you didn't consider. The solution is to literally look around beyond the immediate fix, be aware of (and account for) coupling around the codebase, integrate with the existing design, and periodically refactor.
2. You do your best work when you can verify yourself. With self-verification, you can and should practice continuous trial and error instead of a single shot in the dark.

## Documentation

1. YAML frontmatter is automatically updated in CI. Do not manually update it.
2. CLAUDE.md is a read-only exact copy of AGENTS.md. It is generated automatically in CI. It is read-only for you. Any updates should be made in AGENTS.md and not CLAUDE.md.

</document>
<document path="ARCHITECTURE.md">
---
last-updated: 2025-11-14 14:33, e0594d7
last_updated: 2025-11-18 10:35, af7d9f0
---
# TLDRScraper Architecture Documentation

## Overview

TLDRScraper is a newsletter aggregator that scrapes tech newsletters from multiple sources, displays them in a unified interface, and provides AI-powered TLDRs. The architecture follows a React 19 + Vite frontend communicating with a Flask backend via REST API, with all state and cache data persisted server-side in Supabase PostgreSQL.

## Technology Stack

**Frontend:**
- React 19
- Vite (build tool)
- Marked.js (markdown parsing)
- DOMPurify (XSS sanitization)

**Backend:**
- Flask (Python web framework)
- Supabase PostgreSQL (database for all state/cache persistence)
- curl_cffi (web scraping)
- Jina Reader API (web scraping fallback)
- Firecrawl API (web scraping fallback, optional)
- MarkItDown (HTML → Markdown conversion)
- OpenAI GPT-5 (AI TLDRs)

## Architecture Diagram

```plaintext
┌─────────────────────────────────────────────────────────────────────────┐
│                             User Browser                                 │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                       React 19 Application                        │  │
│  │  ┌────────────┐  ┌──────────────┐  ┌──────────────────────────┐  │  │
│  │  │  App.jsx   │  │ Components   │  │    Hooks                 │  │  │
│  │  │            │  │              │  │                          │  │  │
│  │  │  - Root    │  │ - ScrapeForm │  │ - useArticleState        │  │  │
│  │  │  - Hydrate │  │ - CacheToggle│  │ - useSummary             │  │  │
│  │  │  - Results │  │ - Results    │  │ - useSupabaseStorage     │  │  │
│  │  │    Display │  │   Display    │  │                          │  │  │
│  │  │            │  │ - ArticleList│  │ Lib                      │  │  │
│  │  │            │  │ - ArticleCard│  │ - scraper.js             │  │  │
│  │  │            │  │              │  │ - storageApi.js          │  │  │
│  │  └────────────┘  └──────────────┘  └──────────────────────────┘  │  │
│  └───────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────┘
                                    │
                                    │ HTTP REST API
                                    ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                          Flask Backend (Python)                          │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                         serve.py (Routes)                         │  │
│  │  POST /api/scrape             POST /api/tldr-url                 │  │
│  │  GET/POST /api/storage/setting/<key>                             │  │
│  │  GET/POST /api/storage/daily/<date>                              │  │
│  │  POST /api/storage/daily-range                                   │  │
│  │  GET /api/storage/is-cached/<date>                               │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                                    │                                     │
│                                    ▼                                     │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                       tldr_app.py (App Logic)                     │  │
│  │  - scrape_newsletters()    - tldr_url()                           │  │
│  │  - get_tldr_prompt_template()                                    │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│                                    │                                     │
│                                    ▼                                     │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                    tldr_service.py (Service Layer)                │  │
│  │  - scrape_newsletters_in_date_range()                             │  │
│  │  - tldr_url_content()                                             │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│  ┌───────────────────────────────────────────────────────────────────┐  │
│  │                 storage_service.py (Storage Layer)                │  │
│  │  - get_setting() / set_setting()                                  │  │
│  │  - get_daily_payload() / set_daily_payload()                      │  │
│  │  - get_daily_payloads_range() / is_date_cached()                  │  │
│  └───────────────────────────────────────────────────────────────────┘  │
│              │                            │                              │
│              ▼                            ▼                              │
│  ┌────────────────────────┐   ┌──────────────────────────────────────┐ │
│  │  newsletter_scraper.py │   │       summarizer.py                  │ │
│  │                        │   │                                      │ │
│  │  - scrape_date_range() │   │  - tldr_url()                       │ │
│  │  - Adapter Factory     │   │  - url_to_markdown()                │ │
│  │                        │   │  - scrape_url()                     │ │
│  │  Uses:                 │   │  - _call_llm()                      │ │
│  │  - TLDRAdapter         │   │                                      │ │
│  │  - HackerNewsAdapter   │   │                                      │ │
│  └────────────────────────┘   └──────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                   Database & External Services                           │
│  ┌──────────────────────────────────────────────────────────────────┐  │
│  │  Supabase PostgreSQL Database                                    │  │
│  │  - settings table (key-value for cache:enabled, etc.)            │  │
│  │  - daily_cache table (JSONB payloads by date)                    │  │
│  └──────────────────────────────────────────────────────────────────┘  │
│  ┌──────────────┐  ┌──────────────┐  ┌────────────────────────────┐   │
│  │  TLDR News   │  │ HackerNews   │  │  OpenAI GPT-5 API          │   │
│  │  Newsletter  │  │  API         │  │  (Summaries & TLDRs)       │   │
│  │  Archives    │  │              │  │                            │   │
│  └──────────────┘  └──────────────┘  └────────────────────────────┘   │
│  ┌──────────────┐  ┌──────────────┐  ┌────────────────────────────┐   │
│  │  Jina Reader │  │  curl_cffi   │  │  Firecrawl API             │   │
│  │  r.jina.ai   │  │  (Chrome)    │  │  api.firecrawl.dev         │   │
│  └──────────────┘  └──────────────┘  └────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## Features & User Interactions

### 1. Newsletter Scraping
**User Action:** Enter start/end dates → Click "Scrape Newsletters"

**Available Interactions:**
- Select date range (max 31 days)
- Submit scrape request
- View progress bar
- View results grouped by date/issue

### 2. Cache Management
**User Action:** Toggle cache checkbox

**Available Interactions:**
- Enable/disable cache
- State persists in Supabase settings table

### 3. Article State Management
**User Action:** Click article link / Remove button / Restore button

**Available Interactions:**
- Click article title → Mark as read
- Click "Remove" → Mark as removed (visual strikethrough)
- Click "Restore" → Restore removed article
- Article states persist in Supabase daily_cache table

### 4. TLDR Generation
**User Action:** Click "TLDR" button on article

**Available Interactions:**
- Click "TLDR" → Fetch TLDR from API
- TLDR displayed inline below article
- Click again → Collapse TLDR (marks as tldrHidden; deprioritized)
- Cached TLDRs show "Available" (green)

### 5. Results Display
**User Action:** View scraped results

**Available Interactions:**
- Articles grouped by: Date → Issue/Category → Section
- Articles sorted by state: Unread → Read → TLDR-hidden → Removed
- Visual state indicators (bold = unread, muted = read, strikethrough = removed)
- Stats display (article count, unique URLs, dates processed)
- Collapsible debug logs

---

## State Machines

### Feature 1: Newsletter Scraping

#### States
1. **idle** - No scraping in progress
2. **validating** - Validating date range input
3. **checking_cache** - Checking if range is fully cached
4. **fetching_api** - Calling backend API
5. **merging_cache** - Merging API results with Supabase cache
6. **complete** - Results displayed
7. **error** - Error occurred

#### State Transitions

```
idle
  │
  ├─ User enters dates
  │    ↓
  │  validating
  │    │
  │    ├─ Valid dates
  │    │    ↓
  │    │  checking_cache
  │    │    │
  │    │    ├─ Fully cached & cache enabled
  │    │    │    ↓
  │    │    │  loading_cache (GET /api/storage/daily-range)
  │    │    │    ↓
  │    │    │  complete (load from Supabase)
  │    │    │
  │    │    └─ Not fully cached OR cache disabled
  │    │         ↓
  │    │       fetching_api
  │    │         │
  │    │         ├─ Success
  │    │         │    ↓
  │    │         │  merging_cache (if cache enabled)
  │    │         │    ↓ POST /api/storage/daily/{date}
  │    │         │  complete
  │    │         │
  │    │         └─ Failure
  │    │              ↓
  │    │            error
  │    │
  │    └─ Invalid dates
  │         ↓
  │       error (validation error)
  │
  └─ (loop back to idle on next interaction)
```

#### Key State Data
- **startDate**: string (ISO date)
- **endDate**: string (ISO date)
- **loading**: boolean
- **progress**: number (0-100)
- **error**: string | null
- **results**: ResultsPayload | null

---

### Feature 2: Cache Management

#### States
1. **enabled** - Cache is active
2. **disabled** - Cache is inactive

#### State Transitions

```
enabled
  │
  ├─ User toggles OFF
  │    ↓
  │  disabled
  │    │
  │    └─ POST /api/storage/setting/cache:enabled {value: false}
  │         ↓ Supabase upsert to settings table
  │
  └─ User toggles ON
       ↓
     enabled
       │
       └─ POST /api/storage/setting/cache:enabled {value: true}
            ↓ Supabase upsert to settings table
```

#### Key State Data
- **enabled**: boolean (reactive, synced to Supabase settings table)
- **loading**: boolean (during database read/write)
- **statusText**: computed string ("(enabled)" | "(disabled)")

---

### Feature 3: Article State Management

#### States (per article)
1. **unread** - Default state, bold text
2. **read** - User clicked/viewed, muted text
3. **removed** - User removed, strikethrough + dashed border

#### State Transitions

```
unread
  │
  ├─ User clicks article link
  │    ↓
  │  read
  │    │
  │    ├─ article.read = { isRead: true, markedAt: timestamp }
  │    │
  │    └─ POST /api/storage/daily/{date} → Supabase upsert
  │
  ├─ User clicks "Remove"
  │    ↓
  │  removed
  │    │
  │    ├─ article.removed = true
  │    │
  │    └─ POST /api/storage/daily/{date} → Supabase upsert
  │
read
  │
  └─ User clicks "Remove"
       ↓
     removed
       │
       ├─ article.removed = true
       │
       └─ POST /api/storage/daily/{date} → Supabase upsert

removed
  │
  └─ User clicks "Restore"
       ↓
     unread (or previous state)
       │
       ├─ article.removed = false
       │
       └─ POST /api/storage/daily/{date} → Supabase upsert
```

#### Key State Data (per article)
- **url**: string (unique identifier)
- **issueDate**: string (storage key component)
- **read**: { isRead: boolean, markedAt: string | null }
- **removed**: boolean

---

### Feature 4: TLDR Generation

#### States (per article TLDR)
1. **unknown** - TLDR not yet requested
2. **creating** - API request in progress
3. **available** - TLDR cached and ready
4. **error** - API request failed

#### State Transitions

```
unknown
  │
  └─ User clicks "TLDR"
       ↓
     creating
       │
       ├─ POST /api/tldr-url { url, summary_effort }
       │
       ├─ Success
       │    ↓
       │  available
       │    │
       │    ├─ tldr.status = 'available'
       │    ├─ tldr.markdown = response.tldr_markdown
       │    ├─ tldr.expanded = true
       │    ├─ Mark article as read
       │    │
       │    └─ POST /api/storage/daily/{date} → Supabase upsert
       │
       └─ Failure
            ↓
          error
            │
            ├─ tldr.status = 'error'
            ├─ tldr.errorMessage = error text
            │
            └─ POST /api/storage/daily/{date} → Supabase upsert

available
  │
  └─ User clicks "Available"
       ↓
     (toggle expanded state, no API call)
```

#### Key State Data (per article)
- **tldr.status**: 'unknown' | 'creating' | 'available' | 'error'
- **tldr.markdown**: string
- **tldr.html**: computed (marked + DOMPurify)
- **tldr.effort**: 'minimal' | 'low' | 'medium' | 'high'
- **tldr.expanded**: boolean (UI state)
- **tldr.errorMessage**: string | null

---

## Call Graphs

### Feature 1: Newsletter Scraping - Complete Flow

#### Client → Backend → External Services

```
User clicks "Scrape Newsletters"
  │
  ├─ ScrapeForm.jsx handleSubmit()
  │    │
  │    ├─ Check validation
  │    │    │
  │    │    └─ If invalid: return early
  │    │
  │    └─ Call scraper.scrape(startDate, endDate)
  │
  └─ scraper.js scrape(startDate, endDate)
       │
       ├─ Reset state:
       │    - loading.value = true
       │    - progress.value = 0
       │    - error.value = null
       │
       ├─ Step 1: Check cache
       │    │
  │    └─ scraper.js isRangeCached(startDate, endDate)
  │         │
  │         ├─ Compute date range: computeDateRange()
  │         │    │
  │         │    └─ Returns: ['2024-01-03', '2024-01-02', '2024-01-01']
  │         │
  │         └─ Check each date in Supabase:
  │              │
  │              └─ GET /api/storage/is-cached/2024-01-01
  │                   │
  │                   ├─ If ALL dates cached AND cacheEnabled = true
  │                   │    │
  │                   │    └─ scraper.js loadFromCache()
  │                   │         │
  │                   │         ├─ POST /api/storage/daily-range
  │                   │         ├─ Build stats: buildStatsFromPayloads()
  │                   │         ├─ Update progress state
  │                   │         │
  │                   │         └─ Return cached results
       │                   │
       │                   └─ If NOT fully cached OR cache disabled
       │                        │
       │                        └─ Continue to API call...
       │
       ├─ Step 2: API Call
       │    │
       │    ├─ progress.value = 50
       │    │
       │    └─ window.fetch('/api/scrape', {
       │         method: 'POST',
       │         body: JSON.stringify({ start_date, end_date })
       │       })
       │         │
       │         └─ Server receives request...
       │              │
       │              ├─ serve.py:32 scrape_newsletters_in_date_range()
       │              │    │
       │              │    ├─ Extract request.get_json()
       │              │    │    - start_date: "2024-01-01"
       │              │    │    - end_date: "2024-01-03"
       │              │    │    - sources: null (optional)
       │              │    │
       │              │    └─ tldr_app.py:9 scrape_newsletters(start_date, end_date, source_ids, excluded_urls=[])
       │              │         │
       │              │         └─ tldr_service.py:43 scrape_newsletters_in_date_range()
       │              │              │
       │              │              ├─ tldr_service.py:17 _parse_date_range()
       │              │              │    │
       │              │              │    ├─ Parse ISO dates
       │              │              │    ├─ Validate: start <= end
       │              │              │    ├─ Validate: range < 31 days
       │              │              │    │
       │              │              │    └─ Return (datetime, datetime)
       │              │              │
       │              │              └─ newsletter_scraper.py:251 scrape_date_range(start_date, end_date, source_ids, excluded_urls)
       │              │                   │
       │              │                   ├─ util.get_date_range(start, end)
       │              │                   │    │
       │              │                   │    └─ Returns list of dates: [date1, date2, date3]
       │              │                   │
       │              │                   ├─ Default sources: NEWSLETTER_CONFIGS.keys()
       │              │                   │    - ['tldr_tech', 'tldr_ai', 'hackernews', ...]
       │              │                   │
       │              │                   ├─ Initialize tracking:
       │              │                   │    - all_articles = []
       │              │                   │    - url_set = set()
       │              │                   │    - issue_metadata_by_key = {}
       │              │                   │
       │              │                   └─ For each date in dates:
       │              │                        │
       │              │                        └─ For each source_id in source_ids:
       │              │                             │
       │              │                             ├─ newsletter_scraper.py:172 _collect_newsletters_for_date_from_source()
       │              │                             │    │
       │              │                             │    ├─ newsletter_scraper.py:16 _get_adapter_for_source(config)
       │              │                             │    │    │
       │              │                             │    │    ├─ If source_id.startswith('tldr_'):
       │              │                             │    │    │    │
       │              │                             │    │    │    └─ Return TLDRAdapter(config)
       │              │                             │    │    │
       │              │                             │    │    └─ If source_id == 'hackernews':
       │              │                             │    │         │
       │              │                             │    │         └─ Return HackerNewsAdapter(config)
       │              │                             │    │
       │              │                             │    └─ adapter.scrape_date(date, excluded_urls)
       │              │                             │         │
       │              │                             │         ├─ TLDRAdapter: Scrapes tldr.tech archives
       │              │                             │         │    │
       │              │                             │         │    ├─ Build URL: f"https://tldr.tech/{newsletter_type}/archives/{date}"
       │              │                             │         │    ├─ HTTP GET request
       │              │                             │         │    ├─ Parse HTML for articles
       │              │                             │         │    ├─ Extract metadata from titles: "(N minute read)" or "(GitHub Repo)" → article_meta field
       │              │                             │         │    ├─ Filter out excluded URLs
       │              │                             │         │    │
       │              │                             │         │    └─ Return { articles: [...], issues: [...] }
       │              │                             │         │
       │              │                             │         └─ HackerNewsAdapter: Scrapes HN API (Algolia)
       │              │                             │              │
       │              │                             │              ├─ Fetch 50 stories from Algolia (pre-filtered by date/score)
       │              │                             │              ├─ Filter out excluded URLs (canonical matching)
       │              │                             │              ├─ Calculate leading scores: (2 × upvotes) + comments
       │              │                             │              ├─ Sort by leading score descending
       │              │                             │              ├─ Convert top stories to articles
       │              │                             │              ├─ Extract metadata: "N upvotes, K comments" → article_meta field
       │              │                             │              │
       │              │                             │              └─ Return { articles: [...], issues: [] }
       │              │                             │
       │              │                             ├─ For each article in result:
       │              │                             │    │
       │              │                             │    ├─ Canonicalize URL
       │              │                             │    ├─ Deduplicate via url_set
       │              │                             │    │
       │              │                             │    └─ Append to all_articles
       │              │                             │
       │              │                             └─ Sleep 0.2s (rate limiting)
       │              │
       │              ├─ newsletter_scraper.py:139 _build_scrape_response()
       │              │    │
       │              │    ├─ Group articles by date
       │              │    ├─ Build markdown output (newsletter_merger.py)
       │              │    ├─ Build issues list
       │              │    ├─ Compute stats
       │              │    │
       │              │    └─ Return {
       │              │         success: true,
       │              │         articles: [...],
       │              │         issues: [...],
       │              │         stats: { total_articles, unique_urls, ... }
       │              │       }
       │              │
       │              └─ Flask jsonify() → HTTP Response
       │
       ├─ Step 3: Process Response
       │    │
  │    └─ scraper.js buildDailyPayloadsFromScrape(data)
  │         │
  │         ├─ Group articles by date
  │         ├─ Group issues by date
  │         │
  │         └─ Build daily payloads: [{
  │              date: "2024-01-01",
  │              articles: [...],
  │              issues: [...],
  │              cachedAt: timestamp
  │            }]
  │
  ├─ Step 4: Merge with Cache (if enabled)
  │    │
  │    └─ scraper.js mergeWithCache(payloads)
  │         │
  │         └─ For each payload:
  │              │
  │              ├─ GET /api/storage/daily/{date}
  │              │    │
  │              │    ├─ If cached data exists:
  │              │    │    │
  │              │    │    └─ Merge articles (preserve summary, tldr, read, removed)
  │              │    │
  │              │    └─ POST /api/storage/daily/{date} (save to Supabase)
  │              │
  │              └─ Return merged payload
  │
  ├─ Step 5: Update State
  │    │
  │    ├─ Update progress state
  │    ├─ Set results state: { success, payloads, source, stats }
  │    │
  │    └─ Return results
  │
  └─ Step 6: Display Results
       │
       └─ ScrapeForm.jsx passes results via callback
            │
            └─ App.jsx handleResults(data)
                 │
                 ├─ Update results state
                 │
                 └─ ResultsDisplay.jsx renders:
                      │
                      ├─ Stats
                      ├─ Debug logs
                      │
                      └─ ArticleList (grouped by date/issue)
                           │
                           └─ ArticleCard (for each article)
```

---

### Feature 4: TLDR Generation - Complete Flow

```
User clicks "TLDR" button
  │
  ├─ ArticleCard.jsx onClick={handleTldrClick}
  │    │
  │    └─ useSummary hook toggle()
  │         │
  │         ├─ Check if TLDR already available
  │         │
  │         └─ useSummary.js fetch(summaryEffort)
  │                   │
  │                   └─ window.fetch('/api/tldr-url', {
  │                        method: 'POST',
  │                        body: JSON.stringify({ url, summary_effort })
  │                      })
  │                        │
  │                        └─ Server receives request...
  │                             │
  │                             ├─ serve.py:68 tldr_url()
  │                             │    │
  │                             │    └─ tldr_app.py:32 tldr_url(url, summary_effort)
  │                             │         │
  │                             │         └─ tldr_service.py:79 tldr_url_content(url, summary_effort)
  │                             │              │
  │                             │              ├─ util.canonicalize_url(url)
  │                             │              │
  │                             │              └─ summarizer.py:291 tldr_url(url, summary_effort)
  │                             │                   │
  │                             │                   ├─ url_to_markdown(url)
  │                             │                   │    [Same flow as summarize]
  │                             │                   │
  │                             │                   ├─ Fetch TLDR prompt template:
  │                             │                   │    │
  │                             │                   │    └─ _fetch_tldr_prompt()
  │                             │                   │         │
  │                             │                   │         └─ Fetch from GitHub:
  │                             │                   │              "https://api.github.com/repos/giladbarnea/llm-templates/contents/text/tldr.md"
  │                             │                   │
  │                             │                   ├─ Build prompt:
  │                             │                   │    template + "\n\n<tldr this>\n" + markdown + "\n</tldr this>"
  │                             │                   │
  │                             │                   └─ Call LLM:
  │                             │                        │
  │                             │                        └─ _call_llm(prompt, summary_effort)
  │                             │                             [Same flow as summarize]
  │                             │
  │                             └─ Return { success, tldr_markdown, canonical_url, summary_effort }
  │
  └─ Client receives response:
       │
       ├─ Update article state:
       │    {
       │      status: 'available',
       │      markdown: result.tldr_markdown,
       │      effort: summaryEffort,
       │      checkedAt: timestamp,
       │      errorMessage: null
       │    }
       │
       ├─ Set expanded state to true
       ├─ Mark article as read (if not already)
       │
       └─ Display inline TLDR
```

---

## Data Structures

### DailyPayload (Supabase: `daily_cache` table, keyed by date)

```typescript
{
  date: string,              // "2024-01-01"
  cachedAt: string,          // ISO timestamp
  articles: Article[],       // Array of articles for this date
  issues: Issue[]            // Array of newsletter issues for this date
}
```

### Article

```typescript
{
  url: string,               // Canonical URL (unique identifier)
  title: string,
  articleMeta: string,       // Metadata extracted from source (e.g., "158 upvotes, 57 comments" or "5 minute read")
  issueDate: string,         // "2024-01-01"
  category: string,          // "TLDR Tech", "HackerNews", etc.
  sourceId: string,          // "tldr_tech", "hackernews"
  section: string | null,    // Section title within newsletter
  sectionEmoji: string | null,
  sectionOrder: number | null,
  newsletterType: string | null,

  // User state
  removed: boolean,
  tldrHidden: boolean,
  read: {
    isRead: boolean,
    markedAt: string | null  // ISO timestamp
  },

  // AI-generated content
  summary: {
    status: 'unknown' | 'creating' | 'available' | 'error',
    markdown: string,
    effort: 'minimal' | 'low' | 'medium' | 'high',
    checkedAt: string | null,
    errorMessage: string | null
  },

  tldr: {
    status: 'unknown' | 'creating' | 'available' | 'error',
    markdown: string,
    effort: 'minimal' | 'low' | 'medium' | 'high',
    checkedAt: string | null,
    errorMessage: string | null
  }
}
```

### Issue

```typescript
{
  date: string,              // "2024-01-01"
  source_id: string,         // "tldr_tech"
  category: string,          // "TLDR Tech"
  title: string | null,      // Issue title
  subtitle: string | null    // Issue subtitle
}
```

### ScrapeRequest (POST /api/scrape)

```typescript
{
  start_date: string,        // "2024-01-01"
  end_date: string,          // "2024-01-03"
  sources?: string[]         // ["tldr_tech", "hackernews"] (optional)
}
```

### ScrapeResponse (API response)

```typescript
{
  success: boolean,
  articles: Article[],       // All articles (flattened)
  issues: Issue[],           // All issues
  stats: {
    total_articles: number,
    unique_urls: number,
    dates_processed: number,
    dates_with_content: number,
    network_fetches: number,
    cache_mode: string,
    debug_logs: string[]
  },
  output: string             // Markdown formatted output
}
```

---

## Component Dependency Graph

```
App.jsx
  │
  ├── CacheToggle.jsx
  │     └── useSupabaseStorage('cache:enabled')
  │           └── GET/POST /api/storage/setting/cache:enabled
  │
  ├── ScrapeForm.jsx
  │     └── scraper.js functions
  │           └── storageApi.js (GET/POST /api/storage/daily/*)
  │
  └── ResultsDisplay.jsx
        │
        └── DailyResults (per date)
              │
              ├── useSupabaseStorage('newsletters:scrapes:{date}')
              │     └── GET/POST /api/storage/daily/{date}
              │
              └── ArticleList.jsx
                    │
                    └── ArticleCard.jsx
                          ├── useArticleState(date, url)
                          │     └── useSupabaseStorage('newsletters:scrapes:{date}')
                          │           └── GET/POST /api/storage/daily/{date}
                          │
                          └── useSummary(date, url)
                                └── useArticleState(date, url)
```

---

## Sequence Diagram: Full Scraping Flow

```mermaid
sequenceDiagram
    participant User
    participant ScrapeForm
    participant useScraper
    participant Supabase
    participant Flask
    participant NewsletterScraper
    participant TLDRAdapter
    participant ExternalAPI

    User->>ScrapeForm: Enter dates & click "Scrape"
    ScrapeForm->>useScraper: scrape(startDate, endDate)

    alt Cache enabled & fully cached
        useScraper->>Supabase: GET /api/storage/is-cached/{date} (for each date)
        Supabase-->>useScraper: All dates cached
        useScraper->>Supabase: POST /api/storage/daily-range
        Supabase-->>useScraper: Return cached payloads
        useScraper-->>ScrapeForm: Return cached results
    else Not fully cached
        useScraper->>Flask: POST /api/scrape {start_date, end_date}
        Flask->>NewsletterScraper: scrape_date_range()

        loop For each date
            loop For each source
                NewsletterScraper->>TLDRAdapter: scrape_date(date)
                TLDRAdapter->>ExternalAPI: GET tldr.tech/archives/{date}
                ExternalAPI-->>TLDRAdapter: HTML content
                TLDRAdapter-->>NewsletterScraper: {articles, issues}
            end
        end

        NewsletterScraper->>NewsletterScraper: Build response & dedupe
        NewsletterScraper-->>Flask: {articles, issues, stats}
        Flask-->>useScraper: JSON response

        useScraper->>useScraper: buildDailyPayloadsFromScrape()

        alt Cache enabled
            useScraper->>Supabase: GET /api/storage/daily/{date} (merge)
            useScraper->>Supabase: POST /api/storage/daily/{date} (save)
        end

        useScraper-->>ScrapeForm: Return results
    end

    ScrapeForm->>User: Display articles
```

---

## Key Algorithms

### 1. Article Sorting Algorithm (ArticleList.jsx)

```javascript
// Sort articles by state (unread → read → tldrHidden → removed), then by original order
function sortArticles(articles) {
  return articles.sort((a, b) => {
    const stateA = getArticleState(a)  // 0=unread, 1=read, 2=tldrHidden, 3=removed
    const stateB = getArticleState(b)

    // Primary sort: by state
    if (stateA !== stateB) return stateA - stateB

    // Secondary sort: preserve original order within same state
    return (a.originalOrder ?? 0) - (b.originalOrder ?? 0)
  })
}
```

### 2. Date Range Computation (scraper.js)

```javascript
// Compute all dates between start and end (inclusive, descending)
function computeDateRange(startDate, endDate) {
  const dates = []
  const start = new Date(startDate)
  const end = new Date(endDate)

  const current = new Date(end)
  while (current >= start) {
    dates.push(current.toISOString().split('T')[0])
    current.setDate(current.getDate() - 1)
  }

  return dates  // ['2024-01-03', '2024-01-02', '2024-01-01']
}
```

### 3. Cache Merge Algorithm (scraper.js)

```javascript
// Merge new scrape results with existing cached data from Supabase
async function mergeWithCache(payloads) {
  const merged = []

  for (const payload of payloads) {
    const existing = await storageApi.getDailyPayload(payload.date)

    if (existing) {
      // Merge: preserve user state (read, removed, tldrHidden) and AI content (tldr)
      const mergedPayload = {
        ...payload,
        articles: payload.articles.map(article => {
          const existingArticle = existing.articles?.find(a => a.url === article.url)
          return existingArticle
            ? { ...article, tldr: existingArticle.tldr,
                read: existingArticle.read, removed: existingArticle.removed, tldrHidden: existingArticle.tldrHidden }
            : article
        })
      }
      await storageApi.setDailyPayload(payload.date, mergedPayload)
      merged.push(mergedPayload)
    } else {
      await storageApi.setDailyPayload(payload.date, payload)
      merged.push(payload)
    }
  }

  return merged
}
```

### 4. URL Deduplication (newsletter_scraper.py:172)

```python
# Deduplicate articles across sources using canonical URLs
url_set = set()
all_articles = []

for article in scraped_articles:
    canonical_url = util.canonicalize_url(article['url'])
    article['url'] = canonical_url

    if canonical_url not in url_set:
        url_set.add(canonical_url)
        all_articles.append(article)
```

---

## Database Schema (Supabase PostgreSQL)

### Table: settings

```sql
CREATE TABLE settings (
  key TEXT PRIMARY KEY,
  value JSONB NOT NULL,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Example row:
{ key: 'cache:enabled', value: true, updated_at: '2024-01-01T12:00:00Z' }
```

### Table: daily_cache

```sql
CREATE TABLE daily_cache (
  date DATE PRIMARY KEY,
  payload JSONB NOT NULL,
  cached_at TIMESTAMPTZ DEFAULT NOW()
);

-- Example row:
{
  date: '2024-01-01',
  payload: {
    date: '2024-01-01',
    cachedAt: '2024-01-01T12:00:00Z',
    articles: [{url, title, read, removed, tldr, ...}, ...],
    issues: [{date, source_id, category, ...}, ...]
  },
  cached_at: '2024-01-01T12:00:00Z'
}
```

### Storage Flow

1. **Initial Scrape**: API response → Build payloads → POST /api/storage/daily/{date} → Supabase upsert
2. **Cache Hit**: GET /api/storage/daily-range → Read from Supabase → Skip scrape API call
3. **User Interaction**: Modify article state → POST /api/storage/daily/{date} → Supabase upsert → Dispatches 'supabase-storage-change' event
4. **Summary/TLDR**: Fetch from API → Update article → POST /api/storage/daily/{date} → Supabase upsert

---

## Error Handling

### Frontend Errors

1. **Validation Errors**
   - Date range > 31 days → Show inline error
   - Start date > end date → Show inline error

2. **Network Errors**
   - API unreachable → Show error message
   - Timeout → Show error message

3. **Summary/TLDR Errors**
   - Scraping failed → summary.status = 'error'
   - LLM API failed → summary.errorMessage = "..."
   - Button shows "Retry" instead of "Available"

### Backend Errors

1. **Scraping Errors**
   - Individual source failures → Log warning, continue with other sources
   - All sources fail → Return partial results

2. **Summary/TLDR Errors**
   - Try multiple scraping methods (curl_cffi → Jina Reader → Firecrawl)
   - Return 502 on network errors
   - Return 500 on LLM errors

---

## Performance Considerations

1. **Caching Strategy**
   - Cache at daily granularity (not per-source)
   - Merge strategy preserves user state and AI content
   - Cache check before every API call

2. **Rate Limiting**
   - 0.2s delay between source scrapes
   - Prevents overwhelming external APIs

3. **Lazy Loading**
   - Summaries/TLDRs fetched on-demand
   - Not included in initial scrape
   - Cached after first fetch

4. **Component Optimization**
   - Scoped CSS modules prevent style leakage
   - useMemo caches derived state
   - Conditional rendering optimizes DOM updates

---

## Security Measures

1. **XSS Prevention**
   - DOMPurify sanitizes all markdown → HTML conversions
   - dangerouslySetInnerHTML only used with sanitized content

2. **CSRF Protection**
   - Same-origin policy (frontend served from same domain)

3. **Input Validation**
   - Date range validation (client + server)
   - URL canonicalization (prevents cache poisoning)

4. **API Key Management**
   - OpenAI API key server-side only
   - GitHub token for private repos (optional)
   - Firecrawl API key for hard-to-scrape sites (optional)

---

## Testing Considerations

### Unit Tests (Frontend)

- `scraper.js`: Date range computation, cache hit/miss logic
- `useArticleState.js`: State mutations (read/unread/removed)
- `useSummary.js`: Toggle expansion, fetch logic

### Integration Tests

- Full scraping flow (API → cache → display)
- Summary generation end-to-end
- Cache merge behavior

### E2E Tests

- User scrapes date range → Views results
- User marks article as read → State persists
- User generates summary → Summary displays and caches

---

## Future Enhancements

1. **Reasoning Effort Selector**
   - Dropdown on summary button (minimal/low/medium/high)
   - Currently hardcoded to "low"

2. **Source Filtering**
   - UI to select which newsletters to scrape
   - Backend already supports `sources` parameter

3. **Export Functionality**
   - Export articles as markdown/JSON
   - Bulk copy summaries

4. **Search & Filter**
   - Full-text search across articles
   - Filter by category/source/state

---

## File Structure

```
TLDRScraper/
├── client/                    # React 19 frontend
│   ├── src/
│   │   ├── App.jsx           # Root component
│   │   ├── main.jsx          # Entry point
│   │   ├── components/       # UI components
│   │   │   ├── ArticleCard.jsx
│   │   │   ├── ArticleList.jsx
│   │   │   ├── CacheToggle.jsx
│   │   │   ├── ResultsDisplay.jsx
│   │   │   └── ScrapeForm.jsx
│   │   ├── hooks/            # Custom React hooks
│   │   │   ├── useArticleState.js
│   │   │   ├── useSupabaseStorage.js
│   │   │   └── useSummary.js
│   │   └── lib/              # Utilities & logic
│   │       ├── scraper.js
│   │       ├── storageApi.js
│   │       └── storageKeys.js
│   ├── index.html
│   ├── vite.config.js
│   └── package.json
│
├── api/                       # Backend entry point
│   └── index.py
│
├── serve.py                   # Flask routes
├── tldr_app.py               # Application logic layer
├── tldr_service.py           # Service layer
├── storage_service.py        # Supabase storage operations
├── supabase_client.py        # Supabase client initialization
├── newsletter_scraper.py     # Scraping orchestration
├── summarizer.py             # URL → Summary/TLDR
├── newsletter_adapter.py     # Base adapter
├── tldr_adapter.py           # TLDR newsletter adapter
├── hackernews_adapter.py     # HackerNews adapter
├── newsletter_merger.py      # Markdown formatting
├── newsletter_config.py      # Source configurations
└── util.py                   # Shared utilities
```

---

## Conclusion

TLDRScraper is a full-stack newsletter aggregator with sophisticated client-side state management, intelligent caching, and AI-powered content summarization. The architecture separates concerns clearly:

- **React hooks** handle reactive state and async storage operations
- **Flask routes** provide clean REST endpoints (scraping + storage)
- **Service/adapter layers** abstract data sources and database operations
- **Supabase PostgreSQL** provides server-side persistence with JSONB storage
- **OpenAI integration** enhances content with AI-powered summaries

The system is designed for extensibility (new newsletter sources via adapters), performance (database caching with cache-first scraping), and user experience (reactive UI with async loading states).

</document>
<document path="BUGS.md">
---
last_updated: 2025-11-18 10:35, af7d9f0
---
# Bugs Encountered

## Third Party

- [ ] Supabase error: `SSL: CERTIFICATE_VERIFY_FAILED`. See [this GitHub comment](https://github.com/supabase/supabase/discussions/29935#discussioncomment-12050763) for potential solution.
## Scraping

### Failed fetching content
- [ ] https://olmocr.allen.ai/blog. Content was returned with no error but empty-ish. Needs JS enabled (25-10-26 7:45AM IST)
- [ ] https://www.gatesnotes.com/home/home-page-topic/reader/three-tough-truths-about-climate Empty content
- [ ] https://x.com/satyanadella/status/1989755076353921404: Enable JavaScript or use a supported browser; disable privacy extensions; then try again. (25-11-17)
</document>
<document path="GOTCHAS.md">
---
last_updated: 2025-11-18 10:35, af7d9f0
---
# Gotchas

This document catalogs recurring pitfalls in various topics, including managing client-side state persistence and reactivity, surprising design decisions, and so on.

---

#### 2025-11-17: Child component bypassing state management layer causes infinite API hammering

session-id: 892fa714-0087-4c5a-9930-cffdfc5f5359

**Desired behavior that didn't work**: Just browsing the app should load data once per date, then remain quiet until user interaction.

**What actually happened and falsified original thesis**: Continuous machine-gun API requests to GET /api/storage/daily/{date} for the same dates, hammering the server non-stop. <turned-out-to-be-wrong>We initially assumed it was a React re-render loop from unstable useEffect dependencies (e.g., `defaultValue` object creating new references). Then we thought it was multiple `useSupabaseStorage` hook instances all mounting simultaneously (20+ ArticleCards per date). We added global read deduplication to prevent concurrent requests, which helped but didn't stop the hammering. We had wrongly assumed the hooks were the problem.</turned-out-to-be-wrong>

**Cause & Fix**: `ArticleList.jsx` had a useEffect that directly called `storageApi.getDailyPayload(article.issueDate)` for every article (20+ API calls per date), completely bypassing the `useSupabaseStorage` hook abstraction. This useEffect ran whenever the `articles` prop changed. The parent component recreated the `articles` array on every render (via `.map()`), creating a new reference and triggering the useEffect again. Result: infinite loop of 20+ API calls per render. The entire useEffect was redundant - article states were already being synced via `useSupabaseStorage` in parent components. The fix was to delete the broken useEffect entirely and let ArticleList simply sort the articles it receives from props. **Key lesson**: When you build a proper data management layer (custom hooks), don't bypass it by directly calling storage APIs in child components. Child components should consume data from props, not fetch it themselves. Breaking this rule creates duplicate data fetching, races, and infinite loops.

---

#### 2025-11-15 `750f83e`: Concurrent TLDR updates race to overwrite each other

**Desired behavior that didn't work**: When two articles' TLDR buttons are clicked simultaneously, both TLDRs should be fetched and stored independently.

**What actually happened and falsified original thesis**: One article showed "Available" state but with no content, then clicking "Available" triggered a new TLDR request instead of displaying the cached result. We had wrongly assumed React's state would handle concurrent setValueAsync calls correctly.

**Cause & Fix**: Classic read-modify-write race condition. Both updates captured the same stale `value` from the closure, so the second write overwrote the first, losing one article's TLDR data. The fix was to use a ref (valueRef) to track the latest state, ensuring each concurrent update operates on current data instead of stale closure captures.

---

#### 2025-11-06: useLocalStorage hook instances race to overwrite each other

**Desired behavior that didn't work**: Removed articles should persist their removed state after page refresh.

**What actually happened and falsified original thesis**: Article showed "Restore" button immediately after removal, but after refresh showed "Remove" button. We had wrongly assumed one useLocalStorage instance per key would prevent conflicts.

**Cause & Fix**: Multiple useLocalStorage hook instances (one per ArticleCard) each owned their own copy of the payload. When one instance stored an update, other instances later wrote their stale copy back, erasing the change. Rewrote useLocalStorage to use useSyncExternalStore so every subscriber reads and writes through a single source of truth, dramatically simplifying the flow and eliminating the race.

---

#### 2025-11-04 `102a8dcd`: HackerNews articles not displayed in UI because of surprising server response shape

**Desired behavior that didn't work**: HackerNews articles fetched by backend should appear in the UI.

**What actually happened and falsified original thesis**: HackerNews articles were fetched (183 articles in API response) but invisible in the UI. We had wrongly assumed `articles` field alone was sufficient for display.

**Cause & Fix**: The frontend requires both `articles` and `issues` arrays. It only displays articles that match an issue's category. HackerNews adapter returned empty `issues` array, so all HN articles were filtered out during rendering. The fix was to generate fake issue objects for each HackerNews category.

---

#### 2025-10-31 `3bfceee`: State property lost during cache merge

**Desired behavior that didn't work**: When hiding a TLDR, the article should move to bottom so users can deprioritize completed items.

**What actually happened and falsified original thesis**: The article stayed in place. We had wrongly assumed that saving the state property to storage was sufficient.

**Cause & Fix**: The merge function wasn't transferring the new property from cached data. The fix was to add the missing property to the merge operation.

---

#### 2025-10-31 `16bd653`: Component not reactive to storage changes

**Desired behavior that didn't work**: When state changes in storage, the list should re-sort so visual order reflects current state.

**What actually happened and falsified original thesis**: The list used stale prop values. We had wrongly assumed that components automatically react to storage mutations.

**Cause & Fix**: Computed properties only track their declared dependencies. The fix was to dispatch custom events on storage writes and listen for them in consuming components.

---

</document>
<document path="PROJECT_STRUCTURE.md">
.
├── .claude
│  ├── agents
│  │  ├── codebase-analyzer-narrow.md
│  │  ├── codebase-analyzer.md
│  │  ├── codebase-locator.md
│  │  ├── codebase-pattern-finder.md
│  │  └── web-deep-researcher.md
│  ├── commands
│  │  ├── architecture
│  │  │  ├── create.md
│  │  │  ├── sync_current_changes.md
│  │  │  └── sync_since_last_updated.md
│  │  ├── create_plan_lite.md
│  │  ├── implement_plan.md
│  │  ├── research_codebase.md
│  │  └── research_codebase_nt.md
│  └── settings.json
├── .gitattributes
├── .githooks
│  ├── post-checkout
│  ├── post-merge
│  ├── post-rewrite
│  ├── pre-merge-commit
│  ├── pre-rebase
│  ├── README.md
│  └── sync-upstream-suggestions.md
├── .github
│  └── workflows
│     ├── copy-agents-to-claude.yml
│     ├── update-doc-frontmatter.yml
│     └── update-project-structure.yml
├── .gitignore
├── .vercelignore
├── AGENTS.md
├── api
│  └── index.py
├── ARCHITECTURE.md
├── BUGS.md
├── CLAUDE.md
├── client
│  ├── .gitignore
│  ├── index.html
│  ├── package-lock.json
│  ├── package.json
│  ├── README.md
│  ├── src
│  │  ├── App.css
│  │  ├── App.jsx
│  │  ├── components
│  │  │  ├── ArticleCard.css
│  │  │  ├── ArticleCard.jsx
│  │  │  ├── ArticleList.css
│  │  │  ├── ArticleList.jsx
│  │  │  ├── CacheToggle.css
│  │  │  ├── CacheToggle.jsx
│  │  │  ├── ResultsDisplay.css
│  │  │  ├── ResultsDisplay.jsx
│  │  │  ├── ScrapeForm.css
│  │  │  └── ScrapeForm.jsx
│  │  ├── hooks
│  │  │  ├── useArticleState.js
│  │  │  ├── useSummary.js
│  │  │  └── useSupabaseStorage.js
│  │  ├── lib
│  │  │  ├── scraper.js
│  │  │  ├── storageApi.js
│  │  │  └── storageKeys.js
│  │  └── main.jsx
│  └── vite.config.js
├── GOTCHAS.md
├── hackernews_adapter.py
├── newsletter_adapter.py
├── newsletter_config.py
├── newsletter_merger.py
├── newsletter_scraper.py
├── package-lock.json
├── package.json
├── pyproject.toml
├── README.md
├── requirements.txt
├── SCREENSHOTTING_APP.md
├── scripts
│  ├── markdown_frontmatter.py
│  ├── print_root_markdown_files.sh
│  ├── resolve_quiet_setting.sh
│  └── update_doc_frontmatter.py
├── serve.py
├── setup-hooks.sh
├── setup.sh
├── storage_service.py
├── summarizer.py
├── supabase_client.py
├── tests
│  ├── browser-automation
│  │  ├── test_phase6_supabase.py
│  │  └── test_tldr_loading_state_bug.py
│  ├── test_phase6_e2e.py
│  └── unit
│     └── test_canonicalize_url.py
├── thoughts
│  ├── 2025-11-08-migrate-client-localstorage-to-server-supabase
│  │  ├── implementation
│  │  │  ├── phase-1.md
│  │  │  ├── phase-2.md
│  │  │  ├── phase-3.md
│  │  │  ├── phase-4.md
│  │  │  ├── phase-5.md
│  │  │  ├── phase-6.md
│  │  │  └── phase-7.md
│  │  ├── manual-browser-testing.md
│  │  ├── plans
│  │  │  └── localstorage-to-supabase-migration.md
│  │  └── research
│  │     └── supabase-database.md
│  └── done
│     ├── 25-10-27-hackernews-integration
│     │  ├── plan.md
│     │  └── research.md
│     ├── 25-10-28-fix-cache-ui-state-sync
│     │  └── plan.md
│     ├── 25-10-30-multi-newsletter
│     │  └── plan.md
│     ├── 25-10-31-vue-to-react-19-migration
│     │  └── plan.md
│     ├── 25-11-04-code-duplication
│     │  ├── plan-issue-a-section-parsing.md
│     │  ├── plan-issue-b-localstorage-keys.md
│     │  ├── plan-issue-c-article-normalization.md
│     │  └── plan.md
│     ├── 25-11-04-mixed-concerns-refactor
│     │  ├── plan-issue-b-extract-build-scrape-response.md
│     │  ├── plan-issue-c-eliminate-duplicate-parsing.md
│     │  └── plan.md
│     └── 25-11-04-remove-summarize
│        └── plan.md
├── tldr_adapter.py
├── tldr_app.py
├── tldr_service.py
├── TLDRScraper.code-workspace
├── util.py
├── uv.lock
└── vercel.json

</document>
<document path="README.md">
---
last_updated: 2025-11-14 16:24, 722a1a0
---
# TLDRScraper

Newsletter aggregator that scrapes tech newsletters from multiple sources, displays them in a unified interface, and provides AI-powered TLDRs.

## Architecture

- **Frontend**: React 19 + Vite (in `client/`)
- **Backend**: Flask + Python (serverless on Vercel)
- **AI**: OpenAI GPT-5 for TLDRs

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed flows & user interactions documentation and [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) for a map of the project structure.

## Development

## Development & Setup

### Running the server and logs watchdog
```bash
# Verify the environment and dependencies are set up correctly.
source ./setup.sh

# Start the server and watchdog in the background. Logs output to file.
start_server_and_watchdog

# Verify the server is running.
print_server_and_watchdog_pids

# Exercise the API with curl requests.
curl http://localhost:5001/api/scrape
curl http://localhost:5001/api/tldr-url
curl ...additional endpoints that may be relevant...

# Stop the server and watchdog.
kill_server_and_watchdog
```


## Client setup

```bash
cd client
npm install
npm run build
npm run dev
```

### Frontend development

For frontend development with hot reload:

```bash
cd client
npm run dev
```

This runs Vite dev server on port 3000 with API proxy to localhost:5000.


### `uv` installation and usage

- Install `uv` and use Python via `uv`:
```bash
source setup.sh
ensure_uv
uv --version
```

## Vercel Deployment

### How It Works

The application is deployed to Vercel as a Python serverless function with a built React frontend:

1. **Build Phase** (`buildCommand` in `vercel.json`):
   - `cd client && npm install && npm run build`
   - Builds React app

2. **Install Phase** (automatic):
   - Vercel auto-detects `requirements.txt`
   - Installs Python dependencies for the serverless function

3. **Runtime**:
   - `/api/index.py` imports the Flask app from `serve.py`
   - All routes (`/`, `/api/*`) are handled by the Python serverless function
   - Flask serves the built React app from `client/static/dist/`
   - API endpoints process requests

### Key Configuration Files

#### `vercel.json`
```json
{
  "buildCommand": "cd client && npm install && npm run build",
  "outputDirectory": "static/dist",
  "rewrites": [
    { "source": "/(.*)", "destination": "/api/index" }
  ]
}
```

- **buildCommand**: Builds the React frontend
- **outputDirectory**: Points to where React builds output (matches `client/vite.config.js` outDir)
- **rewrites**: Routes all requests to the Python serverless function

#### `api/index.py`
```python
import sys
import os

# Add parent directory to path so we can import serve.py and other modules
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from serve import app as app
```

This is the Vercel serverless function entry point. The path manipulation is required because Vercel's Python runtime doesn't automatically add the parent directory to `sys.path`.

#### `serve.py`
```python
# Configure Flask to serve React build output
app = Flask(
    __name__,
    static_folder='static/dist/assets',
    static_url_path='/assets'
)

@app.route("/")
def index():
    """Serve the React app"""
    static_dist = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'static', 'dist')
    return send_from_directory(static_dist, 'index.html')
```

Flask is configured to:
- Serve static assets from `static/dist/assets` at `/assets/*`
- Serve the React app's `index.html` at the root `/`
- Handle API routes at `/api/*`

### Deployment Requirements

1. **React build output** must be in `client/static/dist/` (configured in `client/vite.config.js`)
2. **Python dependencies** are managed by `uv` and must manually be added to `requirements.txt` (Vercel auto-installs)
3. **Module imports** in `api/index.py` must handle parent directory path
4. **Flask static configuration** must point to built React assets

### Common Vercel Deployment Issues

**Issue**: `pip: command not found`
- **Cause**: Explicit `installCommand` in vercel.json trying to run pip in Node.js context
- **Solution**: Remove `installCommand` - Vercel auto-installs from requirements.txt

**Issue**: `No Output Directory named "public" found`
- **Cause**: Vercel looking for default output directory
- **Solution**: Add `"outputDirectory": "static/dist"` to vercel.json

**Issue**: `404 for /api/index`
- **Cause**: Python module import failing in serverless function
- **Solution**: Add parent directory to sys.path in api/index.py

## Documentation

- [ARCHITECTURE.md](ARCHITECTURE.md) - Detailed flows & user interactions documentation
- [PROJECT_STRUCTURE.md](PROJECT_STRUCTURE.md) - Map of the project structure
- [GOTCHAS.md](GOTCHAS.md) - Documented solved tricky past bugs
- [BUGS.md](BUGS.md) - Known issues
</document>
<document path="SCREENSHOTTING_APP.md">
---
last_updated: 2025-11-18 10:35, af7d9f0
---
# How to Get Screenshots of the App from Remote

**Setup:** User has ngrok endpoint at `https://josue-ungreedy-unphysically.ngrok-free.dev/`

## Steps

1. **Verify Playwright is installed locally:**
```bash
curl -X POST https://josue-ungreedy-unphysically.ngrok-free.dev/ -d "ls ~/Library/Caches/ms-playwright"
```

2. **Create and run Playwright script via heredoc:**
```bash
curl -X POST https://josue-ungreedy-unphysically.ngrok-free.dev/ -d "cat > /tmp/screenshot.js << 'EOF'
const {chromium} = require('playwright');
(async () => {
  const browser = await chromium.launch({headless: true});
  const page = await browser.newPage();
  await page.setViewport({width: 1920, height: 1080});
  await page.goto('http://localhost:3000', {waitUntil: 'domcontentloaded', timeout: 30000});
  await page.waitForSelector('body');
  await new Promise(r => setTimeout(r, 4000));
  await page.screenshot({path: '/tmp/tldr_local.png', fullPage: true});
  await browser.close();
  console.log('Screenshot saved');
})();
EOF
cd ~/dev/TLDRScraper && node /tmp/screenshot.js && ls -lh /tmp/tldr_local.png" -s
```

3. **Transfer screenshot via Git:**
```bash
# Create temporary branch
curl -X POST https://josue-ungreedy-unphysically.ngrok-free.dev/ -d "cd ~/dev/TLDRScraper && git checkout -b screenshots-$(date +%s)" -s

# Copy, commit, and push
curl -X POST https://josue-ungreedy-unphysically.ngrok-free.dev/ -d "cd ~/dev/TLDRScraper && cp /tmp/tldr_local.png ./screenshot.png && git commit -am 'Add screenshot' && git push --set-upstream origin HEAD 2>&1 | grep -E 'screenshots-|branch'" -s
```

Note the branch name from output (e.g., `screenshots-1763449916`)

4. **Download screenshot via GitHub raw URL:**
```bash
curl -s "https://raw.githubusercontent.com/giladbarnea/TLDRScraper/screenshots-1763449916/screenshot.png" -o /tmp/tldr_screenshot.png
file /tmp/tldr_screenshot.png  # Verify it's a valid PNG
```

5. **Display in conversation:** Use Read tool on `/tmp/tldr_screenshot.png`

## Cleanup

**Remote machine:**
```bash
# Remove temp files
curl -X POST https://josue-ungreedy-unphysically.ngrok-free.dev/ -d "rm /tmp/tldr_local.png /tmp/screenshot.js" -s

# Delete local git branch
curl -X POST https://josue-ungreedy-unphysically.ngrok-free.dev/ -d "cd ~/dev/TLDRScraper && git checkout - && git branch -D screenshots-1763449916" -s
```

**Remote GitHub:**
```bash
# Delete remote branch (requires valid GITHUB_API_TOKEN)
curl -X DELETE "https://api.github.com/repos/giladbarnea/TLDRScraper/git/refs/heads/screenshots-1763449916" \
  -H "Authorization: token ${GITHUB_API_TOKEN}" -s
```

**Local machine:**
```bash
rm /tmp/tldr_screenshot.png /tmp/screenshot_script.js
```

## Notes

* **Heredoc vs inline:** Heredoc approach (`cat > file << 'EOF'`) works better than inline escaped JavaScript - avoids quote escaping hell
* **File upload services:** `file.io` and `transfer.sh` timeout with ngrok (ERR_NGROK_3004) - likely a bug in the remoteshell server handling long-running commands
* **Git method:** Most reliable for this setup despite extra steps

</document>
<document path="client/README.md">
---
last_updated: 2025-11-14 16:24, 722a1a0
---
# Vue 3 + Vite

This template should help get you started developing with Vue 3 in Vite. The template uses Vue 3 `<script setup>` SFCs, check out the [script setup docs](https://v3.vuejs.org/api/sfc-script-setup.html#sfc-script-setup) to learn more.

Learn more about IDE Support for Vue in the [Vue Docs Scaling up Guide](https://vuejs.org/guide/scaling-up/tooling.html#ide-support).

</document>
<document path="client/index.html">
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="color-scheme" content="light" />
    <title>Newsletter Aggregator</title>

    <!-- Vollkorn font for reading surface -->
    <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Vollkorn:wght@400;700&display=swap" rel="stylesheet">
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>

</document>
<document path="client/src/App.css">
/* CSS Variables */
:root {
  --bg: #f6f7f9;
  --surface: #ffffff;
  --text: #0f172a;
  --muted: #475569;
  --border: #e5e7eb;
  --link: #1a73e8;
  --radius: 10px;
  --shadow-sm: 0 1px 2px rgba(0,0,0,0.06), 0 1px 3px rgba(0,0,0,0.10);

  /* Whitey reading surface colors */
  --whitey-text: #333;
  --whitey-link: #2484c1;
  --whitey-divider: #2f2f2f;
  --whitey-rule: #ddd;
  --whitey-s-1: 0.5rem;
  --whitey-s-2: 1rem;
  --whitey-s-3: 1.5rem;
  --whitey-s-4: 2.5rem;
}

/* Global resets and base styles */
* {
  box-sizing: border-box;
}

body {
  font-family: system-ui, -apple-system, Segoe UI, Roboto, Noto Sans, Ubuntu, Cantarell, "Helvetica Neue", Arial, sans-serif;
  max-width: 1000px;
  margin: 0 auto;
  padding: 20px;
  background-color: var(--bg);
  color: var(--text);
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  text-rendering: optimizeLegibility;
  -webkit-text-size-adjust: 100%;
  letter-spacing: 0.003em;
  line-height: 1.6;
}

/* Container styles */
.container {
  background: var(--surface);
  padding: 20px;
  border-radius: var(--radius);
  box-shadow: var(--shadow-sm);
  border: 1px solid var(--border);
}

h1 {
  color: var(--text);
  text-align: center;
  margin-bottom: 30px;
  font-weight: 700;
  letter-spacing: -0.01em;
  line-height: 1.25;
  text-wrap: balance;
}

/* Focus styles */
a:focus-visible,
button:focus-visible,
input:focus-visible {
  outline: 3px solid rgba(26,115,232,0.45);
  outline-offset: 2px;
}

/* Whitey reading surface styles (for #write) */
#write {
  font-size: 19px;
  max-width: 960px;
  margin: 0 auto 2em;
  padding-top: 40px;
  padding-left: 1rem;
  padding-right: 1rem;
  color: var(--whitey-text);
  font-family: "Vollkorn", Palatino, Times, serif;
  line-height: 1.53;
  text-align: left;
  background: transparent;
}

@media only screen and (min-width: 1400px) {
  #write {
    max-width: 1100px;
  }
}

#write h1,
#write h2,
#write h3 {
  text-align: center;
  font-weight: normal;
  color: var(--whitey-text);
}

#write h1 {
  margin-top: 1.6em;
  font-size: 3em;
}

#write h2 {
  margin-top: 2em;
}

#write h3 {
  margin-top: 3em;
  font-style: italic;
}

#write h1 + h2,
#write h2 + h3 {
  margin-top: 0.83em;
}

/* Mobile responsive */
@media (max-width: 42em) {
  #write {
    font-size: clamp(16px, 0.95rem + 0.6vw, 18px);
    line-height: 1.6;
    padding-left: 0;
    padding-right: 0;
  }

  #write h1 {
    font-size: clamp(2rem, 7vw, 2.6rem);
    line-height: 1.15;
    margin: var(--whitey-s-4) 0 var(--whitey-s-2);
  }

  #write h2 {
    font-size: clamp(1.5rem, 5.5vw, 2rem);
    line-height: 1.25;
    margin: var(--whitey-s-4) 0 var(--whitey-s-1);
  }

  #write h3 {
    font-size: clamp(1.25rem, 4.5vw, 1.6rem);
    line-height: 1.3;
    margin: var(--whitey-s-3) 0 var(--whitey-s-1);
  }
}

@media (max-width: 480px) {
  body {
    padding: 8px;
  }
  .container {
    padding: 12px;
  }
}

</document>
<document path="client/src/App.jsx">
import { useState, useEffect } from 'react'
import CacheToggle from './components/CacheToggle'
import ScrapeForm from './components/ScrapeForm'
import ResultsDisplay from './components/ResultsDisplay'
import { loadFromCache } from './lib/scraper'
import './App.css'

function App() {
  const [results, setResults] = useState(null)

  useEffect(() => {
    const today = new Date()
    const threeDaysAgo = new Date(today)
    threeDaysAgo.setDate(today.getDate() - 3)

    const endDate = today.toISOString().split('T')[0]
    const startDate = threeDaysAgo.toISOString().split('T')[0]

    loadFromCache(startDate, endDate)
      .then(cached => {
        if (cached) {
          setResults(cached)
        }
      })
      .catch(err => {
        console.error('Failed to load cached results:', err)
      })
  }, [])

  return (
    <div className="container">
      <h1>Newsletter Aggregator</h1>

      <CacheToggle />

      <ScrapeForm onResults={setResults} />

      {results && <ResultsDisplay results={results} />}
    </div>
  )
}

export default App

</document>
<document path="client/src/components/ArticleCard.css">
.article-card {
  display: flex;
  flex-direction: column;
  background: var(--surface, #ffffff);
  border: 1px solid var(--border, #e5e7eb);
  border-radius: 8px;
  padding: 14px 16px;
  transition: border-color 0.15s ease, box-shadow 0.15s ease, transform 0.15s ease, opacity 0.15s ease;
  position: relative;
  animation: cardFadeIn 0.3s ease backwards;
}

@keyframes cardFadeIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.article-card:hover {
  border-color: var(--link, #1a73e8);
  box-shadow: 0 2px 8px rgba(26, 115, 232, 0.12);
  transform: translateY(-1px);
}

.article-card.unread .article-link {
  font-weight: 600;
}

.article-card.read .article-link {
  font-weight: normal;
  color: var(--muted, #475569);
}

.article-card.tldr-hidden {
  opacity: 0.6;
  background: rgba(156, 163, 175, 0.04);
}

.article-card.tldr-hidden .article-link {
  font-weight: normal;
  color: var(--muted, #475569);
}

.article-card.removed {
  opacity: 0.75;
  border-style: dashed;
  border-color: #d1d5db;
  background: #f3f4f6;
  cursor: pointer;
}

.article-card.removed .article-link {
  text-decoration: line-through;
  color: var(--muted, #475569);
  pointer-events: none;
}

.article-card.removed .article-actions {
  opacity: 1;
}

.article-card.removed .article-actions .article-btn:not(.remove-article-btn) {
  display: none;
}

.article-header {
  display: flex;
  align-items: center;
  gap: 12px;
  width: 100%;
}

.article-number {
  flex-shrink: 0;
  width: 28px;
  height: 28px;
  display: flex;
  align-items: center;
  justify-content: center;
  background: var(--bg, #f6f7f9);
  border-radius: 6px;
  font-weight: 600;
  font-size: 14px;
  color: var(--muted, #475569);
}

.article-content {
  flex: 1;
  min-width: 0;
}

.article-link {
  font-size: 16px;
  line-height: 1.5;
  color: var(--link, #1a73e8);
  text-decoration: none;
  word-wrap: break-word;
  display: block;
  width: 100%;
  cursor: pointer;
}

.article-link:hover {
  text-decoration: underline;
}

.article-link.loading {
  opacity: 0.6;
  cursor: wait;
}

.article-favicon {
  width: 1em;
  height: 1em;
  display: inline-block;
  vertical-align: -0.125em;
  object-fit: contain;
  margin-right: 1ch;
}

.article-actions {
  flex-shrink: 0;
  display: flex;
  align-items: center;
  gap: 6px;
  opacity: 0;
  transition: opacity 0.2s ease;
}

.article-card:hover .article-actions {
  opacity: 1;
}

.article-btn {
  height: 32px;
  display: flex;
  align-items: center;
  justify-content: center;
  background: transparent;
  border: 1px solid var(--border, #e5e7eb);
  border-radius: 6px;
  color: var(--muted, #475569);
  cursor: pointer;
  transition: all 0.15s ease;
  font-size: 13px;
  padding: 0;
  margin: 0;
}

.article-btn:hover {
  background: var(--bg, #f6f7f9);
  border-color: var(--muted, #475569);
  color: var(--text, #0f172a);
}

.article-btn:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.tldr-btn {
  width: auto;
  min-width: 5em;
  padding: 0 10px;
  font-weight: 500;
  white-space: nowrap;
}

.tldr-btn.loaded {
  background: #22c55e;
  color: white;
  border-color: #22c55e;
}

.tldr-btn.loaded:hover {
  background: #16a34a;
  border-color: #16a34a;
}

.tldr-btn.loaded.expanded {
  background: var(--bg, #f6f7f9);
  color: var(--text, #0f172a);
  border-color: var(--border, #e5e7eb);
}

.remove-article-btn {
  width: auto;
  min-width: 6.2em;
  padding: 0 10px;
  font-weight: 500;
  white-space: nowrap;
}

.remove-article-btn:hover {
  background: rgba(220, 53, 69, 0.12);
  border-color: rgba(220, 53, 69, 0.4);
  color: #b91c1c;
}

.article-card.removed .remove-article-btn {
  display: none;
}

.inline-tldr {
  margin-top: 9px;
  padding-top: 9px;
  padding-bottom: 0;
  border-top: 1px dashed var(--border, #e5e7eb);
  font-size: 0.96em;
  line-height: 1.675;
  animation: slideDown 0.2s ease;
}

@keyframes slideDown {
  from {
    opacity: 0;
    transform: translateY(-8px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

.inline-tldr > strong {
  display: block;
  margin-bottom: 0.5em;
  color: var(--text, #0f172a);
}

.inline-tldr ul {
  padding-left: 1.2em;  /* Just enough room for bullets */
  margin: 0.5em 0;
}

@media (max-width: 768px) {
  .article-actions {
    opacity: 1;
    width: 100%;
    margin-left: 0;
    margin-top: 8px;
    justify-content: flex-start;
  }

  .article-header {
    flex-wrap: wrap;
  }

}

</document>
<document path="client/src/components/ArticleCard.jsx">
import { useMemo } from 'react'
import { useArticleState } from '../hooks/useArticleState'
import { useSummary } from '../hooks/useSummary'
import './ArticleCard.css'

function ArticleCard({ article, index }) {
  const { isRead, isRemoved, isTldrHidden, toggleRead, toggleRemove, markTldrHidden, unmarkTldrHidden, loading: stateLoading } = useArticleState(
    article.issueDate,
    article.url
  )

  const tldr = useSummary(article.issueDate, article.url, 'tldr')

  const cardClasses = [
    'article-card',
    !isRead && 'unread',
    isRead && 'read',
    isRemoved && 'removed',
    isTldrHidden && 'tldr-hidden'
  ].filter(Boolean).join(' ')

  const fullUrl = useMemo(() => {
    const url = article.url
    if (url.startsWith('http://') || url.startsWith('https://')) {
      return url
    }
    return `https://${url}`
  }, [article.url])

  const faviconUrl = useMemo(() => {
    try {
      const url = new URL(fullUrl)
      return `${url.origin}/favicon.ico`
    } catch {
      return null
    }
  }, [fullUrl])

  const handleLinkClick = (e) => {
    if (isRemoved) return
    if (e.ctrlKey || e.metaKey) return

    if (!isRead) {
      toggleRead()
    }
  }

  const handleTldrClick = () => {
    if (isRemoved) return

    const wasExpanded = tldr.expanded
    tldr.toggle()

    if (!isRead && tldr.expanded) {
      toggleRead()
    }

    if (wasExpanded && !tldr.expanded) {
      markTldrHidden()
    } else if (tldr.expanded) {
      unmarkTldrHidden()
    }
  }

  const handleRemoveClick = () => {
    if (!isRemoved && tldr.expanded) {
      tldr.collapse()
    }
    toggleRemove()
  }

  return (
    <div className={cardClasses} data-original-order={index} onClick={isRemoved ? handleRemoveClick : undefined}>
      <div className="article-header">
        <div className="article-number">{index + 1}</div>

        <div className="article-content">
          <a
            href={fullUrl}
            className={`article-link ${stateLoading ? 'loading' : ''}`}
            target="_blank"
            rel="noopener noreferrer"
            data-url={fullUrl}
            tabIndex={isRemoved ? -1 : 0}
            onClick={handleLinkClick}
          >
            {faviconUrl && (
              <img
                src={faviconUrl}
                className="article-favicon"
                loading="lazy"
                alt=""
                onError={(e) => e.target.style.display = 'none'}
              />
            )}
            <span className="article-link-text">
              {article.title}{!isRemoved && article.articleMeta ? ` (${article.articleMeta})` : ''}
            </span>
          </a>
        </div>

        <div className="article-actions">
          <button
            className={`article-btn tldr-btn ${tldr.isAvailable ? 'loaded' : ''} ${tldr.expanded ? 'expanded' : ''}`}
            disabled={stateLoading || tldr.loading}
            type="button"
            title={tldr.isAvailable ? 'TLDR cached - click to show' : 'Show TLDR'}
            onClick={handleTldrClick}
          >
            {tldr.buttonLabel}
          </button>

          <button
            className="article-btn remove-article-btn"
            type="button"
            title={isRemoved ? 'Restore this article to the list' : 'Remove this article from the list'}
            disabled={stateLoading}
            onClick={handleRemoveClick}
          >
            {isRemoved ? 'Restore' : 'Remove'}
          </button>
        </div>
      </div>

      {tldr.expanded && tldr.html && (
        <div className="inline-tldr">
          <strong>TLDR</strong>
          <div dangerouslySetInnerHTML={{ __html: tldr.html }} />
        </div>
      )}
    </div>
  )
}

export default ArticleCard

</document>
<document path="client/src/components/ArticleList.css">
.article-list {
  display: flex;
  flex-direction: column;
  gap: 12px;
  margin: 20px 0;
  padding: 0;
  list-style: none;
}

.section-title {
  margin: 18px 0 6px;
  text-align: center;
  font-style: italic;
  color: #9ca3af;
  font-size: 0.92em;
  letter-spacing: 0.01em;
}

.article-card.category-first {
  border-top: 3px solid var(--link, #1a73e8);
  padding-top: 16px;
  margin-top: 12px;
}

</document>
<document path="client/src/components/ArticleList.jsx">
import { useMemo } from 'react'
import ArticleCard from './ArticleCard'
import './ArticleList.css'

function ArticleList({ articles }) {
  const sortedArticles = useMemo(() => {
    return [...articles].sort((a, b) => {
      const stateA = a.removed ? 3
        : a.tldrHidden ? 2
        : a.read?.isRead ? 1
        : 0
      const stateB = b.removed ? 3
        : b.tldrHidden ? 2
        : b.read?.isRead ? 1
        : 0

      if (stateA !== stateB) return stateA - stateB

      return (a.originalOrder ?? 0) - (b.originalOrder ?? 0)
    })
  }, [articles])

  const sectionsWithArticles = useMemo(() => {
    const sections = []
    let currentSection = null

    sortedArticles.forEach((article, index) => {
      const sectionTitle = article.section
      const sectionEmoji = article.sectionEmoji
      const sectionKey = sectionTitle ? `${sectionEmoji || ''} ${sectionTitle}`.trim() : null

      if (sectionKey && sectionKey !== currentSection) {
        sections.push({
          type: 'section',
          key: sectionKey,
          label: sectionKey
        })
        currentSection = sectionKey
      } else if (!sectionTitle && currentSection !== null) {
        currentSection = null
      }

      sections.push({
        type: 'article',
        key: article.url,
        article,
        index
      })
    })

    return sections
  }, [sortedArticles])

  return (
    <div className="article-list">
      {sectionsWithArticles.map((item) => (
        item.type === 'section' ? (
          <div key={item.key} className="section-title">
            {item.label}
          </div>
        ) : (
          <ArticleCard
            key={item.key}
            article={item.article}
            index={item.index}
          />
        )
      ))}
    </div>
  )
}

export default ArticleList

</document>
<document path="client/src/components/CacheToggle.css">
.cache-toggle-container {
  display: flex;
  align-items: center;
  justify-content: center;
  padding: 12px;
  margin-bottom: 20px;
  background: var(--bg);
  border-radius: 8px;
  border: 1px solid var(--border);
}

.cache-toggle-label {
  display: flex;
  align-items: center;
  gap: 10px;
  cursor: pointer;
  font-size: 15px;
  color: var(--text);
  user-select: none;
}

.cache-toggle-checkbox {
  position: relative;
  width: 48px;
  height: 26px;
  background: #cbd5e1;
  border-radius: 13px;
  transition: background-color 0.2s ease;
  cursor: pointer;
  border: 2px solid transparent;
}

.cache-toggle-checkbox::after {
  content: '';
  position: absolute;
  top: 2px;
  left: 2px;
  width: 18px;
  height: 18px;
  background: white;
  border-radius: 50%;
  transition: transform 0.2s ease;
  box-shadow: 0 2px 4px rgba(0,0,0,0.2);
}

.cache-toggle-input {
  position: absolute;
  opacity: 0;
  width: 0;
  height: 0;
}

.cache-toggle-input:checked + .cache-toggle-checkbox {
  background: #22c55e;
}

.cache-toggle-input:checked + .cache-toggle-checkbox::after {
  transform: translateX(22px);
}

.cache-toggle-input:focus-visible + .cache-toggle-checkbox {
  outline: 3px solid rgba(26,115,232,0.45);
  outline-offset: 2px;
}

.cache-toggle-text {
  font-weight: 500;
}

.cache-toggle-status {
  font-size: 13px;
  color: var(--muted);
  margin-left: 4px;
}

</document>
<document path="client/src/components/CacheToggle.jsx">
import { useSupabaseStorage } from '../hooks/useSupabaseStorage'
import './CacheToggle.css'

function CacheToggle() {
  const [enabled, setEnabled, , { loading }] = useSupabaseStorage('cache:enabled', true)

  return (
    <div className="cache-toggle-container" data-testid="cache-toggle-container">
      <label className="cache-toggle-label" htmlFor="cacheToggle">
        <input
          id="cacheToggle"
          type="checkbox"
          className="cache-toggle-input"
          data-testid="cache-toggle-input"
          aria-label="Enable cache"
          checked={enabled}
          disabled={loading}
          onChange={(e) => setEnabled(e.target.checked)}
        />
        <span className="cache-toggle-checkbox" data-testid="cache-toggle-switch" />
        <span className="cache-toggle-text">Cache</span>
        <span className="cache-toggle-status" data-testid="cache-toggle-status">
          {enabled ? '(enabled)' : '(disabled)'}
        </span>
      </label>
    </div>
  )
}

export default CacheToggle

</document>
<document path="client/src/components/ResultsDisplay.css">
.result {
  margin-top: 20px;
  padding: 0;
  background-color: transparent;
  border-radius: 0;
  display: block;
  border: none;
}

.stats {
  background-color: #fff3cd;
  border: 1px solid #ffeaa7;
  color: #856404;
  margin-bottom: 15px;
  padding: 10px;
  border-radius: 4px;
}

.logs-slot {
  margin-bottom: 15px;
}

.logs-slot details {
  background: #f6f7f9;
  padding: 10px;
  border-radius: 4px;
  border: 1px solid #e5e7eb;
}

.logs-slot summary {
  cursor: pointer;
  font-weight: 600;
  color: #475569;
  user-select: none;
}

.logs-slot pre {
  margin-top: 10px;
  font-size: 13px;
  line-height: 1.5;
  white-space: pre-wrap;
  word-wrap: break-word;
  color: #0f172a;
}

.date-header-container {
  display: flex;
  align-items: center;
  justify-content: space-between;
  width: 100%;
  margin: 0;
}

.date-header-container h2 {
  text-align: center;
  width: 100%;
  font-weight: normal;
  color: var(--text, #0f172a);
  margin-top: 2em;
}

.date-header-container h2::after {
  border-bottom: 1px solid #2f2f2f;
  content: '';
  width: 100px;
  display: block;
  margin: 0.4em auto 0;
  height: 1px;
}

.issue-header-container h4 {
  text-align: left;
  margin: 1.5em 0 0.5em;
  font-style: normal;
  color: var(--text, #0f172a);
}

.issue-title-block {
  margin: 12px 0 18px;
  text-align: center;
  font-style: italic;
  color: #9ca3af;
  font-size: 0.95em;
  line-height: 1.6;
}

.issue-title-block .issue-title-line + .issue-title-line {
  margin-top: 4px;
}

.copy-toast {
  position: fixed;
  left: 50%;
  bottom: 32px;
  transform: translateX(-50%) translateY(20px);
  background: rgba(255, 255, 255, 0.95);
  backdrop-filter: blur(10px);
  color: var(--text, #0f172a);
  padding: 10px 18px;
  border-radius: 999px;
  font-size: 14px;
  font-weight: 500;
  opacity: 0;
  pointer-events: none;
  transition: opacity 0.25s ease, transform 0.25s ease;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08), 0 2px 4px rgba(0, 0, 0, 0.06);
  border: 1px solid rgba(0, 0, 0, 0.06);
  z-index: 999;
}

.copy-toast.show {
  opacity: 1;
  transform: translateX(-50%) translateY(0);
}

</document>
<document path="client/src/components/ResultsDisplay.jsx">
import { useSupabaseStorage } from '../hooks/useSupabaseStorage'
import { getNewsletterScrapeKey } from '../lib/storageKeys'
import ArticleList from './ArticleList'
import './ResultsDisplay.css'

function ResultsDisplay({ results }) {
  const statsLines = [
    `📊 Stats: ${results.stats.total_articles} articles, ${results.stats.unique_urls} unique URLs`,
    `📅 Dates: ${results.stats.dates_with_content}/${results.stats.dates_processed} with content`,
    results.source && `Source: ${results.source}`
  ].filter(Boolean)

  const debugLogs = results.debugLogs || []

  return (
    <div id="result" className="result success">
      <div className="stats">
        {statsLines.map((line, index) => (
          <div key={index}>{line}</div>
        ))}
      </div>

      {debugLogs.length > 0 && (
        <div id="logs-slot" className="logs-slot">
          <details>
            <summary>Debug logs</summary>
            <pre>{debugLogs.join('\n')}</pre>
          </details>
        </div>
      )}

      <main id="write">
        {(results.payloads || []).map((payload) => (
          <DailyResults
            key={payload.date}
            payload={payload}
          />
        ))}
      </main>
    </div>
  )
}

function DailyResults({ payload }) {
  const [livePayload, , , { loading }] = useSupabaseStorage(
    getNewsletterScrapeKey(payload.date),
    payload
  )

  const date = livePayload?.date ?? payload.date
  const articles = (livePayload?.articles ?? payload.articles).map((article, index) => ({
    ...article,
    originalOrder: index
  }))
  const issues = livePayload?.issues ?? payload.issues ?? []

  return (
    <div className="date-group">
      <div className="date-header-container" data-date={date}>
        <h2>{date}</h2>
        {loading && <span className="loading-indicator"> (loading...)</span>}
      </div>

      {issues.map((issue) => (
        <div
          key={`${date}-${issue.category}`}
          className="issue-section"
        >
          <div className="issue-header-container">
            <h4>{issue.category}</h4>
          </div>

          {(issue.title || issue.subtitle) && (
            <div className="issue-title-block">
              {issue.title && (
                <div className="issue-title-line">{issue.title}</div>
              )}
              {issue.subtitle && issue.subtitle !== issue.title && (
                <div className="issue-title-line">{issue.subtitle}</div>
              )}
            </div>
          )}

          <ArticleList
            articles={articles.filter((article) => article.category === issue.category)}
          />
        </div>
      ))}

      {articles.some((article) => !article.category) && (
        <ArticleList
          articles={articles.filter((article) => !article.category)}
        />
      )}
    </div>
  )
}

export default ResultsDisplay

</document>
<document path="client/src/components/ScrapeForm.css">
.form-group {
  margin-bottom: 20px;
  display: inline-block;
  width: 45%;
  margin-right: 5%;
}

.form-group:last-child {
  margin-right: 0;
}

label {
  display: block;
  margin-bottom: 5px;
  font-weight: bold;
  color: var(--muted);
}

input[type="date"] {
  width: 100%;
  padding: 10px 12px;
  border: 1px solid var(--border);
  border-radius: 8px;
  font-size: 16px;
  box-sizing: border-box;
  background: var(--surface);
  color: var(--text);
  min-height: 44px;
}

button {
  background-color: #007bff;
  color: white;
  padding: 12px 24px;
  border: none;
  border-radius: 8px;
  cursor: pointer;
  font-size: 16px;
  width: 100%;
  margin-top: 20px;
  min-height: 44px;
}

button:hover {
  background-color: #0056b3;
}

button:disabled {
  background-color: #ccc;
  cursor: not-allowed;
}

.progress {
  display: block;
  margin: 20px 0;
  padding: 15px;
  background-color: #e3f2fd;
  border-radius: 4px;
  border-left: 4px solid #2196f3;
}

.progress-bar {
  width: 100%;
  height: 20px;
  background-color: #ddd;
  border-radius: 10px;
  overflow: hidden;
  margin-top: 10px;
}

.progress-fill {
  height: 100%;
  background-color: #2196f3;
  transition: width 0.3s ease;
}

.error {
  background-color: #f8d7da;
  border: 1px solid #f5c6cb;
  color: #721c24;
  padding: 10px 12px;
  border-radius: 6px;
  margin-top: 10px;
}

@media (max-width: 480px) {
  .form-group {
    width: 100%;
    margin-right: 0;
  }
}

</document>
<document path="client/src/components/ScrapeForm.jsx">
import { useActionState, useState, useEffect } from 'react'
import { scrapeNewsletters } from '../lib/scraper'
import { useSupabaseStorage } from '../hooks/useSupabaseStorage'
import './ScrapeForm.css'

function ScrapeForm({ onResults }) {
  const [startDate, setStartDate] = useState('')
  const [endDate, setEndDate] = useState('')
  const [cacheEnabled] = useSupabaseStorage('cache:enabled', true)
  const [progress, setProgress] = useState(0)

  useEffect(() => {
    const today = new Date()
    const threeDaysAgo = new Date(today)
    threeDaysAgo.setDate(today.getDate() - 3)
    setEndDate(today.toISOString().split('T')[0])
    setStartDate(threeDaysAgo.toISOString().split('T')[0])
  }, [])

  const [state, formAction, isPending] = useActionState(
    async (previousState, formData) => {
      const start = formData.get('start_date')
      const end = formData.get('end_date')

      const startDateObj = new Date(start)
      const endDateObj = new Date(end)
      const daysDiff = Math.ceil((endDateObj - startDateObj) / (1000 * 60 * 60 * 24))

      if (startDateObj > endDateObj) {
        return { error: 'Start date must be before or equal to end date.' }
      }
      if (daysDiff >= 31) {
        return { error: 'Date range cannot exceed 31 days. Please select a smaller range.' }
      }

      setProgress(50)

      try {
        const results = await scrapeNewsletters(start, end, cacheEnabled)
        setProgress(100)
        onResults(results)
        return { success: true }
      } catch (err) {
        setProgress(0)
        return { error: err.message || 'Network error' }
      }
    },
    { success: false }
  )

  const validationError = (() => {
    if (!startDate || !endDate) return null

    const start = new Date(startDate)
    const end = new Date(endDate)
    const daysDiff = Math.ceil((end - start) / (1000 * 60 * 60 * 24))

    if (start > end) {
      return 'Start date must be before or equal to end date.'
    }
    if (daysDiff >= 31) {
      return 'Date range cannot exceed 31 days. Please select a smaller range.'
    }
    return null
  })()

  return (
    <div>
      <form id="scrapeForm" action={formAction}>
        <div className="form-group">
          <label htmlFor="start_date">Start Date:</label>
          <input
            id="start_date"
            name="start_date"
            type="date"
            value={startDate}
            onChange={(e) => setStartDate(e.target.value)}
            required
          />
        </div>

        <div className="form-group">
          <label htmlFor="end_date">End Date:</label>
          <input
            id="end_date"
            name="end_date"
            type="date"
            value={endDate}
            onChange={(e) => setEndDate(e.target.value)}
            required
          />
        </div>

        <button
          id="scrapeBtn"
          type="submit"
          disabled={isPending || !!validationError}
          data-testid="scrape-btn"
        >
          {isPending ? 'Scraping...' : 'Scrape Newsletters'}
        </button>
      </form>

      {isPending && (
        <div className="progress">
          <div id="progress-text">
            Scraping newsletters... This may take several minutes.
          </div>
          <div className="progress-bar">
            <div
              className="progress-fill"
              style={{ width: `${progress}%` }}
            />
          </div>
        </div>
      )}

      {validationError && (
        <div className="error" role="alert">
          {validationError}
        </div>
      )}

      {state.error && (
        <div className="error" role="alert">
          Error: {state.error}
        </div>
      )}
    </div>
  )
}

export default ScrapeForm

</document>
<document path="client/src/hooks/useArticleState.js">
import { useCallback, useMemo } from 'react'
import { useSupabaseStorage } from './useSupabaseStorage'
import { getNewsletterScrapeKey } from '../lib/storageKeys'

export function useArticleState(date, url) {
  const storageKey = getNewsletterScrapeKey(date)
  const [payload, setPayload, , { loading, error }] = useSupabaseStorage(storageKey, null)

  const article = useMemo(() => {
    return payload?.articles?.find(a => a.url === url) || null
  }, [payload, url])

  const isRead = article?.read?.isRead ?? false
  const isRemoved = Boolean(article?.removed)
  const isTldrHidden = Boolean(article?.tldrHidden)

  const state = !article ? 0
    : article.removed ? 3
    : article.tldrHidden ? 2
    : article.read?.isRead ? 1
    : 0

  const updateArticle = useCallback((updater) => {
    if (!article) return

    setPayload(current => {
      if (!current) return current

      return {
        ...current,
        articles: current.articles.map(a =>
          a.url === url ? { ...a, ...updater(a) } : a
        )
      }
    })
  }, [article, url, setPayload])

  const markAsRead = useCallback(() => {
    updateArticle(() => ({
      read: { isRead: true, markedAt: new Date().toISOString() }
    }))
  }, [updateArticle])

  const markAsUnread = useCallback(() => {
    updateArticle(() => ({
      read: { isRead: false, markedAt: null }
    }))
  }, [updateArticle])

  const toggleRead = useCallback(() => {
    if (isRead) markAsUnread()
    else markAsRead()
  }, [isRead, markAsRead, markAsUnread])

  const setRemoved = useCallback((removed) => {
    updateArticle(() => ({ removed: Boolean(removed) }))
  }, [updateArticle])

  const toggleRemove = useCallback(() => {
    setRemoved(!isRemoved)
  }, [isRemoved, setRemoved])

  const setTldrHidden = useCallback((hidden) => {
    updateArticle(() => ({ tldrHidden: Boolean(hidden) }))
  }, [updateArticle])

  const markTldrHidden = useCallback(() => {
    setTldrHidden(true)
  }, [setTldrHidden])

  const unmarkTldrHidden = useCallback(() => {
    setTldrHidden(false)
  }, [setTldrHidden])

  return {
    article,
    isRead,
    isRemoved,
    isTldrHidden,
    state,
    loading,
    error,
    markAsRead,
    markAsUnread,
    toggleRead,
    setRemoved,
    toggleRemove,
    setTldrHidden,
    markTldrHidden,
    unmarkTldrHidden,
    updateArticle
  }
}

</document>
<document path="client/src/hooks/useSummary.js">
import { useState, useMemo, useCallback } from 'react'
import { useArticleState } from './useArticleState'
import { marked } from 'marked'
import DOMPurify from 'dompurify'

export function useSummary(date, url, type = 'tldr') {
  if (type === 'summary') {
    throw new Error('Summary feature has been removed. Use type="tldr" instead.')
  }

  const { article, updateArticle } = useArticleState(date, url)
  const [loading, setLoading] = useState(false)
  const [expanded, setExpanded] = useState(false)
  const [effort, setEffort] = useState('low')

  const data = article?.[type]
  const status = data?.status || 'unknown'
  const markdown = data?.markdown || ''

  const html = useMemo(() => {
    if (!markdown) return ''
    try {
      const rawHtml = marked.parse(markdown)
      return DOMPurify.sanitize(rawHtml)
    } catch (error) {
      console.error('Failed to parse markdown:', error)
      return ''
    }
  }, [markdown])

  const errorMessage = data?.errorMessage || null
  const isAvailable = status === 'available' && markdown
  const isLoading = status === 'creating' || loading
  const isError = status === 'error'

  const buttonLabel = useMemo(() => {
    if (isLoading) return 'Loading...'
    if (expanded) return 'Hide'
    if (isAvailable) return 'Available'
    if (isError) return 'Retry'
    return 'TLDR'
  }, [isLoading, expanded, isAvailable, isError, type])

  const fetch = useCallback(async (summaryEffort = effort) => {
    if (!article) return

    setLoading(true)
    setEffort(summaryEffort)

    const endpoint = '/api/tldr-url'

    try {
      const response = await window.fetch(endpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          url,
          summary_effort: summaryEffort
        })
      })

      const result = await response.json()

      if (result.success) {
        updateArticle(() => ({
          [type]: {
            status: 'available',
            markdown: result[`${type}_markdown`] || '',
            effort: summaryEffort,
            checkedAt: new Date().toISOString(),
            errorMessage: null
          }
        }))
        setExpanded(true)
      } else {
        updateArticle((current) => ({
          [type]: {
            ...(current[type] || {}),
            status: 'error',
            errorMessage: result.error || `Failed to fetch ${type}`
          }
        }))
      }
    } catch (error) {
      updateArticle((current) => ({
        [type]: {
          ...(current[type] || {}),
          status: 'error',
          errorMessage: error.message || 'Network error'
        }
      }))
      console.error(`Failed to fetch ${type}:`, error)
    } finally {
      setLoading(false)
    }
  }, [article, url, type, effort, updateArticle])

  const toggle = useCallback((summaryEffort) => {
    if (isAvailable) {
      setExpanded(!expanded)
    } else {
      fetch(summaryEffort)
    }
  }, [isAvailable, expanded, fetch])

  const collapse = useCallback(() => {
    setExpanded(false)
  }, [])

  const expand = useCallback(() => {
    setExpanded(true)
  }, [])

  return {
    data,
    status,
    markdown,
    html,
    errorMessage,
    loading: isLoading,
    expanded,
    effort,
    isAvailable,
    isError,
    buttonLabel,
    fetch,
    toggle,
    collapse,
    expand
  }
}

</document>
<document path="client/src/hooks/useSupabaseStorage.js">
import { useState, useCallback, useEffect, useRef } from 'react'

const changeListenersByKey = new Map()
const readCache = new Map()
const inflightReads = new Map()

function emitChange(key) {
  const listeners = changeListenersByKey.get(key)
  if (listeners) {
    listeners.forEach(listener => {
      try {
        listener()
      } catch (error) {
        console.error(`Storage listener failed: ${error.message}`)
      }
    })
  }

  if (typeof window !== 'undefined') {
    window.dispatchEvent(new CustomEvent('supabase-storage-change', { detail: { key } }))
  }
}

function subscribe(key, listener) {
  if (!changeListenersByKey.has(key)) {
    changeListenersByKey.set(key, new Set())
  }
  changeListenersByKey.get(key).add(listener)

  return () => {
    const listeners = changeListenersByKey.get(key)
    if (listeners) {
      listeners.delete(listener)
      if (listeners.size === 0) {
        changeListenersByKey.delete(key)
      }
    }
  }
}

async function readValue(key, defaultValue) {
  if (typeof window === 'undefined') return defaultValue

  if (readCache.has(key)) {
    return readCache.get(key)
  }

  if (inflightReads.has(key)) {
    return inflightReads.get(key)
  }

  const readPromise = (async () => {
    try {
      let value = defaultValue

      if (key.startsWith('cache:')) {
        const response = await window.fetch(`/api/storage/setting/${key}`)
        const data = await response.json()
        if (data.success) {
          value = data.value
        }
      } else if (key.startsWith('newsletters:scrapes:')) {
        const date = key.split(':')[2]
        const response = await window.fetch(`/api/storage/daily/${date}`)
        const data = await response.json()
        if (data.success) {
          value = data.payload
        }
      } else {
        console.warn(`Unknown storage key pattern: ${key}`)
      }

      readCache.set(key, value)
      return value

    } catch (error) {
      console.error(`Failed to read from storage: ${error.message}`)
      return defaultValue
    } finally {
      inflightReads.delete(key)
    }
  })()

  inflightReads.set(key, readPromise)
  return readPromise
}

async function writeValue(key, value) {
  if (typeof window === 'undefined') return

  try {
    if (key.startsWith('cache:')) {
      const response = await window.fetch(`/api/storage/setting/${key}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ value })
      })

      const data = await response.json()
      if (!data.success) {
        throw new Error(data.error || 'Failed to write setting')
      }

      readCache.set(key, value)
      emitChange(key)
      return
    }

    if (key.startsWith('newsletters:scrapes:')) {
      const date = key.split(':')[2]
      const response = await window.fetch(`/api/storage/daily/${date}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ payload: value })
      })

      const data = await response.json()
      if (!data.success) {
        throw new Error(data.error || 'Failed to write daily cache')
      }

      readCache.set(key, value)
      emitChange(key)
      return
    }

    throw new Error(`Unknown storage key pattern: ${key}`)

  } catch (error) {
    console.error(`Failed to persist to storage: ${error.message}`)
    throw error
  }
}

export function useSupabaseStorage(key, defaultValue) {
  const [value, setValue] = useState(defaultValue)
  const [loading, setLoading] = useState(true)
  const [error, setError] = useState(null)
  const valueRef = useRef(defaultValue)

  useEffect(() => {
    let cancelled = false

    readValue(key, defaultValue).then(loadedValue => {
      if (!cancelled) {
        setValue(loadedValue)
        valueRef.current = loadedValue
        setLoading(false)
      }
    }).catch(err => {
      if (!cancelled) {
        console.error(`Failed to load storage value for ${key}:`, err)
        setError(err)
        setValue(defaultValue)
        valueRef.current = defaultValue
        setLoading(false)
      }
    })

    return () => {
      cancelled = true
    }
  }, [key])

  useEffect(() => {
    const handleChange = () => {
      readValue(key, defaultValue).then(newValue => {
        setValue(newValue)
        valueRef.current = newValue
      }).catch(err => {
        console.error(`Failed to reload storage value for ${key}:`, err)
      })
    }

    return subscribe(key, handleChange)
  }, [key])

  const setValueAsync = useCallback(async (nextValue) => {
    if (typeof window === 'undefined') return

    setLoading(true)
    setError(null)

    try {
      const previous = valueRef.current
      const resolved = typeof nextValue === 'function' ? nextValue(previous) : nextValue

      if (resolved === previous) {
        setLoading(false)
        return
      }

      valueRef.current = resolved
      await writeValue(key, resolved)
      setValue(resolved)
      setLoading(false)

    } catch (err) {
      console.error(`Failed to set storage value for ${key}:`, err)
      setError(err)
      setLoading(false)
      throw err
    }
  }, [key])

  const remove = useCallback(async () => {
    await setValueAsync(undefined)
  }, [setValueAsync])

  return [value, setValueAsync, remove, { loading, error }]
}

</document>
<document path="client/src/lib/scraper.js">
/**
 * Plain JS scraper utilities for React components
 * Extracted from composables/useScraper.js
 */

import { getNewsletterScrapeKey } from './storageKeys'
import * as storageApi from './storageApi'

function computeDateRange(startDate, endDate) {
  const dates = []
  const start = new Date(startDate)
  const end = new Date(endDate)

  if (isNaN(start.getTime()) || isNaN(end.getTime())) {
    return []
  }

  if (start > end) return []

  const current = new Date(end)
  while (current >= start) {
    dates.push(current.toISOString().split('T')[0])
    current.setDate(current.getDate() - 1)
  }

  return dates
}

function buildStatsFromPayloads(payloads) {
  const uniqueUrls = new Set()
  let totalArticles = 0

  payloads.forEach(payload => {
    if (payload.articles) {
      payload.articles.forEach(article => {
        uniqueUrls.add(article.url)
        totalArticles++
      })
    }
  })

  return {
    total_articles: totalArticles,
    unique_urls: uniqueUrls.size,
    dates_processed: payloads.length,
    dates_with_content: payloads.filter(p => p.articles?.length > 0).length
  }
}

async function isRangeCached(startDate, endDate, cacheEnabled) {
  if (!cacheEnabled) return false

  const dates = computeDateRange(startDate, endDate)

  for (const date of dates) {
    const isCached = await storageApi.isDateCached(date)
    if (!isCached) {
      return false
    }
  }

  return true
}

function normalizeIsoDate(value) {
  if (typeof value !== 'string') return null
  const trimmed = value.trim()
  if (!trimmed) return null
  const date = new Date(trimmed)
  if (isNaN(date.getTime())) return null
  return date.toISOString().split('T')[0]
}

function buildDailyPayloadsFromScrape(data) {
  const payloadByDate = new Map()
  const issuesByDate = new Map()

  if (Array.isArray(data.issues)) {
    data.issues.forEach(issue => {
      const date = normalizeIsoDate(issue.date)
      if (!date) return

      if (!issuesByDate.has(date)) {
        issuesByDate.set(date, [])
      }
      issuesByDate.get(date).push(issue)
    })
  }

  if (Array.isArray(data.articles)) {
    data.articles.forEach(article => {
      const date = normalizeIsoDate(article.date)
      if (!date) return

      const articleData = {
        url: article.url,
        title: article.title || article.url,
        articleMeta: article.article_meta || "",
        issueDate: date,
        category: article.category || 'Newsletter',
        sourceId: article.source_id || null,
        section: article.section_title || null,
        sectionEmoji: article.section_emoji || null,
        sectionOrder: article.section_order ?? null,
        newsletterType: article.newsletter_type || null,
        removed: Boolean(article.removed),
        tldrHidden: false,
        tldr: { status: 'unknown', markdown: '', effort: 'low', checkedAt: null, errorMessage: null },
        read: { isRead: false, markedAt: null }
      }

      if (!payloadByDate.has(date)) {
        payloadByDate.set(date, [])
      }
      payloadByDate.get(date).push(articleData)
    })
  }

  const payloads = []
  payloadByDate.forEach((articles, date) => {
    const issues = issuesByDate.get(date) || []
    payloads.push({
      date,
      cachedAt: new Date().toISOString(),
      articles,
      issues
    })
  })

  return payloads.sort((a, b) => (a.date < b.date ? 1 : a.date > b.date ? -1 : 0))
}

async function mergeWithCache(payloads) {
  const merged = []

  for (const payload of payloads) {
    const existing = await storageApi.getDailyPayload(payload.date)

    if (existing) {
      const mergedPayload = {
        ...payload,
        articles: payload.articles.map(article => {
          const existingArticle = existing.articles?.find(a => a.url === article.url)
          if (existingArticle) {
            return {
              ...article,
              tldr: existingArticle.tldr || article.tldr,
              read: existingArticle.read || article.read,
              removed: existingArticle.removed ?? article.removed,
              tldrHidden: existingArticle.tldrHidden ?? article.tldrHidden
            }
          }
          return article
        })
      }

      await storageApi.setDailyPayload(payload.date, mergedPayload)
      merged.push(mergedPayload)
    } else {
      await storageApi.setDailyPayload(payload.date, payload)
      merged.push(payload)
    }
  }

  return merged
}

export async function loadFromCache(startDate, endDate) {
  const payloads = await storageApi.getDailyPayloadsRange(startDate, endDate)

  if (!payloads || payloads.length === 0) {
    return null
  }

  return {
    success: true,
    payloads,
    source: 'local cache',
    stats: buildStatsFromPayloads(payloads)
  }
}

export async function scrapeNewsletters(startDate, endDate, cacheEnabled = true) {
  if (await isRangeCached(startDate, endDate, cacheEnabled)) {
    const cached = await loadFromCache(startDate, endDate)
    if (cached) {
      return cached
    }
  }

  const response = await window.fetch('/api/scrape', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      start_date: startDate,
      end_date: endDate
    })
  })

  const data = await response.json()

  if (data.success) {
    const payloads = buildDailyPayloadsFromScrape(data)
    const mergedPayloads = cacheEnabled ? await mergeWithCache(payloads) : payloads

    return {
      success: true,
      payloads: mergedPayloads,
      source: 'Live scrape',
      stats: data.stats,
      debugLogs: data.stats?.debug_logs || []
    }
  } else {
    throw new Error(data.error || 'Scraping failed')
  }
}

</document>
<document path="client/src/lib/storageApi.js">
export async function isDateCached(date) {
  const response = await window.fetch(`/api/storage/is-cached/${date}`)
  const data = await response.json()

  if (data.success) {
    return data.is_cached
  }

  throw new Error(data.error || 'Failed to check cache')
}

export async function getDailyPayload(date) {
  const response = await window.fetch(`/api/storage/daily/${date}`)
  const data = await response.json()

  if (data.success) {
    return data.payload
  }

  return null
}

export async function setDailyPayload(date, payload) {
  const response = await window.fetch(`/api/storage/daily/${date}`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ payload })
  })

  const data = await response.json()

  if (!data.success) {
    throw new Error(data.error || 'Failed to save payload')
  }

  return data.data
}

export async function getDailyPayloadsRange(startDate, endDate) {
  const response = await window.fetch('/api/storage/daily-range', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ start_date: startDate, end_date: endDate })
  })

  const data = await response.json()

  if (data.success) {
    return data.payloads
  }

  throw new Error(data.error || 'Failed to load payloads')
}

</document>
<document path="client/src/lib/storageKeys.js">
export const STORAGE_KEYS = {
  CACHE_ENABLED: 'cache:enabled'
}

/**
 * Get the storage key for newsletter scrape data for a specific date.
 * @param {string} date - ISO date string (YYYY-MM-DD)
 * @returns {string} The storage key used for Supabase API endpoints
 */
export function getNewsletterScrapeKey(date) {
  return `newsletters:scrapes:${date}`
}

</document>
<document path="client/src/main.jsx">
import React from 'react'
import ReactDOM from 'react-dom/client'
import App from './App'

ReactDOM.createRoot(document.getElementById('root')).render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
)

</document>
</documents>
